
#######################################################################
Please cite the following paper when using nnU-Net:
Isensee, F., Jaeger, P. F., Kohl, S. A., Petersen, J., & Maier-Hein, K. H. (2021). nnU-Net: a self-configuring method for deep learning-based biomedical image segmentation. Nature methods, 18(2), 203-211.
#######################################################################
 

This is the configuration used by this training:
Configuration name: 3d_fullres
 {'data_identifier': 'nnUNetPlans_3d_fullres', 'preprocessor_name': 'DefaultPreprocessor', 'batch_size': 2, 'patch_size': [48, 192, 256], 'median_image_size_in_voxels': [45.5, 174.0, 253.0], 'spacing': [1.0, 1.0, 1.0], 'normalization_schemes': ['CTNormalization'], 'use_mask_for_norm': [False], 'resampling_fn_data': 'resample_data_or_seg_to_shape', 'resampling_fn_seg': 'resample_data_or_seg_to_shape', 'resampling_fn_data_kwargs': {'is_seg': False, 'order': 3, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_seg_kwargs': {'is_seg': True, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_probabilities': 'resample_data_or_seg_to_shape', 'resampling_fn_probabilities_kwargs': {'is_seg': False, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'architecture': {'network_class_name': 'dynamic_network_architectures.architectures.unet.PlainConvUNet', 'arch_kwargs': {'n_stages': 6, 'features_per_stage': [32, 64, 128, 256, 320, 320], 'conv_op': 'torch.nn.modules.conv.Conv3d', 'kernel_sizes': [[3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3]], 'strides': [[1, 1, 1], [2, 2, 2], [2, 2, 2], [2, 2, 2], [1, 2, 2], [1, 2, 2]], 'n_conv_per_stage': [2, 2, 2, 2, 2, 2], 'n_conv_per_stage_decoder': [2, 2, 2, 2, 2], 'conv_bias': True, 'norm_op': 'torch.nn.modules.instancenorm.InstanceNorm3d', 'norm_op_kwargs': {'eps': 1e-05, 'affine': True}, 'dropout_op': None, 'dropout_op_kwargs': None, 'nonlin': 'torch.nn.LeakyReLU', 'nonlin_kwargs': {'inplace': True}, 'deep_supervision': True}, '_kw_requires_import': ['conv_op', 'norm_op', 'dropout_op', 'nonlin']}, 'batch_dice': False} 
 
These are the global plan.json settings:
 {'dataset_name': 'Dataset030_4DCT_ONLY_2_3D', 'plans_name': 'nnUNetPlans', 'original_median_spacing_after_transp': [1.0, 1.0, 1.0], 'original_median_shape_after_transp': [46, 174, 253], 'image_reader_writer': 'SimpleITKIO', 'transpose_forward': [0, 1, 2], 'transpose_backward': [0, 1, 2], 'experiment_planner_used': 'ExperimentPlanner', 'label_manager': 'LabelManager', 'foreground_intensity_properties_per_channel': {'0': {'max': 1.0, 'mean': 0.4892812408845811, 'median': 0.5165764093399048, 'min': 0.0, 'percentile_00_5': 0.10703501850366592, 'percentile_99_5': 0.8701646634936324, 'std': 0.1428518334997195}}} 
 
2024-05-15 04:28:49.422161: unpacking dataset... 
2024-05-15 04:28:53.000076: unpacking done... 
2024-05-15 04:28:53.000805: do_dummy_2d_data_aug: True 
2024-05-15 04:28:53.010308: Using splits from existing split file: /raid/dataset/nnUNet_preprocessed/Dataset030_4DCT_ONLY_2_3D/splits_final.json 
2024-05-15 04:28:53.011648: The split file contains 5 splits. 
2024-05-15 04:28:53.011690: Desired fold for training: 0 
2024-05-15 04:28:53.011717: This split has 1723 training and 431 validation cases. 
2024-05-15 04:28:53.088115: Unable to plot network architecture: 
2024-05-15 04:28:53.088192: No module named 'hiddenlayer' 
2024-05-15 04:28:53.158678:  
2024-05-15 04:28:53.158758: Epoch 800 
2024-05-15 04:28:53.158887: Current learning rate: 0.00235 
2024-05-15 04:29:47.863886: train_loss -0.7994 
2024-05-15 04:29:47.864087: val_loss -0.7641 
2024-05-15 04:29:47.864140: Pseudo dice [0.8527] 
2024-05-15 04:29:47.864202: Epoch time: 54.71 s 
2024-05-15 04:29:49.035657:  
2024-05-15 04:29:49.035786: Epoch 801 
2024-05-15 04:29:49.035881: Current learning rate: 0.00234 
2024-05-15 04:30:30.181151: train_loss -0.7873 
2024-05-15 04:30:30.181342: val_loss -0.7737 
2024-05-15 04:30:30.181393: Pseudo dice [0.856] 
2024-05-15 04:30:30.181446: Epoch time: 41.15 s 
2024-05-15 04:30:31.506248:  
2024-05-15 04:30:31.506378: Epoch 802 
2024-05-15 04:30:31.506477: Current learning rate: 0.00233 
2024-05-15 04:31:12.716080: train_loss -0.7983 
2024-05-15 04:31:12.716251: val_loss -0.7989 
2024-05-15 04:31:12.716302: Pseudo dice [0.8547] 
2024-05-15 04:31:12.716355: Epoch time: 41.21 s 
2024-05-15 04:31:13.843097:  
2024-05-15 04:31:13.843223: Epoch 803 
2024-05-15 04:31:13.843314: Current learning rate: 0.00232 
2024-05-15 04:31:55.137564: train_loss -0.8056 
2024-05-15 04:31:55.137738: val_loss -0.7803 
2024-05-15 04:31:55.137787: Pseudo dice [0.8567] 
2024-05-15 04:31:55.137841: Epoch time: 41.3 s 
2024-05-15 04:31:56.266127:  
2024-05-15 04:31:56.266311: Epoch 804 
2024-05-15 04:31:56.266407: Current learning rate: 0.00231 
2024-05-15 04:32:37.553121: train_loss -0.8024 
2024-05-15 04:32:37.553292: val_loss -0.7748 
2024-05-15 04:32:37.553341: Pseudo dice [0.8454] 
2024-05-15 04:32:37.553394: Epoch time: 41.29 s 
2024-05-15 04:32:38.690998:  
2024-05-15 04:32:38.691118: Epoch 805 
2024-05-15 04:32:38.691206: Current learning rate: 0.0023 
2024-05-15 04:33:20.062384: train_loss -0.8008 
2024-05-15 04:33:20.062563: val_loss -0.7761 
2024-05-15 04:33:20.062612: Pseudo dice [0.8579] 
2024-05-15 04:33:20.062665: Epoch time: 41.37 s 
2024-05-15 04:33:21.197453:  
2024-05-15 04:33:21.197579: Epoch 806 
2024-05-15 04:33:21.197667: Current learning rate: 0.00229 
2024-05-15 04:34:02.593304: train_loss -0.8051 
2024-05-15 04:34:02.593480: val_loss -0.7656 
2024-05-15 04:34:02.593528: Pseudo dice [0.8396] 
2024-05-15 04:34:02.593580: Epoch time: 41.4 s 
2024-05-15 04:34:03.723993:  
2024-05-15 04:34:03.724175: Epoch 807 
2024-05-15 04:34:03.724269: Current learning rate: 0.00228 
2024-05-15 04:34:45.077561: train_loss -0.8034 
2024-05-15 04:34:45.077734: val_loss -0.7914 
2024-05-15 04:34:45.077782: Pseudo dice [0.8604] 
2024-05-15 04:34:45.077836: Epoch time: 41.35 s 
2024-05-15 04:34:46.418512:  
2024-05-15 04:34:46.418654: Epoch 808 
2024-05-15 04:34:46.418748: Current learning rate: 0.00226 
2024-05-15 04:35:27.867790: train_loss -0.8041 
2024-05-15 04:35:27.867966: val_loss -0.7834 
2024-05-15 04:35:27.868016: Pseudo dice [0.8658] 
2024-05-15 04:35:27.868069: Epoch time: 41.45 s 
2024-05-15 04:35:28.991803:  
2024-05-15 04:35:28.991926: Epoch 809 
2024-05-15 04:35:28.992014: Current learning rate: 0.00225 
2024-05-15 04:36:10.432421: train_loss -0.7944 
2024-05-15 04:36:10.432590: val_loss -0.795 
2024-05-15 04:36:10.432639: Pseudo dice [0.8616] 
2024-05-15 04:36:10.432694: Epoch time: 41.44 s 
2024-05-15 04:36:11.555534:  
2024-05-15 04:36:11.555702: Epoch 810 
2024-05-15 04:36:11.555793: Current learning rate: 0.00224 
2024-05-15 04:36:53.054834: train_loss -0.7997 
2024-05-15 04:36:53.055057: val_loss -0.7566 
2024-05-15 04:36:53.055106: Pseudo dice [0.8458] 
2024-05-15 04:36:53.055163: Epoch time: 41.5 s 
2024-05-15 04:36:54.196163:  
2024-05-15 04:36:54.196316: Epoch 811 
2024-05-15 04:36:54.196409: Current learning rate: 0.00223 
2024-05-15 04:37:35.635683: train_loss -0.7999 
2024-05-15 04:37:35.635857: val_loss -0.7834 
2024-05-15 04:37:35.635943: Pseudo dice [0.8541] 
2024-05-15 04:37:35.635998: Epoch time: 41.44 s 
2024-05-15 04:37:36.759777:  
2024-05-15 04:37:36.760092: Epoch 812 
2024-05-15 04:37:36.760241: Current learning rate: 0.00222 
2024-05-15 04:38:18.161512: train_loss -0.8018 
2024-05-15 04:38:18.161682: val_loss -0.7711 
2024-05-15 04:38:18.161731: Pseudo dice [0.8564] 
2024-05-15 04:38:18.161786: Epoch time: 41.4 s 
2024-05-15 04:38:19.282799:  
2024-05-15 04:38:19.282946: Epoch 813 
2024-05-15 04:38:19.283060: Current learning rate: 0.00221 
2024-05-15 04:39:00.725245: train_loss -0.8114 
2024-05-15 04:39:00.725415: val_loss -0.7894 
2024-05-15 04:39:00.725462: Pseudo dice [0.8618] 
2024-05-15 04:39:00.725517: Epoch time: 41.44 s 
2024-05-15 04:39:01.865293:  
2024-05-15 04:39:01.865426: Epoch 814 
2024-05-15 04:39:01.865519: Current learning rate: 0.0022 
2024-05-15 04:39:43.393112: train_loss -0.812 
2024-05-15 04:39:43.393286: val_loss -0.794 
2024-05-15 04:39:43.393335: Pseudo dice [0.8605] 
2024-05-15 04:39:43.393386: Epoch time: 41.53 s 
2024-05-15 04:39:44.531105:  
2024-05-15 04:39:44.531218: Epoch 815 
2024-05-15 04:39:44.531305: Current learning rate: 0.00219 
2024-05-15 04:40:26.007080: train_loss -0.8084 
2024-05-15 04:40:26.007259: val_loss -0.7932 
2024-05-15 04:40:26.007308: Pseudo dice [0.8638] 
2024-05-15 04:40:26.007364: Epoch time: 41.48 s 
2024-05-15 04:40:27.140620:  
2024-05-15 04:40:27.140749: Epoch 816 
2024-05-15 04:40:27.140840: Current learning rate: 0.00218 
2024-05-15 04:41:08.594882: train_loss -0.8066 
2024-05-15 04:41:08.595052: val_loss -0.774 
2024-05-15 04:41:08.595100: Pseudo dice [0.8613] 
2024-05-15 04:41:08.595152: Epoch time: 41.46 s 
2024-05-15 04:41:09.723740:  
2024-05-15 04:41:09.723864: Epoch 817 
2024-05-15 04:41:09.723949: Current learning rate: 0.00217 
2024-05-15 04:41:51.222758: train_loss -0.7932 
2024-05-15 04:41:51.222933: val_loss -0.7824 
2024-05-15 04:41:51.222980: Pseudo dice [0.8557] 
2024-05-15 04:41:51.223036: Epoch time: 41.5 s 
2024-05-15 04:41:52.354818:  
2024-05-15 04:41:52.354942: Epoch 818 
2024-05-15 04:41:52.355047: Current learning rate: 0.00216 
2024-05-15 04:42:33.917057: train_loss -0.8062 
2024-05-15 04:42:33.917356: val_loss -0.7917 
2024-05-15 04:42:33.917404: Pseudo dice [0.8482] 
2024-05-15 04:42:33.917456: Epoch time: 41.56 s 
2024-05-15 04:42:35.040358:  
2024-05-15 04:42:35.040484: Epoch 819 
2024-05-15 04:42:35.040575: Current learning rate: 0.00215 
2024-05-15 04:43:16.564409: train_loss -0.7892 
2024-05-15 04:43:16.564582: val_loss -0.7834 
2024-05-15 04:43:16.564630: Pseudo dice [0.8568] 
2024-05-15 04:43:16.564682: Epoch time: 41.53 s 
2024-05-15 04:43:17.624390:  
2024-05-15 04:43:17.624512: Epoch 820 
2024-05-15 04:43:17.624606: Current learning rate: 0.00214 
2024-05-15 04:43:59.104596: train_loss -0.806 
2024-05-15 04:43:59.104773: val_loss -0.7978 
2024-05-15 04:43:59.104821: Pseudo dice [0.8689] 
2024-05-15 04:43:59.104873: Epoch time: 41.48 s 
2024-05-15 04:43:59.104917: Yayy! New best EMA pseudo Dice: 0.8579 
2024-05-15 04:44:00.688515:  
2024-05-15 04:44:00.688769: Epoch 821 
2024-05-15 04:44:00.688865: Current learning rate: 0.00213 
2024-05-15 04:44:42.220574: train_loss -0.8088 
2024-05-15 04:44:42.220756: val_loss -0.7723 
2024-05-15 04:44:42.220807: Pseudo dice [0.8522] 
2024-05-15 04:44:42.220860: Epoch time: 41.53 s 
2024-05-15 04:44:43.276172:  
2024-05-15 04:44:43.276378: Epoch 822 
2024-05-15 04:44:43.276473: Current learning rate: 0.00212 
2024-05-15 04:45:24.767294: train_loss -0.8059 
2024-05-15 04:45:24.767462: val_loss -0.7902 
2024-05-15 04:45:24.767515: Pseudo dice [0.8734] 
2024-05-15 04:45:24.767572: Epoch time: 41.49 s 
2024-05-15 04:45:24.767619: Yayy! New best EMA pseudo Dice: 0.8589 
2024-05-15 04:45:26.174404:  
2024-05-15 04:45:26.174531: Epoch 823 
2024-05-15 04:45:26.174628: Current learning rate: 0.0021 
2024-05-15 04:46:07.703621: train_loss -0.7962 
2024-05-15 04:46:07.703800: val_loss -0.7891 
2024-05-15 04:46:07.703848: Pseudo dice [0.8697] 
2024-05-15 04:46:07.703902: Epoch time: 41.53 s 
2024-05-15 04:46:07.703943: Yayy! New best EMA pseudo Dice: 0.86 
2024-05-15 04:46:09.110661:  
2024-05-15 04:46:09.110888: Epoch 824 
2024-05-15 04:46:09.111014: Current learning rate: 0.00209 
2024-05-15 04:46:50.683756: train_loss -0.809 
2024-05-15 04:46:50.684024: val_loss -0.7732 
2024-05-15 04:46:50.684073: Pseudo dice [0.8658] 
2024-05-15 04:46:50.684126: Epoch time: 41.57 s 
2024-05-15 04:46:50.684170: Yayy! New best EMA pseudo Dice: 0.8606 
2024-05-15 04:46:52.090232:  
2024-05-15 04:46:52.090363: Epoch 825 
2024-05-15 04:46:52.090459: Current learning rate: 0.00208 
2024-05-15 04:47:33.605062: train_loss -0.8158 
2024-05-15 04:47:33.605239: val_loss -0.8129 
2024-05-15 04:47:33.605289: Pseudo dice [0.8666] 
2024-05-15 04:47:33.605341: Epoch time: 41.52 s 
2024-05-15 04:47:33.605385: Yayy! New best EMA pseudo Dice: 0.8612 
2024-05-15 04:47:35.055988:  
2024-05-15 04:47:35.056185: Epoch 826 
2024-05-15 04:47:35.056277: Current learning rate: 0.00207 
2024-05-15 04:48:16.648180: train_loss -0.8007 
2024-05-15 04:48:16.648344: val_loss -0.7864 
2024-05-15 04:48:16.648403: Pseudo dice [0.8516] 
2024-05-15 04:48:16.648456: Epoch time: 41.59 s 
2024-05-15 04:48:17.709075:  
2024-05-15 04:48:17.709273: Epoch 827 
2024-05-15 04:48:17.709365: Current learning rate: 0.00206 
2024-05-15 04:48:59.284194: train_loss -0.7997 
2024-05-15 04:48:59.284366: val_loss -0.7774 
2024-05-15 04:48:59.284416: Pseudo dice [0.8584] 
2024-05-15 04:48:59.284469: Epoch time: 41.58 s 
2024-05-15 04:49:00.343450:  
2024-05-15 04:49:00.343572: Epoch 828 
2024-05-15 04:49:00.343663: Current learning rate: 0.00205 
2024-05-15 04:49:41.939404: train_loss -0.8023 
2024-05-15 04:49:41.939571: val_loss -0.7794 
2024-05-15 04:49:41.939620: Pseudo dice [0.8573] 
2024-05-15 04:49:41.939672: Epoch time: 41.6 s 
2024-05-15 04:49:42.989198:  
2024-05-15 04:49:42.989450: Epoch 829 
2024-05-15 04:49:42.989561: Current learning rate: 0.00204 
2024-05-15 04:50:24.543279: train_loss -0.8038 
2024-05-15 04:50:24.543448: val_loss -0.7909 
2024-05-15 04:50:24.543497: Pseudo dice [0.8637] 
2024-05-15 04:50:24.543550: Epoch time: 41.56 s 
2024-05-15 04:50:25.604487:  
2024-05-15 04:50:25.604615: Epoch 830 
2024-05-15 04:50:25.604703: Current learning rate: 0.00203 
2024-05-15 04:51:07.155845: train_loss -0.8048 
2024-05-15 04:51:07.156016: val_loss -0.779 
2024-05-15 04:51:07.156066: Pseudo dice [0.8517] 
2024-05-15 04:51:07.156119: Epoch time: 41.55 s 
2024-05-15 04:51:08.215466:  
2024-05-15 04:51:08.215586: Epoch 831 
2024-05-15 04:51:08.215677: Current learning rate: 0.00202 
2024-05-15 04:51:49.736727: train_loss -0.8045 
2024-05-15 04:51:49.736897: val_loss -0.8024 
2024-05-15 04:51:49.736945: Pseudo dice [0.8542] 
2024-05-15 04:51:49.736998: Epoch time: 41.52 s 
2024-05-15 04:51:50.805520:  
2024-05-15 04:51:50.805644: Epoch 832 
2024-05-15 04:51:50.805730: Current learning rate: 0.00201 
2024-05-15 04:52:32.311044: train_loss -0.8132 
2024-05-15 04:52:32.311215: val_loss -0.8012 
2024-05-15 04:52:32.311264: Pseudo dice [0.8659] 
2024-05-15 04:52:32.311319: Epoch time: 41.51 s 
2024-05-15 04:52:33.554434:  
2024-05-15 04:52:33.554563: Epoch 833 
2024-05-15 04:52:33.554651: Current learning rate: 0.002 
2024-05-15 04:53:15.110723: train_loss -0.8106 
2024-05-15 04:53:15.110895: val_loss -0.7587 
2024-05-15 04:53:15.110944: Pseudo dice [0.8613] 
2024-05-15 04:53:15.110997: Epoch time: 41.56 s 
2024-05-15 04:53:16.177463:  
2024-05-15 04:53:16.177633: Epoch 834 
2024-05-15 04:53:16.177726: Current learning rate: 0.00199 
2024-05-15 04:53:57.723274: train_loss -0.8037 
2024-05-15 04:53:57.723443: val_loss -0.8067 
2024-05-15 04:53:57.723493: Pseudo dice [0.8633] 
2024-05-15 04:53:57.723546: Epoch time: 41.55 s 
2024-05-15 04:53:58.778296:  
2024-05-15 04:53:58.778464: Epoch 835 
2024-05-15 04:53:58.778559: Current learning rate: 0.00198 
2024-05-15 04:54:40.318972: train_loss -0.8105 
2024-05-15 04:54:40.319144: val_loss -0.7996 
2024-05-15 04:54:40.319193: Pseudo dice [0.8722] 
2024-05-15 04:54:40.319246: Epoch time: 41.54 s 
2024-05-15 04:54:40.319290: Yayy! New best EMA pseudo Dice: 0.8613 
2024-05-15 04:54:41.735346:  
2024-05-15 04:54:41.735541: Epoch 836 
2024-05-15 04:54:41.735636: Current learning rate: 0.00196 
2024-05-15 04:55:23.296629: train_loss -0.7979 
2024-05-15 04:55:23.296795: val_loss -0.7369 
2024-05-15 04:55:23.296844: Pseudo dice [0.8474] 
2024-05-15 04:55:23.296898: Epoch time: 41.56 s 
2024-05-15 04:55:24.356051:  
2024-05-15 04:55:24.356174: Epoch 837 
2024-05-15 04:55:24.356263: Current learning rate: 0.00195 
2024-05-15 04:56:05.889887: train_loss -0.7998 
2024-05-15 04:56:05.890065: val_loss -0.7867 
2024-05-15 04:56:05.890113: Pseudo dice [0.8605] 
2024-05-15 04:56:05.890167: Epoch time: 41.53 s 
2024-05-15 04:56:06.944839:  
2024-05-15 04:56:06.945012: Epoch 838 
2024-05-15 04:56:06.945157: Current learning rate: 0.00194 
2024-05-15 04:56:48.479566: train_loss -0.798 
2024-05-15 04:56:48.479738: val_loss -0.7815 
2024-05-15 04:56:48.479787: Pseudo dice [0.855] 
2024-05-15 04:56:48.479840: Epoch time: 41.54 s 
2024-05-15 04:56:49.536738:  
2024-05-15 04:56:49.536949: Epoch 839 
2024-05-15 04:56:49.537078: Current learning rate: 0.00193 
2024-05-15 04:57:31.062272: train_loss -0.8043 
2024-05-15 04:57:31.062447: val_loss -0.764 
2024-05-15 04:57:31.062495: Pseudo dice [0.8582] 
2024-05-15 04:57:31.062547: Epoch time: 41.53 s 
2024-05-15 04:57:32.304574:  
2024-05-15 04:57:32.304793: Epoch 840 
2024-05-15 04:57:32.304898: Current learning rate: 0.00192 
2024-05-15 04:58:13.833768: train_loss -0.8038 
2024-05-15 04:58:13.833961: val_loss -0.7755 
2024-05-15 04:58:13.834013: Pseudo dice [0.8548] 
2024-05-15 04:58:13.834068: Epoch time: 41.53 s 
2024-05-15 04:58:14.886853:  
2024-05-15 04:58:14.887063: Epoch 841 
2024-05-15 04:58:14.887158: Current learning rate: 0.00191 
2024-05-15 04:58:56.410285: train_loss -0.8026 
2024-05-15 04:58:56.410470: val_loss -0.7761 
2024-05-15 04:58:56.410519: Pseudo dice [0.8544] 
2024-05-15 04:58:56.410571: Epoch time: 41.52 s 
2024-05-15 04:58:57.462018:  
2024-05-15 04:58:57.462275: Epoch 842 
2024-05-15 04:58:57.462391: Current learning rate: 0.0019 
2024-05-15 04:59:38.999204: train_loss -0.8075 
2024-05-15 04:59:38.999370: val_loss -0.8014 
2024-05-15 04:59:38.999418: Pseudo dice [0.871] 
2024-05-15 04:59:38.999471: Epoch time: 41.54 s 
2024-05-15 04:59:40.050687:  
2024-05-15 04:59:40.051005: Epoch 843 
2024-05-15 04:59:40.051101: Current learning rate: 0.00189 
2024-05-15 05:00:21.564739: train_loss -0.8033 
2024-05-15 05:00:21.564905: val_loss -0.8091 
2024-05-15 05:00:21.564955: Pseudo dice [0.8647] 
2024-05-15 05:00:21.565006: Epoch time: 41.52 s 
2024-05-15 05:00:22.655177:  
2024-05-15 05:00:22.655307: Epoch 844 
2024-05-15 05:00:22.655397: Current learning rate: 0.00188 
2024-05-15 05:01:04.217580: train_loss -0.8098 
2024-05-15 05:01:04.217753: val_loss -0.7782 
2024-05-15 05:01:04.217799: Pseudo dice [0.8613] 
2024-05-15 05:01:04.217851: Epoch time: 41.56 s 
2024-05-15 05:01:05.278291:  
2024-05-15 05:01:05.278414: Epoch 845 
2024-05-15 05:01:05.278511: Current learning rate: 0.00187 
2024-05-15 05:01:46.823980: train_loss -0.8112 
2024-05-15 05:01:46.824154: val_loss -0.7915 
2024-05-15 05:01:46.824202: Pseudo dice [0.8496] 
2024-05-15 05:01:46.824255: Epoch time: 41.55 s 
2024-05-15 05:01:48.067664:  
2024-05-15 05:01:48.067802: Epoch 846 
2024-05-15 05:01:48.067893: Current learning rate: 0.00186 
2024-05-15 05:02:29.584897: train_loss -0.8004 
2024-05-15 05:02:29.585068: val_loss -0.776 
2024-05-15 05:02:29.585117: Pseudo dice [0.8618] 
2024-05-15 05:02:29.585171: Epoch time: 41.52 s 
2024-05-15 05:02:30.639535:  
2024-05-15 05:02:30.639717: Epoch 847 
2024-05-15 05:02:30.639810: Current learning rate: 0.00185 
2024-05-15 05:03:12.154949: train_loss -0.7945 
2024-05-15 05:03:12.155124: val_loss -0.7748 
2024-05-15 05:03:12.155172: Pseudo dice [0.8513] 
2024-05-15 05:03:12.155224: Epoch time: 41.52 s 
2024-05-15 05:03:13.208378:  
2024-05-15 05:03:13.208507: Epoch 848 
2024-05-15 05:03:13.208599: Current learning rate: 0.00184 
2024-05-15 05:03:54.741286: train_loss -0.8094 
2024-05-15 05:03:54.741455: val_loss -0.7705 
2024-05-15 05:03:54.741503: Pseudo dice [0.8605] 
2024-05-15 05:03:54.741555: Epoch time: 41.53 s 
2024-05-15 05:03:55.801183:  
2024-05-15 05:03:55.801390: Epoch 849 
2024-05-15 05:03:55.801485: Current learning rate: 0.00182 
2024-05-15 05:04:37.314913: train_loss -0.802 
2024-05-15 05:04:37.315091: val_loss -0.7909 
2024-05-15 05:04:37.315139: Pseudo dice [0.8543] 
2024-05-15 05:04:37.315204: Epoch time: 41.51 s 
2024-05-15 05:04:38.726659:  
2024-05-15 05:04:38.726863: Epoch 850 
2024-05-15 05:04:38.726965: Current learning rate: 0.00181 
2024-05-15 05:05:20.277509: train_loss -0.8131 
2024-05-15 05:05:20.277683: val_loss -0.7828 
2024-05-15 05:05:20.277730: Pseudo dice [0.8551] 
2024-05-15 05:05:20.277782: Epoch time: 41.55 s 
2024-05-15 05:05:21.323416:  
2024-05-15 05:05:21.323595: Epoch 851 
2024-05-15 05:05:21.323691: Current learning rate: 0.0018 
2024-05-15 05:06:02.874582: train_loss -0.8135 
2024-05-15 05:06:02.874753: val_loss -0.7762 
2024-05-15 05:06:02.874801: Pseudo dice [0.8676] 
2024-05-15 05:06:02.874855: Epoch time: 41.55 s 
2024-05-15 05:06:03.950332:  
2024-05-15 05:06:03.950454: Epoch 852 
2024-05-15 05:06:03.950542: Current learning rate: 0.00179 
2024-05-15 05:06:45.496850: train_loss -0.81 
2024-05-15 05:06:45.497010: val_loss -0.7948 
2024-05-15 05:06:45.497058: Pseudo dice [0.8697] 
2024-05-15 05:06:45.497110: Epoch time: 41.55 s 
2024-05-15 05:06:46.738554:  
2024-05-15 05:06:46.738883: Epoch 853 
2024-05-15 05:06:46.739030: Current learning rate: 0.00178 
2024-05-15 05:07:28.324717: train_loss -0.8102 
2024-05-15 05:07:28.324895: val_loss -0.7895 
2024-05-15 05:07:28.324944: Pseudo dice [0.8494] 
2024-05-15 05:07:28.324998: Epoch time: 41.59 s 
2024-05-15 05:07:29.374493:  
2024-05-15 05:07:29.374611: Epoch 854 
2024-05-15 05:07:29.374704: Current learning rate: 0.00177 
2024-05-15 05:08:10.991843: train_loss -0.8034 
2024-05-15 05:08:10.992011: val_loss -0.7776 
2024-05-15 05:08:10.992059: Pseudo dice [0.8549] 
2024-05-15 05:08:10.992112: Epoch time: 41.62 s 
2024-05-15 05:08:12.081349:  
2024-05-15 05:08:12.081483: Epoch 855 
2024-05-15 05:08:12.081572: Current learning rate: 0.00176 
2024-05-15 05:08:53.640917: train_loss -0.7953 
2024-05-15 05:08:53.641089: val_loss -0.793 
2024-05-15 05:08:53.641138: Pseudo dice [0.8576] 
2024-05-15 05:08:53.641198: Epoch time: 41.56 s 
2024-05-15 05:08:54.693748:  
2024-05-15 05:08:54.693947: Epoch 856 
2024-05-15 05:08:54.694072: Current learning rate: 0.00175 
2024-05-15 05:09:36.228381: train_loss -0.8102 
2024-05-15 05:09:36.228559: val_loss -0.7997 
2024-05-15 05:09:36.228607: Pseudo dice [0.8596] 
2024-05-15 05:09:36.228661: Epoch time: 41.54 s 
2024-05-15 05:09:37.270606:  
2024-05-15 05:09:37.270742: Epoch 857 
2024-05-15 05:09:37.270836: Current learning rate: 0.00174 
2024-05-15 05:10:18.775320: train_loss -0.812 
2024-05-15 05:10:18.775493: val_loss -0.787 
2024-05-15 05:10:18.775539: Pseudo dice [0.8626] 
2024-05-15 05:10:18.775591: Epoch time: 41.51 s 
2024-05-15 05:10:19.817028:  
2024-05-15 05:10:19.817163: Epoch 858 
2024-05-15 05:10:19.817258: Current learning rate: 0.00173 
2024-05-15 05:11:01.342716: train_loss -0.8135 
2024-05-15 05:11:01.342906: val_loss -0.7705 
2024-05-15 05:11:01.342954: Pseudo dice [0.8645] 
2024-05-15 05:11:01.343006: Epoch time: 41.53 s 
2024-05-15 05:11:02.380934:  
2024-05-15 05:11:02.381060: Epoch 859 
2024-05-15 05:11:02.381151: Current learning rate: 0.00172 
2024-05-15 05:11:43.887862: train_loss -0.8079 
2024-05-15 05:11:43.888042: val_loss -0.789 
2024-05-15 05:11:43.888090: Pseudo dice [0.8687] 
2024-05-15 05:11:43.888144: Epoch time: 41.51 s 
2024-05-15 05:11:45.134708:  
2024-05-15 05:11:45.134853: Epoch 860 
2024-05-15 05:11:45.134945: Current learning rate: 0.0017 
2024-05-15 05:12:26.670263: train_loss -0.8096 
2024-05-15 05:12:26.670435: val_loss -0.7887 
2024-05-15 05:12:26.670485: Pseudo dice [0.8454] 
2024-05-15 05:12:26.670539: Epoch time: 41.54 s 
2024-05-15 05:12:27.712204:  
2024-05-15 05:12:27.712428: Epoch 861 
2024-05-15 05:12:27.712527: Current learning rate: 0.00169 
2024-05-15 05:13:09.252377: train_loss -0.8148 
2024-05-15 05:13:09.252546: val_loss -0.8073 
2024-05-15 05:13:09.252594: Pseudo dice [0.8664] 
2024-05-15 05:13:09.252646: Epoch time: 41.54 s 
2024-05-15 05:13:10.295533:  
2024-05-15 05:13:10.295742: Epoch 862 
2024-05-15 05:13:10.295834: Current learning rate: 0.00168 
2024-05-15 05:13:51.850772: train_loss -0.8048 
2024-05-15 05:13:51.850944: val_loss -0.7764 
2024-05-15 05:13:51.850991: Pseudo dice [0.867] 
2024-05-15 05:13:51.851043: Epoch time: 41.56 s 
2024-05-15 05:13:52.899436:  
2024-05-15 05:13:52.899659: Epoch 863 
2024-05-15 05:13:52.899752: Current learning rate: 0.00167 
2024-05-15 05:14:34.416636: train_loss -0.8104 
2024-05-15 05:14:34.416797: val_loss -0.7924 
2024-05-15 05:14:34.416850: Pseudo dice [0.8592] 
2024-05-15 05:14:34.416902: Epoch time: 41.52 s 
2024-05-15 05:14:35.486746:  
2024-05-15 05:14:35.486882: Epoch 864 
2024-05-15 05:14:35.486974: Current learning rate: 0.00166 
2024-05-15 05:15:17.003537: train_loss -0.8003 
2024-05-15 05:15:17.003718: val_loss -0.7631 
2024-05-15 05:15:17.003777: Pseudo dice [0.8586] 
2024-05-15 05:15:17.003831: Epoch time: 41.52 s 
2024-05-15 05:15:18.047662:  
2024-05-15 05:15:18.047785: Epoch 865 
2024-05-15 05:15:18.047882: Current learning rate: 0.00165 
2024-05-15 05:15:59.562302: train_loss -0.8132 
2024-05-15 05:15:59.562479: val_loss -0.7789 
2024-05-15 05:15:59.562528: Pseudo dice [0.8459] 
2024-05-15 05:15:59.562580: Epoch time: 41.52 s 
2024-05-15 05:16:00.603203:  
2024-05-15 05:16:00.603410: Epoch 866 
2024-05-15 05:16:00.603512: Current learning rate: 0.00164 
2024-05-15 05:16:42.118697: train_loss -0.813 
2024-05-15 05:16:42.118866: val_loss -0.7845 
2024-05-15 05:16:42.118914: Pseudo dice [0.8692] 
2024-05-15 05:16:42.118966: Epoch time: 41.52 s 
2024-05-15 05:16:43.356747:  
2024-05-15 05:16:43.356949: Epoch 867 
2024-05-15 05:16:43.357042: Current learning rate: 0.00163 
2024-05-15 05:17:24.882711: train_loss -0.8103 
2024-05-15 05:17:24.882901: val_loss -0.7981 
2024-05-15 05:17:24.882953: Pseudo dice [0.8669] 
2024-05-15 05:17:24.883007: Epoch time: 41.53 s 
2024-05-15 05:17:25.927863:  
2024-05-15 05:17:25.927991: Epoch 868 
2024-05-15 05:17:25.928080: Current learning rate: 0.00162 
2024-05-15 05:18:07.454683: train_loss -0.8135 
2024-05-15 05:18:07.454854: val_loss -0.7915 
2024-05-15 05:18:07.454902: Pseudo dice [0.8561] 
2024-05-15 05:18:07.454956: Epoch time: 41.53 s 
2024-05-15 05:18:08.496102:  
2024-05-15 05:18:08.496240: Epoch 869 
2024-05-15 05:18:08.496334: Current learning rate: 0.00161 
2024-05-15 05:18:50.098821: train_loss -0.8046 
2024-05-15 05:18:50.098996: val_loss -0.7987 
2024-05-15 05:18:50.099046: Pseudo dice [0.8769] 
2024-05-15 05:18:50.099100: Epoch time: 41.6 s 
2024-05-15 05:18:50.099149: Yayy! New best EMA pseudo Dice: 0.8617 
2024-05-15 05:18:51.502443:  
2024-05-15 05:18:51.502580: Epoch 870 
2024-05-15 05:18:51.502675: Current learning rate: 0.00159 
2024-05-15 05:19:33.036002: train_loss -0.8064 
2024-05-15 05:19:33.036176: val_loss -0.7981 
2024-05-15 05:19:33.036223: Pseudo dice [0.8669] 
2024-05-15 05:19:33.036276: Epoch time: 41.53 s 
2024-05-15 05:19:33.036318: Yayy! New best EMA pseudo Dice: 0.8622 
2024-05-15 05:19:34.440047:  
2024-05-15 05:19:34.440238: Epoch 871 
2024-05-15 05:19:34.440331: Current learning rate: 0.00158 
2024-05-15 05:20:15.961329: train_loss -0.8099 
2024-05-15 05:20:15.961505: val_loss -0.7971 
2024-05-15 05:20:15.961553: Pseudo dice [0.8578] 
2024-05-15 05:20:15.961605: Epoch time: 41.52 s 
2024-05-15 05:20:17.015521:  
2024-05-15 05:20:17.015646: Epoch 872 
2024-05-15 05:20:17.015736: Current learning rate: 0.00157 
2024-05-15 05:20:58.539503: train_loss -0.8056 
2024-05-15 05:20:58.539672: val_loss -0.7879 
2024-05-15 05:20:58.539719: Pseudo dice [0.8585] 
2024-05-15 05:20:58.539772: Epoch time: 41.52 s 
2024-05-15 05:20:59.584716:  
2024-05-15 05:20:59.585051: Epoch 873 
2024-05-15 05:20:59.585205: Current learning rate: 0.00156 
2024-05-15 05:21:41.139173: train_loss -0.815 
2024-05-15 05:21:41.139347: val_loss -0.7833 
2024-05-15 05:21:41.139396: Pseudo dice [0.8567] 
2024-05-15 05:21:41.139449: Epoch time: 41.56 s 
2024-05-15 05:21:42.181051:  
2024-05-15 05:21:42.181288: Epoch 874 
2024-05-15 05:21:42.181382: Current learning rate: 0.00155 
2024-05-15 05:22:23.657263: train_loss -0.8076 
2024-05-15 05:22:23.657435: val_loss -0.7688 
2024-05-15 05:22:23.657485: Pseudo dice [0.8586] 
2024-05-15 05:22:23.657537: Epoch time: 41.48 s 
2024-05-15 05:22:24.706245:  
2024-05-15 05:22:24.706519: Epoch 875 
2024-05-15 05:22:24.706632: Current learning rate: 0.00154 
2024-05-15 05:23:06.177629: train_loss -0.8068 
2024-05-15 05:23:06.177799: val_loss -0.7883 
2024-05-15 05:23:06.177848: Pseudo dice [0.8598] 
2024-05-15 05:23:06.177900: Epoch time: 41.47 s 
2024-05-15 05:23:07.227153:  
2024-05-15 05:23:07.227453: Epoch 876 
2024-05-15 05:23:07.227547: Current learning rate: 0.00153 
2024-05-15 05:23:48.706458: train_loss -0.8115 
2024-05-15 05:23:48.706632: val_loss -0.7745 
2024-05-15 05:23:48.706679: Pseudo dice [0.8518] 
2024-05-15 05:23:48.706731: Epoch time: 41.48 s 
2024-05-15 05:23:49.756892:  
2024-05-15 05:23:49.757020: Epoch 877 
2024-05-15 05:23:49.757113: Current learning rate: 0.00152 
2024-05-15 05:24:31.319965: train_loss -0.8105 
2024-05-15 05:24:31.320141: val_loss -0.8037 
2024-05-15 05:24:31.320189: Pseudo dice [0.8573] 
2024-05-15 05:24:31.320241: Epoch time: 41.56 s 
2024-05-15 05:24:32.367913:  
2024-05-15 05:24:32.368150: Epoch 878 
2024-05-15 05:24:32.368246: Current learning rate: 0.00151 
2024-05-15 05:25:13.878074: train_loss -0.8105 
2024-05-15 05:25:13.878239: val_loss -0.7953 
2024-05-15 05:25:13.878286: Pseudo dice [0.8599] 
2024-05-15 05:25:13.878341: Epoch time: 41.51 s 
2024-05-15 05:25:14.927767:  
2024-05-15 05:25:14.927892: Epoch 879 
2024-05-15 05:25:14.927983: Current learning rate: 0.00149 
2024-05-15 05:25:56.440071: train_loss -0.8074 
2024-05-15 05:25:56.440246: val_loss -0.7632 
2024-05-15 05:25:56.440294: Pseudo dice [0.8503] 
2024-05-15 05:25:56.440346: Epoch time: 41.51 s 
2024-05-15 05:25:57.659327:  
2024-05-15 05:25:57.659478: Epoch 880 
2024-05-15 05:25:57.659569: Current learning rate: 0.00148 
2024-05-15 05:26:39.207641: train_loss -0.8072 
2024-05-15 05:26:39.207804: val_loss -0.7846 
2024-05-15 05:26:39.207854: Pseudo dice [0.8644] 
2024-05-15 05:26:39.207906: Epoch time: 41.55 s 
2024-05-15 05:26:40.259308:  
2024-05-15 05:26:40.259442: Epoch 881 
2024-05-15 05:26:40.259534: Current learning rate: 0.00147 
2024-05-15 05:27:21.765313: train_loss -0.7979 
2024-05-15 05:27:21.765472: val_loss -0.8017 
2024-05-15 05:27:21.765520: Pseudo dice [0.8663] 
2024-05-15 05:27:21.765575: Epoch time: 41.51 s 
2024-05-15 05:27:22.819072:  
2024-05-15 05:27:22.819203: Epoch 882 
2024-05-15 05:27:22.819293: Current learning rate: 0.00146 
2024-05-15 05:28:04.351408: train_loss -0.7999 
2024-05-15 05:28:04.351581: val_loss -0.7879 
2024-05-15 05:28:04.351630: Pseudo dice [0.861] 
2024-05-15 05:28:04.351683: Epoch time: 41.53 s 
2024-05-15 05:28:05.401598:  
2024-05-15 05:28:05.401729: Epoch 883 
2024-05-15 05:28:05.401822: Current learning rate: 0.00145 
2024-05-15 05:28:46.896570: train_loss -0.7955 
2024-05-15 05:28:46.896741: val_loss -0.7956 
2024-05-15 05:28:46.896790: Pseudo dice [0.8671] 
2024-05-15 05:28:46.896842: Epoch time: 41.5 s 
2024-05-15 05:28:47.961483:  
2024-05-15 05:28:47.961690: Epoch 884 
2024-05-15 05:28:47.961785: Current learning rate: 0.00144 
2024-05-15 05:29:29.435498: train_loss -0.802 
2024-05-15 05:29:29.435812: val_loss -0.7889 
2024-05-15 05:29:29.435866: Pseudo dice [0.8672] 
2024-05-15 05:29:29.435919: Epoch time: 41.48 s 
2024-05-15 05:29:30.486072:  
2024-05-15 05:29:30.486199: Epoch 885 
2024-05-15 05:29:30.486290: Current learning rate: 0.00143 
2024-05-15 05:30:11.964246: train_loss -0.805 
2024-05-15 05:30:11.964417: val_loss -0.7713 
2024-05-15 05:30:11.964465: Pseudo dice [0.857] 
2024-05-15 05:30:11.964517: Epoch time: 41.48 s 
2024-05-15 05:30:13.009734:  
2024-05-15 05:30:13.010055: Epoch 886 
2024-05-15 05:30:13.010145: Current learning rate: 0.00142 
2024-05-15 05:30:54.484659: train_loss -0.8106 
2024-05-15 05:30:54.484832: val_loss -0.8035 
2024-05-15 05:30:54.484880: Pseudo dice [0.8622] 
2024-05-15 05:30:54.484933: Epoch time: 41.48 s 
2024-05-15 05:30:55.531248:  
2024-05-15 05:30:55.531567: Epoch 887 
2024-05-15 05:30:55.531663: Current learning rate: 0.00141 
2024-05-15 05:31:36.988529: train_loss -0.8098 
2024-05-15 05:31:36.988688: val_loss -0.783 
2024-05-15 05:31:36.988737: Pseudo dice [0.8597] 
2024-05-15 05:31:36.988790: Epoch time: 41.46 s 
2024-05-15 05:31:38.232182:  
2024-05-15 05:31:38.232318: Epoch 888 
2024-05-15 05:31:38.232407: Current learning rate: 0.00139 
2024-05-15 05:32:19.754145: train_loss -0.8009 
2024-05-15 05:32:19.754319: val_loss -0.7664 
2024-05-15 05:32:19.754366: Pseudo dice [0.8531] 
2024-05-15 05:32:19.754419: Epoch time: 41.52 s 
2024-05-15 05:32:20.803026:  
2024-05-15 05:32:20.803237: Epoch 889 
2024-05-15 05:32:20.803357: Current learning rate: 0.00138 
2024-05-15 05:33:02.308548: train_loss -0.8059 
2024-05-15 05:33:02.308713: val_loss -0.785 
2024-05-15 05:33:02.308771: Pseudo dice [0.8558] 
2024-05-15 05:33:02.308823: Epoch time: 41.51 s 
2024-05-15 05:33:03.350694:  
2024-05-15 05:33:03.350832: Epoch 890 
2024-05-15 05:33:03.350931: Current learning rate: 0.00137 
2024-05-15 05:33:44.857521: train_loss -0.8088 
2024-05-15 05:33:44.857692: val_loss -0.7455 
2024-05-15 05:33:44.857739: Pseudo dice [0.8584] 
2024-05-15 05:33:44.857794: Epoch time: 41.51 s 
2024-05-15 05:33:45.902366:  
2024-05-15 05:33:45.902647: Epoch 891 
2024-05-15 05:33:45.902760: Current learning rate: 0.00136 
2024-05-15 05:34:27.425748: train_loss -0.8186 
2024-05-15 05:34:27.425919: val_loss -0.7912 
2024-05-15 05:34:27.425967: Pseudo dice [0.8683] 
2024-05-15 05:34:27.426019: Epoch time: 41.52 s 
2024-05-15 05:34:28.475437:  
2024-05-15 05:34:28.475551: Epoch 892 
2024-05-15 05:34:28.475648: Current learning rate: 0.00135 
2024-05-15 05:35:09.997288: train_loss -0.8183 
2024-05-15 05:35:09.997463: val_loss -0.794 
2024-05-15 05:35:09.997511: Pseudo dice [0.86] 
2024-05-15 05:35:09.997563: Epoch time: 41.52 s 
2024-05-15 05:35:11.036448:  
2024-05-15 05:35:11.036571: Epoch 893 
2024-05-15 05:35:11.036660: Current learning rate: 0.00134 
2024-05-15 05:35:52.544384: train_loss -0.8142 
2024-05-15 05:35:52.544559: val_loss -0.7997 
2024-05-15 05:35:52.544607: Pseudo dice [0.8639] 
2024-05-15 05:35:52.544660: Epoch time: 41.51 s 
2024-05-15 05:35:53.578358:  
2024-05-15 05:35:53.578490: Epoch 894 
2024-05-15 05:35:53.578583: Current learning rate: 0.00133 
2024-05-15 05:36:35.095768: train_loss -0.8177 
2024-05-15 05:36:35.095944: val_loss -0.7631 
2024-05-15 05:36:35.095992: Pseudo dice [0.8598] 
2024-05-15 05:36:35.096045: Epoch time: 41.52 s 
2024-05-15 05:36:36.343212:  
2024-05-15 05:36:36.343426: Epoch 895 
2024-05-15 05:36:36.343518: Current learning rate: 0.00132 
2024-05-15 05:37:17.877770: train_loss -0.8122 
2024-05-15 05:37:17.877973: val_loss -0.7557 
2024-05-15 05:37:17.878022: Pseudo dice [0.8654] 
2024-05-15 05:37:17.878074: Epoch time: 41.54 s 
2024-05-15 05:37:18.924977:  
2024-05-15 05:37:18.925324: Epoch 896 
2024-05-15 05:37:18.925420: Current learning rate: 0.0013 
2024-05-15 05:38:00.436657: train_loss -0.8095 
2024-05-15 05:38:00.436842: val_loss -0.7819 
2024-05-15 05:38:00.436890: Pseudo dice [0.8655] 
2024-05-15 05:38:00.436943: Epoch time: 41.51 s 
2024-05-15 05:38:01.490146:  
2024-05-15 05:38:01.490386: Epoch 897 
2024-05-15 05:38:01.490489: Current learning rate: 0.00129 
2024-05-15 05:38:43.000586: train_loss -0.8166 
2024-05-15 05:38:43.000761: val_loss -0.8058 
2024-05-15 05:38:43.000808: Pseudo dice [0.8693] 
2024-05-15 05:38:43.000861: Epoch time: 41.51 s 
2024-05-15 05:38:43.000907: Yayy! New best EMA pseudo Dice: 0.8624 
2024-05-15 05:38:44.463942:  
2024-05-15 05:38:44.464081: Epoch 898 
2024-05-15 05:38:44.464171: Current learning rate: 0.00128 
2024-05-15 05:39:25.970701: train_loss -0.8105 
2024-05-15 05:39:25.970888: val_loss -0.774 
2024-05-15 05:39:25.970935: Pseudo dice [0.854] 
2024-05-15 05:39:25.970988: Epoch time: 41.51 s 
2024-05-15 05:39:27.010053:  
2024-05-15 05:39:27.010184: Epoch 899 
2024-05-15 05:39:27.010276: Current learning rate: 0.00127 
2024-05-15 05:40:08.523236: train_loss -0.8088 
2024-05-15 05:40:08.523409: val_loss -0.7752 
2024-05-15 05:40:08.523458: Pseudo dice [0.8563] 
2024-05-15 05:40:08.523516: Epoch time: 41.51 s 
2024-05-15 05:40:09.930316:  
2024-05-15 05:40:09.930527: Epoch 900 
2024-05-15 05:40:09.930630: Current learning rate: 0.00126 
2024-05-15 05:40:51.443447: train_loss -0.8182 
2024-05-15 05:40:51.443616: val_loss -0.7794 
2024-05-15 05:40:51.443664: Pseudo dice [0.8628] 
2024-05-15 05:40:51.443717: Epoch time: 41.51 s 
2024-05-15 05:40:52.492648:  
2024-05-15 05:40:52.492870: Epoch 901 
2024-05-15 05:40:52.493005: Current learning rate: 0.00125 
2024-05-15 05:41:33.993181: train_loss -0.8117 
2024-05-15 05:41:33.993353: val_loss -0.7983 
2024-05-15 05:41:33.993400: Pseudo dice [0.8639] 
2024-05-15 05:41:33.993455: Epoch time: 41.5 s 
2024-05-15 05:41:35.236747:  
2024-05-15 05:41:35.236890: Epoch 902 
2024-05-15 05:41:35.236976: Current learning rate: 0.00124 
2024-05-15 05:42:16.783172: train_loss -0.8157 
2024-05-15 05:42:16.783344: val_loss -0.7837 
2024-05-15 05:42:16.783392: Pseudo dice [0.8513] 
2024-05-15 05:42:16.783446: Epoch time: 41.55 s 
2024-05-15 05:42:17.828083:  
2024-05-15 05:42:17.828329: Epoch 903 
2024-05-15 05:42:17.828526: Current learning rate: 0.00122 
2024-05-15 05:42:59.365579: train_loss -0.8113 
2024-05-15 05:42:59.365754: val_loss -0.7898 
2024-05-15 05:42:59.365928: Pseudo dice [0.8709] 
2024-05-15 05:42:59.365981: Epoch time: 41.54 s 
2024-05-15 05:43:00.409079:  
2024-05-15 05:43:00.409255: Epoch 904 
2024-05-15 05:43:00.409348: Current learning rate: 0.00121 
2024-05-15 05:43:41.951130: train_loss -0.8253 
2024-05-15 05:43:41.951294: val_loss -0.7897 
2024-05-15 05:43:41.951343: Pseudo dice [0.8656] 
2024-05-15 05:43:41.951396: Epoch time: 41.54 s 
2024-05-15 05:43:42.992384:  
2024-05-15 05:43:42.992567: Epoch 905 
2024-05-15 05:43:42.992663: Current learning rate: 0.0012 
2024-05-15 05:44:24.528714: train_loss -0.8092 
2024-05-15 05:44:24.528887: val_loss -0.8087 
2024-05-15 05:44:24.528937: Pseudo dice [0.8734] 
2024-05-15 05:44:24.528990: Epoch time: 41.54 s 
2024-05-15 05:44:24.529036: Yayy! New best EMA pseudo Dice: 0.863 
2024-05-15 05:44:25.949789:  
2024-05-15 05:44:25.949952: Epoch 906 
2024-05-15 05:44:25.950044: Current learning rate: 0.00119 
2024-05-15 05:45:07.470607: train_loss -0.8134 
2024-05-15 05:45:07.470790: val_loss -0.8001 
2024-05-15 05:45:07.470839: Pseudo dice [0.8572] 
2024-05-15 05:45:07.470894: Epoch time: 41.52 s 
2024-05-15 05:45:08.503866:  
2024-05-15 05:45:08.503989: Epoch 907 
2024-05-15 05:45:08.504079: Current learning rate: 0.00118 
2024-05-15 05:45:50.005780: train_loss -0.8124 
2024-05-15 05:45:50.005958: val_loss -0.7749 
2024-05-15 05:45:50.006015: Pseudo dice [0.8745] 
2024-05-15 05:45:50.006077: Epoch time: 41.5 s 
2024-05-15 05:45:50.006122: Yayy! New best EMA pseudo Dice: 0.8637 
2024-05-15 05:45:51.579540:  
2024-05-15 05:45:51.579699: Epoch 908 
2024-05-15 05:45:51.579794: Current learning rate: 0.00117 
2024-05-15 05:46:33.073166: train_loss -0.81 
2024-05-15 05:46:33.073343: val_loss -0.7792 
2024-05-15 05:46:33.073392: Pseudo dice [0.8601] 
2024-05-15 05:46:33.073445: Epoch time: 41.49 s 
2024-05-15 05:46:34.116880:  
2024-05-15 05:46:34.117030: Epoch 909 
2024-05-15 05:46:34.117131: Current learning rate: 0.00116 
2024-05-15 05:47:15.622891: train_loss -0.8117 
2024-05-15 05:47:15.623064: val_loss -0.7865 
2024-05-15 05:47:15.623112: Pseudo dice [0.8653] 
2024-05-15 05:47:15.623164: Epoch time: 41.51 s 
2024-05-15 05:47:16.667042:  
2024-05-15 05:47:16.667180: Epoch 910 
2024-05-15 05:47:16.667267: Current learning rate: 0.00115 
2024-05-15 05:47:58.165070: train_loss -0.8191 
2024-05-15 05:47:58.165239: val_loss -0.7659 
2024-05-15 05:47:58.165286: Pseudo dice [0.8562] 
2024-05-15 05:47:58.165342: Epoch time: 41.5 s 
2024-05-15 05:47:59.207731:  
2024-05-15 05:47:59.207867: Epoch 911 
2024-05-15 05:47:59.207965: Current learning rate: 0.00113 
2024-05-15 05:48:40.698910: train_loss -0.8203 
2024-05-15 05:48:40.699081: val_loss -0.7759 
2024-05-15 05:48:40.699129: Pseudo dice [0.8579] 
2024-05-15 05:48:40.699181: Epoch time: 41.49 s 
2024-05-15 05:48:41.732504:  
2024-05-15 05:48:41.732644: Epoch 912 
2024-05-15 05:48:41.732733: Current learning rate: 0.00112 
2024-05-15 05:49:23.215719: train_loss -0.8097 
2024-05-15 05:49:23.215895: val_loss -0.7962 
2024-05-15 05:49:23.215942: Pseudo dice [0.8649] 
2024-05-15 05:49:23.215994: Epoch time: 41.48 s 
2024-05-15 05:49:24.256240:  
2024-05-15 05:49:24.256364: Epoch 913 
2024-05-15 05:49:24.256455: Current learning rate: 0.00111 
2024-05-15 05:50:05.705005: train_loss -0.8196 
2024-05-15 05:50:05.705179: val_loss -0.7784 
2024-05-15 05:50:05.705227: Pseudo dice [0.8615] 
2024-05-15 05:50:05.705280: Epoch time: 41.45 s 
2024-05-15 05:50:06.740186:  
2024-05-15 05:50:06.740313: Epoch 914 
2024-05-15 05:50:06.740404: Current learning rate: 0.0011 
2024-05-15 05:50:48.229889: train_loss -0.8151 
2024-05-15 05:50:48.230063: val_loss -0.8064 
2024-05-15 05:50:48.230112: Pseudo dice [0.8642] 
2024-05-15 05:50:48.230165: Epoch time: 41.49 s 
2024-05-15 05:50:49.263703:  
2024-05-15 05:50:49.263829: Epoch 915 
2024-05-15 05:50:49.263924: Current learning rate: 0.00109 
2024-05-15 05:51:30.790917: train_loss -0.8064 
2024-05-15 05:51:30.791092: val_loss -0.7945 
2024-05-15 05:51:30.791140: Pseudo dice [0.8681] 
2024-05-15 05:51:30.791195: Epoch time: 41.53 s 
2024-05-15 05:51:32.037791:  
2024-05-15 05:51:32.037933: Epoch 916 
2024-05-15 05:51:32.038029: Current learning rate: 0.00108 
2024-05-15 05:52:13.587408: train_loss -0.8131 
2024-05-15 05:52:13.587579: val_loss -0.7973 
2024-05-15 05:52:13.587629: Pseudo dice [0.8574] 
2024-05-15 05:52:13.587684: Epoch time: 41.55 s 
2024-05-15 05:52:14.621687:  
2024-05-15 05:52:14.621827: Epoch 917 
2024-05-15 05:52:14.621913: Current learning rate: 0.00106 
2024-05-15 05:52:56.165615: train_loss -0.8142 
2024-05-15 05:52:56.165795: val_loss -0.7846 
2024-05-15 05:52:56.165843: Pseudo dice [0.8636] 
2024-05-15 05:52:56.165899: Epoch time: 41.54 s 
2024-05-15 05:52:57.207347:  
2024-05-15 05:52:57.207480: Epoch 918 
2024-05-15 05:52:57.207580: Current learning rate: 0.00105 
2024-05-15 05:53:38.812709: train_loss -0.7991 
2024-05-15 05:53:38.812883: val_loss -0.7771 
2024-05-15 05:53:38.812932: Pseudo dice [0.8562] 
2024-05-15 05:53:38.812986: Epoch time: 41.61 s 
2024-05-15 05:53:39.860468:  
2024-05-15 05:53:39.860820: Epoch 919 
2024-05-15 05:53:39.860915: Current learning rate: 0.00104 
2024-05-15 05:54:21.329787: train_loss -0.8113 
2024-05-15 05:54:21.329961: val_loss -0.7902 
2024-05-15 05:54:21.330009: Pseudo dice [0.864] 
2024-05-15 05:54:21.330062: Epoch time: 41.47 s 
2024-05-15 05:54:22.371537:  
2024-05-15 05:54:22.371669: Epoch 920 
2024-05-15 05:54:22.371756: Current learning rate: 0.00103 
2024-05-15 05:55:03.838415: train_loss -0.8146 
2024-05-15 05:55:03.838590: val_loss -0.7953 
2024-05-15 05:55:03.838639: Pseudo dice [0.8632] 
2024-05-15 05:55:03.838693: Epoch time: 41.47 s 
2024-05-15 05:55:04.890815:  
2024-05-15 05:55:04.891001: Epoch 921 
2024-05-15 05:55:04.891142: Current learning rate: 0.00102 
2024-05-15 05:55:46.347089: train_loss -0.8214 
2024-05-15 05:55:46.347270: val_loss -0.7812 
2024-05-15 05:55:46.347319: Pseudo dice [0.8701] 
2024-05-15 05:55:46.347372: Epoch time: 41.46 s 
2024-05-15 05:55:47.380822:  
2024-05-15 05:55:47.380949: Epoch 922 
2024-05-15 05:55:47.381041: Current learning rate: 0.00101 
2024-05-15 05:56:28.849623: train_loss -0.8196 
2024-05-15 05:56:28.849809: val_loss -0.7789 
2024-05-15 05:56:28.849858: Pseudo dice [0.8772] 
2024-05-15 05:56:28.849912: Epoch time: 41.47 s 
2024-05-15 05:56:28.849956: Yayy! New best EMA pseudo Dice: 0.8645 
2024-05-15 05:56:30.428347:  
2024-05-15 05:56:30.428494: Epoch 923 
2024-05-15 05:56:30.428589: Current learning rate: 0.001 
2024-05-15 05:57:11.940167: train_loss -0.8198 
2024-05-15 05:57:11.940351: val_loss -0.8083 
2024-05-15 05:57:11.940400: Pseudo dice [0.8621] 
2024-05-15 05:57:11.940453: Epoch time: 41.51 s 
2024-05-15 05:57:12.972096:  
2024-05-15 05:57:12.972229: Epoch 924 
2024-05-15 05:57:12.972321: Current learning rate: 0.00098 
2024-05-15 05:57:54.497261: train_loss -0.8176 
2024-05-15 05:57:54.497428: val_loss -0.8056 
2024-05-15 05:57:54.497478: Pseudo dice [0.868] 
2024-05-15 05:57:54.497531: Epoch time: 41.53 s 
2024-05-15 05:57:54.497576: Yayy! New best EMA pseudo Dice: 0.8647 
2024-05-15 05:57:55.893932:  
2024-05-15 05:57:55.894058: Epoch 925 
2024-05-15 05:57:55.894144: Current learning rate: 0.00097 
2024-05-15 05:58:37.403313: train_loss -0.8116 
2024-05-15 05:58:37.403487: val_loss -0.7832 
2024-05-15 05:58:37.403535: Pseudo dice [0.8558] 
2024-05-15 05:58:37.403588: Epoch time: 41.51 s 
2024-05-15 05:58:38.439906:  
2024-05-15 05:58:38.440039: Epoch 926 
2024-05-15 05:58:38.440130: Current learning rate: 0.00096 
2024-05-15 05:59:19.938230: train_loss -0.8139 
2024-05-15 05:59:19.938435: val_loss -0.7768 
2024-05-15 05:59:19.938489: Pseudo dice [0.8684] 
2024-05-15 05:59:19.938554: Epoch time: 41.5 s 
2024-05-15 05:59:20.976330:  
2024-05-15 05:59:20.976457: Epoch 927 
2024-05-15 05:59:20.976546: Current learning rate: 0.00095 
2024-05-15 06:00:02.476123: train_loss -0.816 
2024-05-15 06:00:02.476311: val_loss -0.8078 
2024-05-15 06:00:02.476361: Pseudo dice [0.8715] 
2024-05-15 06:00:02.476415: Epoch time: 41.5 s 
2024-05-15 06:00:02.476459: Yayy! New best EMA pseudo Dice: 0.865 
2024-05-15 06:00:03.874781:  
2024-05-15 06:00:03.874909: Epoch 928 
2024-05-15 06:00:03.875003: Current learning rate: 0.00094 
2024-05-15 06:00:45.391052: train_loss -0.8284 
2024-05-15 06:00:45.391243: val_loss -0.7908 
2024-05-15 06:00:45.391292: Pseudo dice [0.868] 
2024-05-15 06:00:45.391347: Epoch time: 41.52 s 
2024-05-15 06:00:45.391393: Yayy! New best EMA pseudo Dice: 0.8653 
2024-05-15 06:00:46.791055:  
2024-05-15 06:00:46.791178: Epoch 929 
2024-05-15 06:00:46.791274: Current learning rate: 0.00092 
2024-05-15 06:01:28.308815: train_loss -0.8174 
2024-05-15 06:01:28.308999: val_loss -0.79 
2024-05-15 06:01:28.309048: Pseudo dice [0.8574] 
2024-05-15 06:01:28.309104: Epoch time: 41.52 s 
2024-05-15 06:01:29.557018:  
2024-05-15 06:01:29.557161: Epoch 930 
2024-05-15 06:01:29.557251: Current learning rate: 0.00091 
2024-05-15 06:02:11.055878: train_loss -0.8166 
2024-05-15 06:02:11.056054: val_loss -0.7881 
2024-05-15 06:02:11.056102: Pseudo dice [0.8661] 
2024-05-15 06:02:11.056156: Epoch time: 41.5 s 
2024-05-15 06:02:12.091956:  
2024-05-15 06:02:12.092081: Epoch 931 
2024-05-15 06:02:12.092174: Current learning rate: 0.0009 
2024-05-15 06:02:53.627193: train_loss -0.8184 
2024-05-15 06:02:53.627365: val_loss -0.7951 
2024-05-15 06:02:53.627412: Pseudo dice [0.869] 
2024-05-15 06:02:53.627465: Epoch time: 41.54 s 
2024-05-15 06:02:54.673886:  
2024-05-15 06:02:54.674027: Epoch 932 
2024-05-15 06:02:54.674117: Current learning rate: 0.00089 
2024-05-15 06:03:36.208345: train_loss -0.8254 
2024-05-15 06:03:36.208514: val_loss -0.7883 
2024-05-15 06:03:36.208563: Pseudo dice [0.863] 
2024-05-15 06:03:36.208615: Epoch time: 41.54 s 
2024-05-15 06:03:37.252181:  
2024-05-15 06:03:37.252371: Epoch 933 
2024-05-15 06:03:37.252464: Current learning rate: 0.00088 
2024-05-15 06:04:18.790568: train_loss -0.8085 
2024-05-15 06:04:18.790740: val_loss -0.7883 
2024-05-15 06:04:18.790788: Pseudo dice [0.8587] 
2024-05-15 06:04:18.790868: Epoch time: 41.54 s 
2024-05-15 06:04:19.847396:  
2024-05-15 06:04:19.847526: Epoch 934 
2024-05-15 06:04:19.847621: Current learning rate: 0.00087 
2024-05-15 06:05:01.393659: train_loss -0.8181 
2024-05-15 06:05:01.393837: val_loss -0.7962 
2024-05-15 06:05:01.393886: Pseudo dice [0.8621] 
2024-05-15 06:05:01.393942: Epoch time: 41.55 s 
2024-05-15 06:05:02.427253:  
2024-05-15 06:05:02.427378: Epoch 935 
2024-05-15 06:05:02.427473: Current learning rate: 0.00085 
2024-05-15 06:05:43.905324: train_loss -0.8153 
2024-05-15 06:05:43.905496: val_loss -0.8087 
2024-05-15 06:05:43.905545: Pseudo dice [0.8633] 
2024-05-15 06:05:43.905597: Epoch time: 41.48 s 
2024-05-15 06:05:44.942415:  
2024-05-15 06:05:44.942544: Epoch 936 
2024-05-15 06:05:44.942633: Current learning rate: 0.00084 
2024-05-15 06:06:26.410618: train_loss -0.8051 
2024-05-15 06:06:26.410790: val_loss -0.7748 
2024-05-15 06:06:26.410838: Pseudo dice [0.8607] 
2024-05-15 06:06:26.410891: Epoch time: 41.47 s 
2024-05-15 06:06:27.648013:  
2024-05-15 06:06:27.648166: Epoch 937 
2024-05-15 06:06:27.648260: Current learning rate: 0.00083 
2024-05-15 06:07:09.130188: train_loss -0.8142 
2024-05-15 06:07:09.130367: val_loss -0.7778 
2024-05-15 06:07:09.130414: Pseudo dice [0.858] 
2024-05-15 06:07:09.130555: Epoch time: 41.48 s 
2024-05-15 06:07:10.173196:  
2024-05-15 06:07:10.173447: Epoch 938 
2024-05-15 06:07:10.173617: Current learning rate: 0.00082 
2024-05-15 06:07:51.682750: train_loss -0.8093 
2024-05-15 06:07:51.682921: val_loss -0.79 
2024-05-15 06:07:51.682969: Pseudo dice [0.8662] 
2024-05-15 06:07:51.683020: Epoch time: 41.51 s 
2024-05-15 06:07:52.729269:  
2024-05-15 06:07:52.729468: Epoch 939 
2024-05-15 06:07:52.729587: Current learning rate: 0.00081 
2024-05-15 06:08:34.235510: train_loss -0.8115 
2024-05-15 06:08:34.235679: val_loss -0.8013 
2024-05-15 06:08:34.235728: Pseudo dice [0.8727] 
2024-05-15 06:08:34.235781: Epoch time: 41.51 s 
2024-05-15 06:08:35.280822:  
2024-05-15 06:08:35.281100: Epoch 940 
2024-05-15 06:08:35.281281: Current learning rate: 0.00079 
2024-05-15 06:09:16.819971: train_loss -0.8099 
2024-05-15 06:09:16.820145: val_loss -0.7894 
2024-05-15 06:09:16.820194: Pseudo dice [0.8677] 
2024-05-15 06:09:16.820246: Epoch time: 41.54 s 
2024-05-15 06:09:17.862756:  
2024-05-15 06:09:17.863195: Epoch 941 
2024-05-15 06:09:17.863291: Current learning rate: 0.00078 
2024-05-15 06:09:59.377499: train_loss -0.8014 
2024-05-15 06:09:59.377772: val_loss -0.8094 
2024-05-15 06:09:59.377883: Pseudo dice [0.855] 
2024-05-15 06:09:59.377938: Epoch time: 41.52 s 
2024-05-15 06:10:00.416856:  
2024-05-15 06:10:00.417069: Epoch 942 
2024-05-15 06:10:00.417202: Current learning rate: 0.00077 
2024-05-15 06:10:41.937509: train_loss -0.8099 
2024-05-15 06:10:41.937682: val_loss -0.79 
2024-05-15 06:10:41.937728: Pseudo dice [0.8679] 
2024-05-15 06:10:41.937780: Epoch time: 41.52 s 
2024-05-15 06:10:42.971568:  
2024-05-15 06:10:42.971693: Epoch 943 
2024-05-15 06:10:42.971784: Current learning rate: 0.00076 
2024-05-15 06:11:24.470984: train_loss -0.8226 
2024-05-15 06:11:24.471155: val_loss -0.7842 
2024-05-15 06:11:24.471202: Pseudo dice [0.8577] 
2024-05-15 06:11:24.471255: Epoch time: 41.5 s 
2024-05-15 06:11:25.701425:  
2024-05-15 06:11:25.701563: Epoch 944 
2024-05-15 06:11:25.701664: Current learning rate: 0.00075 
2024-05-15 06:12:07.191685: train_loss -0.8079 
2024-05-15 06:12:07.191869: val_loss -0.7949 
2024-05-15 06:12:07.191918: Pseudo dice [0.8763] 
2024-05-15 06:12:07.191972: Epoch time: 41.49 s 
2024-05-15 06:12:08.235867:  
2024-05-15 06:12:08.236151: Epoch 945 
2024-05-15 06:12:08.236244: Current learning rate: 0.00074 
2024-05-15 06:12:49.737874: train_loss -0.8164 
2024-05-15 06:12:49.738045: val_loss -0.7991 
2024-05-15 06:12:49.738100: Pseudo dice [0.8745] 
2024-05-15 06:12:49.738154: Epoch time: 41.5 s 
2024-05-15 06:12:49.738197: Yayy! New best EMA pseudo Dice: 0.8657 
2024-05-15 06:12:51.130302:  
2024-05-15 06:12:51.130438: Epoch 946 
2024-05-15 06:12:51.130527: Current learning rate: 0.00072 
2024-05-15 06:13:32.661052: train_loss -0.8167 
2024-05-15 06:13:32.661223: val_loss -0.8021 
2024-05-15 06:13:32.661273: Pseudo dice [0.868] 
2024-05-15 06:13:32.661327: Epoch time: 41.53 s 
2024-05-15 06:13:32.661370: Yayy! New best EMA pseudo Dice: 0.866 
2024-05-15 06:13:34.062804:  
2024-05-15 06:13:34.062947: Epoch 947 
2024-05-15 06:13:34.063035: Current learning rate: 0.00071 
2024-05-15 06:14:15.587969: train_loss -0.806 
2024-05-15 06:14:15.588143: val_loss -0.8033 
2024-05-15 06:14:15.588192: Pseudo dice [0.8683] 
2024-05-15 06:14:15.588246: Epoch time: 41.53 s 
2024-05-15 06:14:15.588289: Yayy! New best EMA pseudo Dice: 0.8662 
2024-05-15 06:14:16.991572:  
2024-05-15 06:14:16.991703: Epoch 948 
2024-05-15 06:14:16.991790: Current learning rate: 0.0007 
2024-05-15 06:14:58.511327: train_loss -0.8136 
2024-05-15 06:14:58.511504: val_loss -0.7791 
2024-05-15 06:14:58.511551: Pseudo dice [0.862] 
2024-05-15 06:14:58.511604: Epoch time: 41.52 s 
2024-05-15 06:14:59.562565:  
2024-05-15 06:14:59.562687: Epoch 949 
2024-05-15 06:14:59.562777: Current learning rate: 0.00069 
2024-05-15 06:15:41.091916: train_loss -0.819 
2024-05-15 06:15:41.092090: val_loss -0.8189 
2024-05-15 06:15:41.092141: Pseudo dice [0.8815] 
2024-05-15 06:15:41.092196: Epoch time: 41.53 s 
2024-05-15 06:15:41.454543: Yayy! New best EMA pseudo Dice: 0.8673 
2024-05-15 06:15:42.842909:  
2024-05-15 06:15:42.843036: Epoch 950 
2024-05-15 06:15:42.843127: Current learning rate: 0.00067 
2024-05-15 06:16:24.362730: train_loss -0.8206 
2024-05-15 06:16:24.362908: val_loss -0.7837 
2024-05-15 06:16:24.362967: Pseudo dice [0.8625] 
2024-05-15 06:16:24.363031: Epoch time: 41.52 s 
2024-05-15 06:16:25.598391:  
2024-05-15 06:16:25.598540: Epoch 951 
2024-05-15 06:16:25.598638: Current learning rate: 0.00066 
2024-05-15 06:17:07.143303: train_loss -0.8186 
2024-05-15 06:17:07.143480: val_loss -0.8061 
2024-05-15 06:17:07.143529: Pseudo dice [0.8652] 
2024-05-15 06:17:07.143584: Epoch time: 41.55 s 
2024-05-15 06:17:08.193439:  
2024-05-15 06:17:08.193627: Epoch 952 
2024-05-15 06:17:08.193719: Current learning rate: 0.00065 
2024-05-15 06:17:49.738380: train_loss -0.8182 
2024-05-15 06:17:49.738562: val_loss -0.8109 
2024-05-15 06:17:49.738611: Pseudo dice [0.8666] 
2024-05-15 06:17:49.738665: Epoch time: 41.55 s 
2024-05-15 06:17:50.829056:  
2024-05-15 06:17:50.829233: Epoch 953 
2024-05-15 06:17:50.829327: Current learning rate: 0.00064 
2024-05-15 06:18:32.321204: train_loss -0.8177 
2024-05-15 06:18:32.321381: val_loss -0.7824 
2024-05-15 06:18:32.321430: Pseudo dice [0.8681] 
2024-05-15 06:18:32.321484: Epoch time: 41.49 s 
2024-05-15 06:18:33.372127:  
2024-05-15 06:18:33.372310: Epoch 954 
2024-05-15 06:18:33.372404: Current learning rate: 0.00063 
2024-05-15 06:19:14.871081: train_loss -0.8176 
2024-05-15 06:19:14.871249: val_loss -0.8109 
2024-05-15 06:19:14.871297: Pseudo dice [0.8752] 
2024-05-15 06:19:14.871351: Epoch time: 41.5 s 
2024-05-15 06:19:14.871393: Yayy! New best EMA pseudo Dice: 0.8677 
2024-05-15 06:19:16.290852:  
2024-05-15 06:19:16.290983: Epoch 955 
2024-05-15 06:19:16.291070: Current learning rate: 0.00061 
2024-05-15 06:19:57.788229: train_loss -0.8222 
2024-05-15 06:19:57.788401: val_loss -0.8005 
2024-05-15 06:19:57.788448: Pseudo dice [0.8634] 
2024-05-15 06:19:57.788500: Epoch time: 41.5 s 
2024-05-15 06:19:58.852311:  
2024-05-15 06:19:58.852420: Epoch 956 
2024-05-15 06:19:58.852514: Current learning rate: 0.0006 
2024-05-15 06:20:40.362789: train_loss -0.8233 
2024-05-15 06:20:40.362966: val_loss -0.7846 
2024-05-15 06:20:40.363014: Pseudo dice [0.8555] 
2024-05-15 06:20:40.363066: Epoch time: 41.51 s 
2024-05-15 06:20:41.426517:  
2024-05-15 06:20:41.426642: Epoch 957 
2024-05-15 06:20:41.426732: Current learning rate: 0.00059 
2024-05-15 06:21:22.937906: train_loss -0.8109 
2024-05-15 06:21:22.938076: val_loss -0.7709 
2024-05-15 06:21:22.938123: Pseudo dice [0.8654] 
2024-05-15 06:21:22.938177: Epoch time: 41.51 s 
2024-05-15 06:21:24.189513:  
2024-05-15 06:21:24.189686: Epoch 958 
2024-05-15 06:21:24.189779: Current learning rate: 0.00058 
2024-05-15 06:22:05.707449: train_loss -0.822 
2024-05-15 06:22:05.707615: val_loss -0.7915 
2024-05-15 06:22:05.707662: Pseudo dice [0.8641] 
2024-05-15 06:22:05.707715: Epoch time: 41.52 s 
2024-05-15 06:22:06.762487:  
2024-05-15 06:22:06.762617: Epoch 959 
2024-05-15 06:22:06.762710: Current learning rate: 0.00056 
2024-05-15 06:22:48.260441: train_loss -0.8271 
2024-05-15 06:22:48.260629: val_loss -0.7946 
2024-05-15 06:22:48.260678: Pseudo dice [0.8682] 
2024-05-15 06:22:48.260732: Epoch time: 41.5 s 
2024-05-15 06:22:49.311823:  
2024-05-15 06:22:49.311983: Epoch 960 
2024-05-15 06:22:49.312073: Current learning rate: 0.00055 
2024-05-15 06:23:30.825244: train_loss -0.8211 
2024-05-15 06:23:30.825432: val_loss -0.7852 
2024-05-15 06:23:30.825480: Pseudo dice [0.8686] 
2024-05-15 06:23:30.825534: Epoch time: 41.51 s 
2024-05-15 06:23:31.878236:  
2024-05-15 06:23:31.878409: Epoch 961 
2024-05-15 06:23:31.878507: Current learning rate: 0.00054 
2024-05-15 06:24:13.405091: train_loss -0.8176 
2024-05-15 06:24:13.405284: val_loss -0.7985 
2024-05-15 06:24:13.405333: Pseudo dice [0.8713] 
2024-05-15 06:24:13.405388: Epoch time: 41.53 s 
2024-05-15 06:24:14.468353:  
2024-05-15 06:24:14.468482: Epoch 962 
2024-05-15 06:24:14.468570: Current learning rate: 0.00053 
2024-05-15 06:24:55.982056: train_loss -0.8152 
2024-05-15 06:24:55.982229: val_loss -0.8014 
2024-05-15 06:24:55.982277: Pseudo dice [0.8732] 
2024-05-15 06:24:55.982329: Epoch time: 41.51 s 
2024-05-15 06:24:57.032559:  
2024-05-15 06:24:57.032728: Epoch 963 
2024-05-15 06:24:57.032821: Current learning rate: 0.00051 
2024-05-15 06:25:38.606589: train_loss -0.8169 
2024-05-15 06:25:38.606751: val_loss -0.8029 
2024-05-15 06:25:38.606833: Pseudo dice [0.872] 
2024-05-15 06:25:38.606920: Epoch time: 41.58 s 
2024-05-15 06:25:38.606986: Yayy! New best EMA pseudo Dice: 0.8679 
2024-05-15 06:25:40.018664:  
2024-05-15 06:25:40.018789: Epoch 964 
2024-05-15 06:25:40.018883: Current learning rate: 0.0005 
2024-05-15 06:26:21.596471: train_loss -0.8198 
2024-05-15 06:26:21.596692: val_loss -0.7727 
2024-05-15 06:26:21.596743: Pseudo dice [0.8566] 
2024-05-15 06:26:21.596796: Epoch time: 41.58 s 
2024-05-15 06:26:22.845464:  
2024-05-15 06:26:22.845612: Epoch 965 
2024-05-15 06:26:22.845705: Current learning rate: 0.00049 
2024-05-15 06:27:04.432202: train_loss -0.8246 
2024-05-15 06:27:04.432376: val_loss -0.8116 
2024-05-15 06:27:04.432425: Pseudo dice [0.8711] 
2024-05-15 06:27:04.432479: Epoch time: 41.59 s 
2024-05-15 06:27:05.521919:  
2024-05-15 06:27:05.522104: Epoch 966 
2024-05-15 06:27:05.522220: Current learning rate: 0.00048 
2024-05-15 06:27:47.081961: train_loss -0.8209 
2024-05-15 06:27:47.082134: val_loss -0.7939 
2024-05-15 06:27:47.082183: Pseudo dice [0.8577] 
2024-05-15 06:27:47.082236: Epoch time: 41.56 s 
2024-05-15 06:27:48.136192:  
2024-05-15 06:27:48.136329: Epoch 967 
2024-05-15 06:27:48.136420: Current learning rate: 0.00046 
2024-05-15 06:28:29.698740: train_loss -0.8226 
2024-05-15 06:28:29.698914: val_loss -0.7766 
2024-05-15 06:28:29.699013: Pseudo dice [0.8776] 
2024-05-15 06:28:29.699066: Epoch time: 41.56 s 
2024-05-15 06:28:30.754879:  
2024-05-15 06:28:30.755171: Epoch 968 
2024-05-15 06:28:30.755285: Current learning rate: 0.00045 
2024-05-15 06:29:12.311713: train_loss -0.8193 
2024-05-15 06:29:12.311890: val_loss -0.8076 
2024-05-15 06:29:12.311938: Pseudo dice [0.8666] 
2024-05-15 06:29:12.311991: Epoch time: 41.56 s 
2024-05-15 06:29:13.373311:  
2024-05-15 06:29:13.373437: Epoch 969 
2024-05-15 06:29:13.373528: Current learning rate: 0.00044 
2024-05-15 06:29:54.926126: train_loss -0.8225 
2024-05-15 06:29:54.926298: val_loss -0.8029 
2024-05-15 06:29:54.926347: Pseudo dice [0.8646] 
2024-05-15 06:29:54.926403: Epoch time: 41.55 s 
2024-05-15 06:29:55.990479:  
2024-05-15 06:29:55.990608: Epoch 970 
2024-05-15 06:29:55.990705: Current learning rate: 0.00043 
2024-05-15 06:30:37.544644: train_loss -0.8225 
2024-05-15 06:30:37.544874: val_loss -0.7783 
2024-05-15 06:30:37.544994: Pseudo dice [0.8644] 
2024-05-15 06:30:37.545051: Epoch time: 41.56 s 
2024-05-15 06:30:38.605520:  
2024-05-15 06:30:38.605706: Epoch 971 
2024-05-15 06:30:38.605800: Current learning rate: 0.00041 
2024-05-15 06:31:20.126204: train_loss -0.8159 
2024-05-15 06:31:20.126379: val_loss -0.7854 
2024-05-15 06:31:20.126434: Pseudo dice [0.865] 
2024-05-15 06:31:20.126489: Epoch time: 41.52 s 
2024-05-15 06:31:21.384125:  
2024-05-15 06:31:21.384287: Epoch 972 
2024-05-15 06:31:21.384387: Current learning rate: 0.0004 
2024-05-15 06:32:02.908184: train_loss -0.8255 
2024-05-15 06:32:02.908356: val_loss -0.7913 
2024-05-15 06:32:02.908404: Pseudo dice [0.8624] 
2024-05-15 06:32:02.908460: Epoch time: 41.53 s 
2024-05-15 06:32:03.966601:  
2024-05-15 06:32:03.966740: Epoch 973 
2024-05-15 06:32:03.966828: Current learning rate: 0.00039 
2024-05-15 06:32:45.485920: train_loss -0.8268 
2024-05-15 06:32:45.486096: val_loss -0.7913 
2024-05-15 06:32:45.486145: Pseudo dice [0.8661] 
2024-05-15 06:32:45.486196: Epoch time: 41.52 s 
2024-05-15 06:32:46.545731:  
2024-05-15 06:32:46.545921: Epoch 974 
2024-05-15 06:32:46.546023: Current learning rate: 0.00037 
2024-05-15 06:33:28.112789: train_loss -0.8284 
2024-05-15 06:33:28.112960: val_loss -0.8049 
2024-05-15 06:33:28.113008: Pseudo dice [0.8638] 
2024-05-15 06:33:28.113063: Epoch time: 41.57 s 
2024-05-15 06:33:29.184757:  
2024-05-15 06:33:29.184890: Epoch 975 
2024-05-15 06:33:29.184983: Current learning rate: 0.00036 
2024-05-15 06:34:10.741327: train_loss -0.8167 
2024-05-15 06:34:10.741495: val_loss -0.7819 
2024-05-15 06:34:10.741544: Pseudo dice [0.8583] 
2024-05-15 06:34:10.741600: Epoch time: 41.56 s 
2024-05-15 06:34:11.800913:  
2024-05-15 06:34:11.801039: Epoch 976 
2024-05-15 06:34:11.801135: Current learning rate: 0.00035 
2024-05-15 06:34:53.352539: train_loss -0.8172 
2024-05-15 06:34:53.352711: val_loss -0.8041 
2024-05-15 06:34:53.352759: Pseudo dice [0.8723] 
2024-05-15 06:34:53.352815: Epoch time: 41.55 s 
2024-05-15 06:34:54.408554:  
2024-05-15 06:34:54.408721: Epoch 977 
2024-05-15 06:34:54.408813: Current learning rate: 0.00034 
2024-05-15 06:35:35.960737: train_loss -0.8205 
2024-05-15 06:35:35.960907: val_loss -0.8165 
2024-05-15 06:35:35.960955: Pseudo dice [0.8737] 
2024-05-15 06:35:35.961010: Epoch time: 41.55 s 
2024-05-15 06:35:37.041956:  
2024-05-15 06:35:37.042079: Epoch 978 
2024-05-15 06:35:37.042168: Current learning rate: 0.00032 
2024-05-15 06:36:18.588148: train_loss -0.8124 
2024-05-15 06:36:18.588320: val_loss -0.7756 
2024-05-15 06:36:18.588368: Pseudo dice [0.8654] 
2024-05-15 06:36:18.588424: Epoch time: 41.55 s 
2024-05-15 06:36:19.860725:  
2024-05-15 06:36:19.860971: Epoch 979 
2024-05-15 06:36:19.861073: Current learning rate: 0.00031 
2024-05-15 06:37:01.407868: train_loss -0.8274 
2024-05-15 06:37:01.408035: val_loss -0.8005 
2024-05-15 06:37:01.408131: Pseudo dice [0.8615] 
2024-05-15 06:37:01.408190: Epoch time: 41.55 s 
2024-05-15 06:37:02.469364:  
2024-05-15 06:37:02.469522: Epoch 980 
2024-05-15 06:37:02.469620: Current learning rate: 0.0003 
2024-05-15 06:37:44.001927: train_loss -0.826 
2024-05-15 06:37:44.002098: val_loss -0.8042 
2024-05-15 06:37:44.002146: Pseudo dice [0.871] 
2024-05-15 06:37:44.002202: Epoch time: 41.53 s 
2024-05-15 06:37:45.060846:  
2024-05-15 06:37:45.060982: Epoch 981 
2024-05-15 06:37:45.061074: Current learning rate: 0.00028 
2024-05-15 06:38:26.593122: train_loss -0.8234 
2024-05-15 06:38:26.593295: val_loss -0.7944 
2024-05-15 06:38:26.593343: Pseudo dice [0.8802] 
2024-05-15 06:38:26.593399: Epoch time: 41.53 s 
2024-05-15 06:38:26.593441: Yayy! New best EMA pseudo Dice: 0.8679 
2024-05-15 06:38:28.014273:  
2024-05-15 06:38:28.014410: Epoch 982 
2024-05-15 06:38:28.014505: Current learning rate: 0.00027 
2024-05-15 06:39:09.561938: train_loss -0.8247 
2024-05-15 06:39:09.562109: val_loss -0.7904 
2024-05-15 06:39:09.562162: Pseudo dice [0.8562] 
2024-05-15 06:39:09.562217: Epoch time: 41.55 s 
2024-05-15 06:39:10.618297:  
2024-05-15 06:39:10.618433: Epoch 983 
2024-05-15 06:39:10.618522: Current learning rate: 0.00026 
2024-05-15 06:39:52.154177: train_loss -0.826 
2024-05-15 06:39:52.154352: val_loss -0.8027 
2024-05-15 06:39:52.154399: Pseudo dice [0.8747] 
2024-05-15 06:39:52.154462: Epoch time: 41.54 s 
2024-05-15 06:39:53.207721:  
2024-05-15 06:39:53.207884: Epoch 984 
2024-05-15 06:39:53.207976: Current learning rate: 0.00024 
2024-05-15 06:40:34.733747: train_loss -0.8261 
2024-05-15 06:40:34.733919: val_loss -0.795 
2024-05-15 06:40:34.733967: Pseudo dice [0.8571] 
2024-05-15 06:40:34.734021: Epoch time: 41.53 s 
2024-05-15 06:40:35.797782:  
2024-05-15 06:40:35.797987: Epoch 985 
2024-05-15 06:40:35.798080: Current learning rate: 0.00023 
2024-05-15 06:41:17.334879: train_loss -0.8224 
2024-05-15 06:41:17.335051: val_loss -0.7893 
2024-05-15 06:41:17.335099: Pseudo dice [0.8652] 
2024-05-15 06:41:17.335156: Epoch time: 41.54 s 
2024-05-15 06:41:18.589575:  
2024-05-15 06:41:18.589723: Epoch 986 
2024-05-15 06:41:18.589817: Current learning rate: 0.00021 
2024-05-15 06:42:00.146471: train_loss -0.8302 
2024-05-15 06:42:00.146642: val_loss -0.8085 
2024-05-15 06:42:00.146692: Pseudo dice [0.8688] 
2024-05-15 06:42:00.146744: Epoch time: 41.56 s 
2024-05-15 06:42:01.202560:  
2024-05-15 06:42:01.202691: Epoch 987 
2024-05-15 06:42:01.202786: Current learning rate: 0.0002 
2024-05-15 06:42:42.770989: train_loss -0.8361 
2024-05-15 06:42:42.771156: val_loss -0.7994 
2024-05-15 06:42:42.771205: Pseudo dice [0.8696] 
2024-05-15 06:42:42.771257: Epoch time: 41.57 s 
2024-05-15 06:42:43.837877:  
2024-05-15 06:42:43.838006: Epoch 988 
2024-05-15 06:42:43.838102: Current learning rate: 0.00019 
2024-05-15 06:43:25.436440: train_loss -0.8136 
2024-05-15 06:43:25.436609: val_loss -0.7885 
2024-05-15 06:43:25.436670: Pseudo dice [0.8797] 
2024-05-15 06:43:25.436726: Epoch time: 41.6 s 
2024-05-15 06:43:25.436770: Yayy! New best EMA pseudo Dice: 0.8682 
2024-05-15 06:43:26.886630:  
2024-05-15 06:43:26.886762: Epoch 989 
2024-05-15 06:43:26.886852: Current learning rate: 0.00017 
2024-05-15 06:44:08.417666: train_loss -0.827 
2024-05-15 06:44:08.417842: val_loss -0.7981 
2024-05-15 06:44:08.417890: Pseudo dice [0.8737] 
2024-05-15 06:44:08.417947: Epoch time: 41.53 s 
2024-05-15 06:44:08.417990: Yayy! New best EMA pseudo Dice: 0.8687 
2024-05-15 06:44:09.832737:  
2024-05-15 06:44:09.832866: Epoch 990 
2024-05-15 06:44:09.832956: Current learning rate: 0.00016 
2024-05-15 06:44:51.349609: train_loss -0.8279 
2024-05-15 06:44:51.349782: val_loss -0.8106 
2024-05-15 06:44:51.349830: Pseudo dice [0.8763] 
2024-05-15 06:44:51.349886: Epoch time: 41.52 s 
2024-05-15 06:44:51.349929: Yayy! New best EMA pseudo Dice: 0.8695 
2024-05-15 06:44:52.769486:  
2024-05-15 06:44:52.769616: Epoch 991 
2024-05-15 06:44:52.769711: Current learning rate: 0.00014 
2024-05-15 06:45:34.285908: train_loss -0.8186 
2024-05-15 06:45:34.286082: val_loss -0.795 
2024-05-15 06:45:34.286131: Pseudo dice [0.8686] 
2024-05-15 06:45:34.286187: Epoch time: 41.52 s 
2024-05-15 06:45:35.359915:  
2024-05-15 06:45:35.360261: Epoch 992 
2024-05-15 06:45:35.360368: Current learning rate: 0.00013 
2024-05-15 06:46:16.875869: train_loss -0.8194 
2024-05-15 06:46:16.876032: val_loss -0.7832 
2024-05-15 06:46:16.876081: Pseudo dice [0.8659] 
2024-05-15 06:46:16.876137: Epoch time: 41.52 s 
2024-05-15 06:46:18.135496:  
2024-05-15 06:46:18.135647: Epoch 993 
2024-05-15 06:46:18.135738: Current learning rate: 0.00011 
2024-05-15 06:46:59.646221: train_loss -0.8207 
2024-05-15 06:46:59.646391: val_loss -0.7941 
2024-05-15 06:46:59.646446: Pseudo dice [0.8666] 
2024-05-15 06:46:59.646502: Epoch time: 41.51 s 
2024-05-15 06:47:00.703751:  
2024-05-15 06:47:00.703960: Epoch 994 
2024-05-15 06:47:00.704057: Current learning rate: 0.0001 
2024-05-15 06:47:42.211235: train_loss -0.8214 
2024-05-15 06:47:42.211412: val_loss -0.7907 
2024-05-15 06:47:42.211460: Pseudo dice [0.8555] 
2024-05-15 06:47:42.211516: Epoch time: 41.51 s 
2024-05-15 06:47:43.270384:  
2024-05-15 06:47:43.270523: Epoch 995 
2024-05-15 06:47:43.270617: Current learning rate: 8e-05 
2024-05-15 06:48:24.815119: train_loss -0.8283 
2024-05-15 06:48:24.815290: val_loss -0.7669 
2024-05-15 06:48:24.815338: Pseudo dice [0.8609] 
2024-05-15 06:48:24.815393: Epoch time: 41.55 s 
2024-05-15 06:48:25.874235:  
2024-05-15 06:48:25.874375: Epoch 996 
2024-05-15 06:48:25.874475: Current learning rate: 7e-05 
2024-05-15 06:49:07.432010: train_loss -0.8284 
2024-05-15 06:49:07.432183: val_loss -0.8011 
2024-05-15 06:49:07.432229: Pseudo dice [0.87] 
2024-05-15 06:49:07.432285: Epoch time: 41.56 s 
2024-05-15 06:49:08.491447:  
2024-05-15 06:49:08.491579: Epoch 997 
2024-05-15 06:49:08.491673: Current learning rate: 5e-05 
2024-05-15 06:49:50.038784: train_loss -0.8222 
2024-05-15 06:49:50.038962: val_loss -0.8126 
2024-05-15 06:49:50.039011: Pseudo dice [0.8708] 
2024-05-15 06:49:50.039067: Epoch time: 41.55 s 
2024-05-15 06:49:51.098113:  
2024-05-15 06:49:51.098240: Epoch 998 
2024-05-15 06:49:51.098335: Current learning rate: 4e-05 
2024-05-15 06:50:32.654855: train_loss -0.8223 
2024-05-15 06:50:32.655034: val_loss -0.7713 
2024-05-15 06:50:32.655088: Pseudo dice [0.8582] 
2024-05-15 06:50:32.655146: Epoch time: 41.56 s 
2024-05-15 06:50:33.714207:  
2024-05-15 06:50:33.714334: Epoch 999 
2024-05-15 06:50:33.714438: Current learning rate: 2e-05 
2024-05-15 06:51:15.268533: train_loss -0.8207 
2024-05-15 06:51:15.268703: val_loss -0.8003 
2024-05-15 06:51:15.268751: Pseudo dice [0.874] 
2024-05-15 06:51:15.268806: Epoch time: 41.56 s 
2024-05-15 06:51:16.913160: Training done. 
2024-05-15 06:51:17.335362: Using splits from existing split file: /raid/dataset/nnUNet_preprocessed/Dataset030_4DCT_ONLY_2_3D/splits_final.json 
2024-05-15 06:51:17.337035: The split file contains 5 splits. 
2024-05-15 06:51:17.337075: Desired fold for training: 0 
2024-05-15 06:51:17.337118: This split has 1723 training and 431 validation cases. 
2024-05-15 06:51:17.343103: predicting 102_0_0_0_0 
2024-05-15 06:51:17.344195: 102_0_0_0_0, shape torch.Size([1, 49, 207, 293]), rank 0 
2024-05-15 06:51:20.566352: predicting 106_0_16_1_2 
2024-05-15 06:51:20.567771: 106_0_16_1_2, shape torch.Size([1, 34, 159, 224]), rank 0 
2024-05-15 06:51:20.817529: predicting 106_0_4_2_1 
2024-05-15 06:51:20.818603: 106_0_4_2_1, shape torch.Size([1, 34, 159, 224]), rank 0 
2024-05-15 06:51:21.069501: predicting 106_0_4_2_2 
2024-05-15 06:51:21.070539: 106_0_4_2_2, shape torch.Size([1, 34, 159, 224]), rank 0 
2024-05-15 06:51:21.321055: predicting 111_0_16_2_0 
2024-05-15 06:51:21.321793: 111_0_16_2_0, shape torch.Size([1, 43, 223, 284]), rank 0 
2024-05-15 06:51:22.288834: predicting 111_0_4_2_0 
2024-05-15 06:51:22.289942: 111_0_4_2_0, shape torch.Size([1, 43, 223, 284]), rank 0 
2024-05-15 06:51:23.257160: predicting 111_0_4_2_1 
2024-05-15 06:51:23.257991: 111_0_4_2_1, shape torch.Size([1, 43, 223, 284]), rank 0 
2024-05-15 06:51:24.226678: predicting 111_0_8_2_1 
2024-05-15 06:51:24.227801: 111_0_8_2_1, shape torch.Size([1, 43, 223, 284]), rank 0 
2024-05-15 06:51:25.193485: predicting 111_0_8_2_2 
2024-05-15 06:51:25.194291: 111_0_8_2_2, shape torch.Size([1, 43, 223, 284]), rank 0 
2024-05-15 06:51:26.160088: predicting 111_0_8_3_2 
2024-05-15 06:51:26.161244: 111_0_8_3_2, shape torch.Size([1, 43, 223, 284]), rank 0 
2024-05-15 06:51:27.126872: predicting 112_5_16_2_1 
2024-05-15 06:51:27.127996: 112_5_16_2_1, shape torch.Size([1, 47, 178, 264]), rank 0 
2024-05-15 06:51:27.616067: predicting 114_4_16_1_1 
2024-05-15 06:51:27.616849: 114_4_16_1_1, shape torch.Size([1, 54, 206, 288]), rank 0 
2024-05-15 06:51:29.536990: predicting 114_4_4_2_2 
2024-05-15 06:51:29.537793: 114_4_4_2_2, shape torch.Size([1, 54, 206, 288]), rank 0 
2024-05-15 06:51:31.456463: predicting 114_4_8_3_2 
2024-05-15 06:51:31.457641: 114_4_8_3_2, shape torch.Size([1, 54, 206, 288]), rank 0 
2024-05-15 06:51:33.374740: predicting 116_4_0_0_0 
2024-05-15 06:51:33.375781: 116_4_0_0_0, shape torch.Size([1, 41, 202, 257]), rank 0 
2024-05-15 06:51:34.340179: predicting 117_0_8_3_2 
2024-05-15 06:51:34.340942: 117_0_8_3_2, shape torch.Size([1, 30, 215, 285]), rank 0 
2024-05-15 06:51:35.304712: predicting 117_7_16_1_0 
2024-05-15 06:51:35.305435: 117_7_16_1_0, shape torch.Size([1, 37, 226, 287]), rank 0 
2024-05-15 06:51:36.272281: predicting 117_7_16_2_0 
2024-05-15 06:51:36.273072: 117_7_16_2_0, shape torch.Size([1, 37, 226, 287]), rank 0 
2024-05-15 06:51:37.238717: predicting 117_7_4_2_1 
2024-05-15 06:51:37.239526: 117_7_4_2_1, shape torch.Size([1, 37, 226, 287]), rank 0 
2024-05-15 06:51:38.204937: predicting 117_7_4_3_0 
2024-05-15 06:51:38.206061: 117_7_4_3_0, shape torch.Size([1, 37, 226, 287]), rank 0 
2024-05-15 06:51:39.170901: predicting 117_7_8_3_0 
2024-05-15 06:51:39.171664: 117_7_8_3_0, shape torch.Size([1, 37, 226, 287]), rank 0 
2024-05-15 06:51:40.135329: predicting 117_7_8_3_2 
2024-05-15 06:51:40.136431: 117_7_8_3_2, shape torch.Size([1, 37, 226, 287]), rank 0 
2024-05-15 06:51:41.101779: predicting 118_0_16_1_1 
2024-05-15 06:51:41.102897: 118_0_16_1_1, shape torch.Size([1, 35, 151, 237]), rank 0 
2024-05-15 06:51:41.352440: predicting 118_0_16_2_2 
2024-05-15 06:51:41.353162: 118_0_16_2_2, shape torch.Size([1, 35, 151, 237]), rank 0 
2024-05-15 06:51:41.604038: predicting 118_0_4_2_0 
2024-05-15 06:51:41.604759: 118_0_4_2_0, shape torch.Size([1, 35, 151, 237]), rank 0 
2024-05-15 06:51:41.855982: predicting 118_0_8_3_2 
2024-05-15 06:51:41.856706: 118_0_8_3_2, shape torch.Size([1, 35, 151, 237]), rank 0 
2024-05-15 06:51:42.108410: predicting 118_5_4_2_2 
2024-05-15 06:51:42.109116: 118_5_4_2_2, shape torch.Size([1, 41, 150, 236]), rank 0 
2024-05-15 06:51:42.359544: predicting 118_5_4_3_0 
2024-05-15 06:51:42.360279: 118_5_4_3_0, shape torch.Size([1, 41, 150, 236]), rank 0 
2024-05-15 06:51:42.611821: predicting B10_01_16_1_1 
2024-05-15 06:51:42.612576: B10_01_16_1_1, shape torch.Size([1, 46, 146, 229]), rank 0 
2024-05-15 06:51:42.863945: predicting B10_01_16_1_2 
2024-05-15 06:51:42.864713: B10_01_16_1_2, shape torch.Size([1, 46, 146, 229]), rank 0 
2024-05-15 06:51:43.116491: predicting B10_01_16_2_0 
2024-05-15 06:51:43.117270: B10_01_16_2_0, shape torch.Size([1, 45, 146, 229]), rank 0 
2024-05-15 06:51:43.368908: predicting B10_01_16_2_1 
2024-05-15 06:51:43.369655: B10_01_16_2_1, shape torch.Size([1, 44, 146, 229]), rank 0 
2024-05-15 06:51:43.625073: predicting B10_01_16_2_2 
2024-05-15 06:51:43.625808: B10_01_16_2_2, shape torch.Size([1, 45, 146, 229]), rank 0 
2024-05-15 06:51:43.878134: predicting B10_01_4_2_2 
2024-05-15 06:51:43.878915: B10_01_4_2_2, shape torch.Size([1, 46, 146, 229]), rank 0 
2024-05-15 06:51:44.130429: predicting B10_01_8_3_1 
2024-05-15 06:51:44.131247: B10_01_8_3_1, shape torch.Size([1, 44, 146, 229]), rank 0 
2024-05-15 06:51:44.382752: predicting B14_01_16_1_0 
2024-05-15 06:51:44.383499: B14_01_16_1_0, shape torch.Size([1, 45, 175, 253]), rank 0 
2024-05-15 06:51:44.635437: predicting B14_01_16_1_2 
2024-05-15 06:51:44.636262: B14_01_16_1_2, shape torch.Size([1, 45, 175, 253]), rank 0 
2024-05-15 06:51:44.887972: predicting B14_01_4_2_1 
2024-05-15 06:51:44.888746: B14_01_4_2_1, shape torch.Size([1, 45, 175, 253]), rank 0 
2024-05-15 06:51:45.140216: predicting B14_01_4_2_2 
2024-05-15 06:51:45.140981: B14_01_4_2_2, shape torch.Size([1, 45, 175, 253]), rank 0 
2024-05-15 06:51:45.392896: predicting B14_01_8_2_2 
2024-05-15 06:51:45.393656: B14_01_8_2_2, shape torch.Size([1, 45, 175, 253]), rank 0 
2024-05-15 06:51:45.645355: predicting B14_01_8_3_0 
2024-05-15 06:51:45.646121: B14_01_8_3_0, shape torch.Size([1, 45, 175, 253]), rank 0 
2024-05-15 06:51:45.898473: predicting B14_01_8_3_1 
2024-05-15 06:51:45.899231: B14_01_8_3_1, shape torch.Size([1, 45, 175, 253]), rank 0 
2024-05-15 06:51:46.150811: predicting B14_01_8_3_2 
2024-05-15 06:51:46.151583: B14_01_8_3_2, shape torch.Size([1, 45, 175, 253]), rank 0 
2024-05-15 06:51:46.404147: predicting B20_00_16_2_2 
2024-05-15 06:51:46.404901: B20_00_16_2_2, shape torch.Size([1, 45, 150, 230]), rank 0 
2024-05-15 06:51:46.655969: predicting B20_00_4_2_0 
2024-05-15 06:51:46.656691: B20_00_4_2_0, shape torch.Size([1, 45, 150, 230]), rank 0 
2024-05-15 06:51:46.908154: predicting B20_00_4_2_2 
2024-05-15 06:51:46.908888: B20_00_4_2_2, shape torch.Size([1, 45, 150, 230]), rank 0 
2024-05-15 06:51:47.159978: predicting B20_00_4_3_2 
2024-05-15 06:51:47.160713: B20_00_4_3_2, shape torch.Size([1, 45, 150, 230]), rank 0 
2024-05-15 06:51:47.412121: predicting B20_00_8_3_2 
2024-05-15 06:51:47.412859: B20_00_8_3_2, shape torch.Size([1, 45, 150, 230]), rank 0 
2024-05-15 06:51:47.664179: predicting B21_00_16_1_1 
2024-05-15 06:51:47.665219: B21_00_16_1_1, shape torch.Size([1, 44, 154, 199]), rank 0 
2024-05-15 06:51:47.915787: predicting B21_00_16_2_0 
2024-05-15 06:51:47.916515: B21_00_16_2_0, shape torch.Size([1, 41, 154, 199]), rank 0 
2024-05-15 06:51:48.167393: predicting B21_00_16_2_1 
2024-05-15 06:51:48.168124: B21_00_16_2_1, shape torch.Size([1, 43, 154, 199]), rank 0 
2024-05-15 06:51:48.419366: predicting B21_00_4_2_0 
2024-05-15 06:51:48.420089: B21_00_4_2_0, shape torch.Size([1, 41, 154, 199]), rank 0 
2024-05-15 06:51:48.671249: predicting B21_00_4_2_1 
2024-05-15 06:51:48.671954: B21_00_4_2_1, shape torch.Size([1, 44, 154, 199]), rank 0 
2024-05-15 06:51:48.922488: predicting B21_00_4_2_2 
2024-05-15 06:51:48.923225: B21_00_4_2_2, shape torch.Size([1, 44, 154, 199]), rank 0 
2024-05-15 06:51:49.174843: predicting B21_00_4_3_0 
2024-05-15 06:51:49.175649: B21_00_4_3_0, shape torch.Size([1, 44, 154, 199]), rank 0 
2024-05-15 06:51:49.427096: predicting B21_01_4_3_0 
2024-05-15 06:51:49.427848: B21_01_4_3_0, shape torch.Size([1, 44, 160, 201]), rank 0 
2024-05-15 06:51:49.679720: predicting B21_01_4_3_2 
2024-05-15 06:51:49.680478: B21_01_4_3_2, shape torch.Size([1, 42, 160, 201]), rank 0 
2024-05-15 06:51:49.931887: predicting B25_00_16_1_2 
2024-05-15 06:51:49.932595: B25_00_16_1_2, shape torch.Size([1, 46, 163, 219]), rank 0 
2024-05-15 06:51:50.185037: predicting B25_00_4_2_0 
2024-05-15 06:51:50.185812: B25_00_4_2_0, shape torch.Size([1, 46, 163, 219]), rank 0 
2024-05-15 06:51:50.437130: predicting B25_00_8_2_2 
2024-05-15 06:51:50.437890: B25_00_8_2_2, shape torch.Size([1, 46, 163, 219]), rank 0 
2024-05-15 06:51:50.690016: predicting B26_00_16_2_1 
2024-05-15 06:51:50.690819: B26_00_16_2_1, shape torch.Size([1, 44, 131, 224]), rank 0 
2024-05-15 06:51:50.941806: predicting B26_00_4_2_0 
2024-05-15 06:51:50.942529: B26_00_4_2_0, shape torch.Size([1, 44, 131, 224]), rank 0 
2024-05-15 06:51:51.194824: predicting B26_00_4_2_1 
2024-05-15 06:51:51.195565: B26_00_4_2_1, shape torch.Size([1, 44, 131, 224]), rank 0 
2024-05-15 06:51:51.446761: predicting B27_00_16_1_2 
2024-05-15 06:51:51.447542: B27_00_16_1_2, shape torch.Size([1, 39, 162, 236]), rank 0 
2024-05-15 06:51:51.698329: predicting B27_00_8_2_0 
2024-05-15 06:51:51.699078: B27_00_8_2_0, shape torch.Size([1, 39, 162, 236]), rank 0 
2024-05-15 06:51:51.949722: predicting B28_01_16_2_1 
2024-05-15 06:51:51.950438: B28_01_16_2_1, shape torch.Size([1, 43, 183, 244]), rank 0 
2024-05-15 06:51:52.202161: predicting B28_01_4_3_1 
2024-05-15 06:51:52.202946: B28_01_4_3_1, shape torch.Size([1, 43, 183, 244]), rank 0 
2024-05-15 06:51:52.454023: predicting B28_01_8_2_0 
2024-05-15 06:51:52.454817: B28_01_8_2_0, shape torch.Size([1, 43, 183, 244]), rank 0 
2024-05-15 06:51:52.706262: predicting B32_00_16_1_0 
2024-05-15 06:51:52.707025: B32_00_16_1_0, shape torch.Size([1, 36, 147, 213]), rank 0 
2024-05-15 06:51:52.957902: predicting B32_00_4_2_0 
2024-05-15 06:51:52.958653: B32_00_4_2_0, shape torch.Size([1, 36, 147, 213]), rank 0 
2024-05-15 06:51:53.209769: predicting B37_01_16_2_1 
2024-05-15 06:51:53.210518: B37_01_16_2_1, shape torch.Size([1, 44, 166, 245]), rank 0 
2024-05-15 06:51:53.462133: predicting B37_01_4_2_2 
2024-05-15 06:51:53.462905: B37_01_4_2_2, shape torch.Size([1, 44, 166, 245]), rank 0 
2024-05-15 06:51:53.714469: predicting B37_01_4_3_2 
2024-05-15 06:51:53.715245: B37_01_4_3_2, shape torch.Size([1, 44, 166, 245]), rank 0 
2024-05-15 06:51:53.967264: predicting B37_01_8_3_1 
2024-05-15 06:51:53.968043: B37_01_8_3_1, shape torch.Size([1, 44, 166, 245]), rank 0 
2024-05-15 06:51:54.219652: predicting B40_00_16_1_2 
2024-05-15 06:51:54.220427: B40_00_16_1_2, shape torch.Size([1, 49, 159, 266]), rank 0 
2024-05-15 06:51:55.186504: predicting B40_00_8_2_0 
2024-05-15 06:51:55.187662: B40_00_8_2_0, shape torch.Size([1, 49, 159, 266]), rank 0 
2024-05-15 06:51:56.153215: predicting B40_00_8_2_2 
2024-05-15 06:51:56.154003: B40_00_8_2_2, shape torch.Size([1, 49, 159, 266]), rank 0 
2024-05-15 06:51:57.119832: predicting B40_00_8_3_0 
2024-05-15 06:51:57.120633: B40_00_8_3_0, shape torch.Size([1, 49, 159, 266]), rank 0 
2024-05-15 06:51:58.085550: predicting B40_00_8_3_1 
2024-05-15 06:51:58.086384: B40_00_8_3_1, shape torch.Size([1, 49, 159, 266]), rank 0 
2024-05-15 06:51:59.051230: predicting B40_00_8_3_2 
2024-05-15 06:51:59.052003: B40_00_8_3_2, shape torch.Size([1, 49, 159, 266]), rank 0 
2024-05-15 06:52:00.017263: predicting B41_00_4_2_0 
2024-05-15 06:52:00.018045: B41_00_4_2_0, shape torch.Size([1, 48, 218, 269]), rank 0 
2024-05-15 06:52:00.982821: predicting B41_00_4_2_2 
2024-05-15 06:52:00.983665: B41_00_4_2_2, shape torch.Size([1, 48, 218, 269]), rank 0 
2024-05-15 06:52:01.949682: predicting B41_00_4_3_1 
2024-05-15 06:52:01.950488: B41_00_4_3_1, shape torch.Size([1, 48, 218, 269]), rank 0 
2024-05-15 06:52:02.915989: predicting B41_00_8_3_2 
2024-05-15 06:52:02.917184: B41_00_8_3_2, shape torch.Size([1, 48, 218, 269]), rank 0 
2024-05-15 06:52:03.881644: predicting B44_00_16_1_1 
2024-05-15 06:52:03.882856: B44_00_16_1_1, shape torch.Size([1, 36, 146, 199]), rank 0 
2024-05-15 06:52:04.131502: predicting B44_00_4_2_1 
2024-05-15 06:52:04.132251: B44_00_4_2_1, shape torch.Size([1, 36, 146, 199]), rank 0 
2024-05-15 06:52:04.383715: predicting B44_00_4_2_2 
2024-05-15 06:52:04.384477: B44_00_4_2_2, shape torch.Size([1, 36, 146, 199]), rank 0 
2024-05-15 06:52:04.634688: predicting B44_00_4_3_0 
2024-05-15 06:52:04.636008: B44_00_4_3_0, shape torch.Size([1, 36, 146, 199]), rank 0 
2024-05-15 06:52:04.888119: predicting B44_00_8_2_0 
2024-05-15 06:52:04.888851: B44_00_8_2_0, shape torch.Size([1, 36, 146, 199]), rank 0 
2024-05-15 06:52:05.140022: predicting B_03_preCT_16_1_1 
2024-05-15 06:52:05.140722: B_03_preCT_16_1_1, shape torch.Size([1, 39, 174, 247]), rank 0 
2024-05-15 06:52:05.392155: predicting B_03_preCT_16_1_2 
2024-05-15 06:52:05.392898: B_03_preCT_16_1_2, shape torch.Size([1, 38, 174, 247]), rank 0 
2024-05-15 06:52:05.643319: predicting B_03_preCT_4_3_2 
2024-05-15 06:52:05.644071: B_03_preCT_4_3_2, shape torch.Size([1, 39, 174, 247]), rank 0 
2024-05-15 06:52:05.895076: predicting B_03_preCT_8_2_0 
2024-05-15 06:52:05.895814: B_03_preCT_8_2_0, shape torch.Size([1, 38, 174, 247]), rank 0 
2024-05-15 06:52:06.146859: predicting B_03_preCT_8_2_1 
2024-05-15 06:52:06.147609: B_03_preCT_8_2_1, shape torch.Size([1, 38, 174, 247]), rank 0 
2024-05-15 06:52:06.398856: predicting B_05_postCT_16_2_1 
2024-05-15 06:52:06.399612: B_05_postCT_16_2_1, shape torch.Size([1, 47, 150, 231]), rank 0 
2024-05-15 06:52:06.650917: predicting B_05_postCT_16_2_2 
2024-05-15 06:52:06.651684: B_05_postCT_16_2_2, shape torch.Size([1, 47, 150, 231]), rank 0 
2024-05-15 06:52:06.902500: predicting B_05_postCT_4_2_2 
2024-05-15 06:52:06.903641: B_05_postCT_4_2_2, shape torch.Size([1, 47, 150, 231]), rank 0 
2024-05-15 06:52:07.154828: predicting B_05_postCT_4_3_0 
2024-05-15 06:52:07.155978: B_05_postCT_4_3_0, shape torch.Size([1, 47, 150, 231]), rank 0 
2024-05-15 06:52:07.407120: predicting B_05_postCT_8_2_0 
2024-05-15 06:52:07.407851: B_05_postCT_8_2_0, shape torch.Size([1, 47, 150, 231]), rank 0 
2024-05-15 06:52:07.664686: predicting B_10_postCT_8_3_1 
2024-05-15 06:52:07.665420: B_10_postCT_8_3_1, shape torch.Size([1, 46, 146, 229]), rank 0 
2024-05-15 06:52:07.917328: predicting B_15_postCT_16_2_2 
2024-05-15 06:52:07.918124: B_15_postCT_16_2_2, shape torch.Size([1, 40, 183, 257]), rank 0 
2024-05-15 06:52:08.407852: predicting B_15_postCT_4_3_2 
2024-05-15 06:52:08.408636: B_15_postCT_4_3_2, shape torch.Size([1, 40, 183, 257]), rank 0 
2024-05-15 06:52:08.897810: predicting B_15_postCT_8_2_0 
2024-05-15 06:52:08.898632: B_15_postCT_8_2_0, shape torch.Size([1, 40, 183, 257]), rank 0 
2024-05-15 06:52:09.387597: predicting B_15_preCT_4_2_2 
2024-05-15 06:52:09.388413: B_15_preCT_4_2_2, shape torch.Size([1, 42, 182, 264]), rank 0 
2024-05-15 06:52:09.877951: predicting B_15_preCT_4_3_1 
2024-05-15 06:52:09.878788: B_15_preCT_4_3_1, shape torch.Size([1, 42, 182, 264]), rank 0 
2024-05-15 06:52:10.367994: predicting B_15_preCT_4_3_2 
2024-05-15 06:52:10.369056: B_15_preCT_4_3_2, shape torch.Size([1, 42, 182, 264]), rank 0 
2024-05-15 06:52:10.859138: predicting B_15_preCT_8_2_0 
2024-05-15 06:52:10.860217: B_15_preCT_8_2_0, shape torch.Size([1, 41, 182, 264]), rank 0 
2024-05-15 06:52:11.350052: predicting B_15_preCT_8_2_1 
2024-05-15 06:52:11.350831: B_15_preCT_8_2_1, shape torch.Size([1, 42, 182, 264]), rank 0 
2024-05-15 06:52:11.840170: predicting B_21_postCT_4_2_2 
2024-05-15 06:52:11.841326: B_21_postCT_4_2_2, shape torch.Size([1, 42, 160, 201]), rank 0 
2024-05-15 06:52:12.092227: predicting B_21_postCT_8_2_2 
2024-05-15 06:52:12.092977: B_21_postCT_8_2_2, shape torch.Size([1, 44, 160, 201]), rank 0 
2024-05-15 06:52:12.343987: predicting B_21_postCT_8_3_0 
2024-05-15 06:52:12.345058: B_21_postCT_8_3_0, shape torch.Size([1, 42, 160, 201]), rank 0 
2024-05-15 06:52:12.596789: predicting B_21_postCT_8_3_2 
2024-05-15 06:52:12.597545: B_21_postCT_8_3_2, shape torch.Size([1, 42, 160, 201]), rank 0 
2024-05-15 06:52:12.848973: predicting B_21_preCT_16_1_1 
2024-05-15 06:52:12.849756: B_21_preCT_16_1_1, shape torch.Size([1, 44, 154, 199]), rank 0 
2024-05-15 06:52:13.100978: predicting B_21_preCT_4_3_0 
2024-05-15 06:52:13.102005: B_21_preCT_4_3_0, shape torch.Size([1, 41, 154, 199]), rank 0 
2024-05-15 06:52:13.353314: predicting B_21_preCT_4_3_1 
2024-05-15 06:52:13.354122: B_21_preCT_4_3_1, shape torch.Size([1, 44, 154, 199]), rank 0 
2024-05-15 06:52:13.605074: predicting B_21_preCT_8_2_2 
2024-05-15 06:52:13.605804: B_21_preCT_8_2_2, shape torch.Size([1, 43, 154, 199]), rank 0 
2024-05-15 06:52:13.856985: predicting B_21_preCT_8_3_2 
2024-05-15 06:52:13.857706: B_21_preCT_8_3_2, shape torch.Size([1, 41, 154, 199]), rank 0 
2024-05-15 06:52:14.108688: predicting B_24_postCT_8_2_2 
2024-05-15 06:52:14.109401: B_24_postCT_8_2_2, shape torch.Size([1, 46, 179, 252]), rank 0 
2024-05-15 06:52:14.360919: predicting B_24_preCT_16_2_0 
2024-05-15 06:52:14.361690: B_24_preCT_16_2_0, shape torch.Size([1, 49, 175, 244]), rank 0 
2024-05-15 06:52:14.851510: predicting B_24_preCT_16_2_1 
2024-05-15 06:52:14.852305: B_24_preCT_16_2_1, shape torch.Size([1, 49, 175, 244]), rank 0 
2024-05-15 06:52:15.342675: predicting B_24_preCT_8_3_0 
2024-05-15 06:52:15.343483: B_24_preCT_8_3_0, shape torch.Size([1, 49, 175, 244]), rank 0 
2024-05-15 06:52:15.832949: predicting B_25_preCT_16_1_2 
2024-05-15 06:52:15.834015: B_25_preCT_16_1_2, shape torch.Size([1, 46, 163, 219]), rank 0 
2024-05-15 06:52:16.085396: predicting B_25_preCT_16_2_1 
2024-05-15 06:52:16.086482: B_25_preCT_16_2_1, shape torch.Size([1, 46, 163, 219]), rank 0 
2024-05-15 06:52:16.339020: predicting B_25_preCT_8_2_2 
2024-05-15 06:52:16.339777: B_25_preCT_8_2_2, shape torch.Size([1, 46, 163, 219]), rank 0 
2024-05-15 06:52:16.591637: predicting B_26_postCT_16_1_0 
2024-05-15 06:52:16.592357: B_26_postCT_16_1_0, shape torch.Size([1, 45, 130, 224]), rank 0 
2024-05-15 06:52:16.843824: predicting B_26_postCT_16_1_2 
2024-05-15 06:52:16.844532: B_26_postCT_16_1_2, shape torch.Size([1, 45, 130, 224]), rank 0 
2024-05-15 06:52:17.096141: predicting B_26_postCT_8_2_0 
2024-05-15 06:52:17.096880: B_26_postCT_8_2_0, shape torch.Size([1, 45, 130, 224]), rank 0 
2024-05-15 06:52:17.347708: predicting B_26_preCT_16_1_0 
2024-05-15 06:52:17.348761: B_26_preCT_16_1_0, shape torch.Size([1, 44, 131, 224]), rank 0 
2024-05-15 06:52:17.599083: predicting B_26_preCT_16_1_1 
2024-05-15 06:52:17.599805: B_26_preCT_16_1_1, shape torch.Size([1, 44, 131, 224]), rank 0 
2024-05-15 06:52:17.850961: predicting B_26_preCT_16_1_2 
2024-05-15 06:52:17.851726: B_26_preCT_16_1_2, shape torch.Size([1, 44, 131, 224]), rank 0 
2024-05-15 06:52:18.102885: predicting B_26_preCT_4_3_1 
2024-05-15 06:52:18.103579: B_26_preCT_4_3_1, shape torch.Size([1, 44, 131, 224]), rank 0 
2024-05-15 06:52:18.354364: predicting B_26_preCT_8_3_0 
2024-05-15 06:52:18.355080: B_26_preCT_8_3_0, shape torch.Size([1, 44, 131, 224]), rank 0 
2024-05-15 06:52:18.606888: predicting B_28_postCT_16_1_1 
2024-05-15 06:52:18.607633: B_28_postCT_16_1_1, shape torch.Size([1, 43, 183, 244]), rank 0 
2024-05-15 06:52:18.858719: predicting B_28_postCT_16_2_2 
2024-05-15 06:52:18.859817: B_28_postCT_16_2_2, shape torch.Size([1, 43, 183, 244]), rank 0 
2024-05-15 06:52:19.111935: predicting B_28_postCT_4_3_0 
2024-05-15 06:52:19.112720: B_28_postCT_4_3_0, shape torch.Size([1, 43, 183, 244]), rank 0 
2024-05-15 06:52:19.364520: predicting B_28_postCT_8_3_2 
2024-05-15 06:52:19.365322: B_28_postCT_8_3_2, shape torch.Size([1, 43, 183, 244]), rank 0 
2024-05-15 06:52:19.617748: predicting B_32_postCT_16_2_0 
2024-05-15 06:52:19.618516: B_32_postCT_16_2_0, shape torch.Size([1, 34, 139, 213]), rank 0 
2024-05-15 06:52:19.869286: predicting B_32_postCT_4_2_1 
2024-05-15 06:52:19.869986: B_32_postCT_4_2_1, shape torch.Size([1, 34, 139, 213]), rank 0 
2024-05-15 06:52:20.121685: predicting B_32_postCT_8_3_0 
2024-05-15 06:52:20.122383: B_32_postCT_8_3_0, shape torch.Size([1, 34, 139, 213]), rank 0 
2024-05-15 06:52:20.373262: predicting B_32_postCT_8_3_2 
2024-05-15 06:52:20.373954: B_32_postCT_8_3_2, shape torch.Size([1, 34, 139, 213]), rank 0 
2024-05-15 06:52:20.624795: predicting B_33_preCT_16_1_1 
2024-05-15 06:52:20.625499: B_33_preCT_16_1_1, shape torch.Size([1, 43, 195, 289]), rank 0 
2024-05-15 06:52:21.591070: predicting B_33_preCT_16_1_2 
2024-05-15 06:52:21.591889: B_33_preCT_16_1_2, shape torch.Size([1, 43, 195, 289]), rank 0 
2024-05-15 06:52:22.558772: predicting B_33_preCT_16_2_2 
2024-05-15 06:52:22.559592: B_33_preCT_16_2_2, shape torch.Size([1, 42, 195, 289]), rank 0 
2024-05-15 06:52:23.524658: predicting B_33_preCT_4_2_0 
2024-05-15 06:52:23.525476: B_33_preCT_4_2_0, shape torch.Size([1, 43, 195, 289]), rank 0 
2024-05-15 06:52:24.491776: predicting B_33_preCT_4_3_0 
2024-05-15 06:52:24.492923: B_33_preCT_4_3_0, shape torch.Size([1, 43, 195, 289]), rank 0 
2024-05-15 06:52:25.457447: predicting B_33_preCT_4_3_2 
2024-05-15 06:52:25.458252: B_33_preCT_4_3_2, shape torch.Size([1, 43, 195, 289]), rank 0 
2024-05-15 06:52:26.423637: predicting B_33_preCT_8_3_0 
2024-05-15 06:52:26.424432: B_33_preCT_8_3_0, shape torch.Size([1, 43, 195, 289]), rank 0 
2024-05-15 06:52:27.389895: predicting B_33_preCT_8_3_2 
2024-05-15 06:52:27.390737: B_33_preCT_8_3_2, shape torch.Size([1, 41, 195, 289]), rank 0 
2024-05-15 06:52:28.355375: predicting B_34_postCT_16_1_0 
2024-05-15 06:52:28.356190: B_34_postCT_16_1_0, shape torch.Size([1, 42, 179, 244]), rank 0 
2024-05-15 06:52:28.606966: predicting B_34_postCT_4_3_2 
2024-05-15 06:52:28.607761: B_34_postCT_4_3_2, shape torch.Size([1, 42, 179, 244]), rank 0 
2024-05-15 06:52:28.859180: predicting B_34_postCT_8_2_0 
2024-05-15 06:52:28.859954: B_34_postCT_8_2_0, shape torch.Size([1, 42, 179, 244]), rank 0 
2024-05-15 06:52:29.111348: predicting B_34_postCT_8_3_2 
2024-05-15 06:52:29.112094: B_34_postCT_8_3_2, shape torch.Size([1, 42, 179, 244]), rank 0 
2024-05-15 06:52:29.363646: predicting B_34_preCT_16_2_2 
2024-05-15 06:52:29.364373: B_34_preCT_16_2_2, shape torch.Size([1, 40, 175, 244]), rank 0 
2024-05-15 06:52:29.616195: predicting B_34_preCT_4_3_0 
2024-05-15 06:52:29.616942: B_34_preCT_4_3_0, shape torch.Size([1, 40, 175, 244]), rank 0 
2024-05-15 06:52:29.868084: predicting B_34_preCT_8_3_2 
2024-05-15 06:52:29.868868: B_34_preCT_8_3_2, shape torch.Size([1, 40, 175, 244]), rank 0 
2024-05-15 06:52:30.120054: predicting B_36_postCT_16_1_0 
2024-05-15 06:52:30.120793: B_36_postCT_16_1_0, shape torch.Size([1, 40, 174, 237]), rank 0 
2024-05-15 06:52:30.371527: predicting B_36_postCT_16_2_0 
2024-05-15 06:52:30.372532: B_36_postCT_16_2_0, shape torch.Size([1, 40, 174, 237]), rank 0 
2024-05-15 06:52:30.623907: predicting B_36_postCT_16_2_2 
2024-05-15 06:52:30.624647: B_36_postCT_16_2_2, shape torch.Size([1, 40, 174, 237]), rank 0 
2024-05-15 06:52:30.875050: predicting B_36_postCT_4_3_1 
2024-05-15 06:52:30.875801: B_36_postCT_4_3_1, shape torch.Size([1, 40, 174, 237]), rank 0 
2024-05-15 06:52:31.127007: predicting B_36_postCT_8_2_1 
2024-05-15 06:52:31.128055: B_36_postCT_8_2_1, shape torch.Size([1, 40, 174, 237]), rank 0 
2024-05-15 06:52:31.378762: predicting B_40_postCT_4_3_1 
2024-05-15 06:52:31.379507: B_40_postCT_4_3_1, shape torch.Size([1, 46, 159, 264]), rank 0 
2024-05-15 06:52:31.869453: predicting B_40_postCT_8_2_2 
2024-05-15 06:52:31.870496: B_40_postCT_8_2_2, shape torch.Size([1, 46, 159, 264]), rank 0 
2024-05-15 06:52:32.358926: predicting B_40_postCT_8_3_0 
2024-05-15 06:52:32.359714: B_40_postCT_8_3_0, shape torch.Size([1, 46, 159, 264]), rank 0 
2024-05-15 06:52:32.849520: predicting B_41_preCT_16_1_1 
2024-05-15 06:52:32.850268: B_41_preCT_16_1_1, shape torch.Size([1, 48, 218, 269]), rank 0 
2024-05-15 06:52:33.815988: predicting B_41_preCT_16_1_2 
2024-05-15 06:52:33.816774: B_41_preCT_16_1_2, shape torch.Size([1, 48, 218, 269]), rank 0 
2024-05-15 06:52:34.782224: predicting B_41_preCT_4_2_1 
2024-05-15 06:52:34.783054: B_41_preCT_4_2_1, shape torch.Size([1, 48, 218, 269]), rank 0 
2024-05-15 06:52:35.749065: predicting B_41_preCT_8_2_2 
2024-05-15 06:52:35.749878: B_41_preCT_8_2_2, shape torch.Size([1, 48, 218, 269]), rank 0 
2024-05-15 06:52:36.714131: predicting B_41_preCT_8_3_0 
2024-05-15 06:52:36.715217: B_41_preCT_8_3_0, shape torch.Size([1, 48, 218, 269]), rank 0 
2024-05-15 06:52:37.680329: predicting B_44_preCT_16_1_1 
2024-05-15 06:52:37.681132: B_44_preCT_16_1_1, shape torch.Size([1, 36, 146, 199]), rank 0 
2024-05-15 06:52:37.930124: predicting B_44_preCT_16_2_0 
2024-05-15 06:52:37.930874: B_44_preCT_16_2_0, shape torch.Size([1, 36, 146, 199]), rank 0 
2024-05-15 06:52:38.181829: predicting B_44_preCT_16_2_2 
2024-05-15 06:52:38.182546: B_44_preCT_16_2_2, shape torch.Size([1, 36, 146, 199]), rank 0 
2024-05-15 06:52:38.433275: predicting B_44_preCT_8_3_2 
2024-05-15 06:52:38.433954: B_44_preCT_8_3_2, shape torch.Size([1, 36, 146, 199]), rank 0 
2024-05-15 06:52:38.685079: predicting B_45_preCT_16_1_0 
2024-05-15 06:52:38.685830: B_45_preCT_16_1_0, shape torch.Size([1, 40, 163, 239]), rank 0 
2024-05-15 06:52:38.937797: predicting B_45_preCT_4_3_0 
2024-05-15 06:52:38.938584: B_45_preCT_4_3_0, shape torch.Size([1, 40, 163, 239]), rank 0 
2024-05-15 06:52:39.189775: predicting B_45_preCT_8_2_2 
2024-05-15 06:52:39.190511: B_45_preCT_8_2_2, shape torch.Size([1, 40, 163, 239]), rank 0 
2024-05-15 06:52:39.441313: predicting CU_04_postCT_4_2_1 
2024-05-15 06:52:39.442356: CU_04_postCT_4_2_1, shape torch.Size([1, 42, 123, 229]), rank 0 
2024-05-15 06:52:39.692998: predicting CU_05_postCT_16_1_1 
2024-05-15 06:52:39.693707: CU_05_postCT_16_1_1, shape torch.Size([1, 37, 135, 217]), rank 0 
2024-05-15 06:52:39.944138: predicting CU_05_postCT_16_1_2 
2024-05-15 06:52:39.944862: CU_05_postCT_16_1_2, shape torch.Size([1, 37, 135, 217]), rank 0 
2024-05-15 06:52:40.197339: predicting CU_05_postCT_4_2_2 
2024-05-15 06:52:40.198160: CU_05_postCT_4_2_2, shape torch.Size([1, 37, 135, 217]), rank 0 
2024-05-15 06:52:40.448110: predicting CU_05_postCT_8_3_0 
2024-05-15 06:52:40.449145: CU_05_postCT_8_3_0, shape torch.Size([1, 37, 135, 217]), rank 0 
2024-05-15 06:52:40.700664: predicting CU_05_postCT_8_3_2 
2024-05-15 06:52:40.701397: CU_05_postCT_8_3_2, shape torch.Size([1, 37, 135, 217]), rank 0 
2024-05-15 06:52:40.952054: predicting CU_05_preCT_16_2_0 
2024-05-15 06:52:40.952804: CU_05_preCT_16_2_0, shape torch.Size([1, 32, 138, 215]), rank 0 
2024-05-15 06:52:41.203756: predicting CU_05_preCT_4_2_1 
2024-05-15 06:52:41.204476: CU_05_preCT_4_2_1, shape torch.Size([1, 32, 138, 215]), rank 0 
2024-05-15 06:52:41.455804: predicting CU_05_preCT_4_2_2 
2024-05-15 06:52:41.456507: CU_05_preCT_4_2_2, shape torch.Size([1, 32, 138, 215]), rank 0 
2024-05-15 06:52:41.707217: predicting CU_14_postCT_16_1_0 
2024-05-15 06:52:41.708100: CU_14_postCT_16_1_0, shape torch.Size([1, 34, 151, 225]), rank 0 
2024-05-15 06:52:41.961207: predicting CU_14_postCT_4_2_0 
2024-05-15 06:52:41.961966: CU_14_postCT_4_2_0, shape torch.Size([1, 34, 151, 225]), rank 0 
2024-05-15 06:52:42.213241: predicting CU_14_postCT_4_3_0 
2024-05-15 06:52:42.213977: CU_14_postCT_4_3_0, shape torch.Size([1, 34, 151, 225]), rank 0 
2024-05-15 06:52:42.464696: predicting CU_14_postCT_8_2_0 
2024-05-15 06:52:42.465419: CU_14_postCT_8_2_0, shape torch.Size([1, 34, 151, 225]), rank 0 
2024-05-15 06:52:42.716332: predicting CU_21_postCT_16_1_1 
2024-05-15 06:52:42.717056: CU_21_postCT_16_1_1, shape torch.Size([1, 38, 123, 212]), rank 0 
2024-05-15 06:52:42.967645: predicting CU_21_postCT_16_1_2 
2024-05-15 06:52:42.968370: CU_21_postCT_16_1_2, shape torch.Size([1, 38, 123, 212]), rank 0 
2024-05-15 06:52:43.220222: predicting CU_21_preCT_0_0_0 
2024-05-15 06:52:43.220872: CU_21_preCT_0_0_0, shape torch.Size([1, 36, 119, 197]), rank 0 
2024-05-15 06:52:43.471444: predicting CU_24_preCT_16_1_2 
2024-05-15 06:52:43.472258: CU_24_preCT_16_1_2, shape torch.Size([1, 49, 190, 213]), rank 0 
2024-05-15 06:52:43.962058: predicting CU_24_preCT_16_2_1 
2024-05-15 06:52:43.962857: CU_24_preCT_16_2_1, shape torch.Size([1, 49, 190, 213]), rank 0 
2024-05-15 06:52:44.452741: predicting CU_24_preCT_16_2_2 
2024-05-15 06:52:44.453475: CU_24_preCT_16_2_2, shape torch.Size([1, 49, 190, 213]), rank 0 
2024-05-15 06:52:44.943761: predicting CU_24_preCT_4_3_1 
2024-05-15 06:52:44.944549: CU_24_preCT_4_3_1, shape torch.Size([1, 49, 190, 213]), rank 0 
2024-05-15 06:52:45.434178: predicting CU_24_preCT_8_2_1 
2024-05-15 06:52:45.435021: CU_24_preCT_8_2_1, shape torch.Size([1, 49, 190, 213]), rank 0 
2024-05-15 06:52:45.926533: predicting CU_39_postCT_16_1_1 
2024-05-15 06:52:45.927347: CU_39_postCT_16_1_1, shape torch.Size([1, 52, 166, 237]), rank 0 
2024-05-15 06:52:46.417665: predicting CU_39_postCT_16_1_2 
2024-05-15 06:52:46.418484: CU_39_postCT_16_1_2, shape torch.Size([1, 52, 166, 237]), rank 0 
2024-05-15 06:52:46.908205: predicting CU_39_postCT_4_3_1 
2024-05-15 06:52:46.909143: CU_39_postCT_4_3_1, shape torch.Size([1, 52, 166, 237]), rank 0 
2024-05-15 06:52:47.400688: predicting CU_39_postCT_4_3_2 
2024-05-15 06:52:47.401498: CU_39_postCT_4_3_2, shape torch.Size([1, 52, 166, 237]), rank 0 
2024-05-15 06:52:47.891192: predicting CU_39_postCT_8_2_0 
2024-05-15 06:52:47.892013: CU_39_postCT_8_2_0, shape torch.Size([1, 52, 166, 237]), rank 0 
2024-05-15 06:52:48.380806: predicting CU_50_postCT_0_0_0 
2024-05-15 06:52:48.381502: CU_50_postCT_0_0_0, shape torch.Size([1, 43, 163, 241]), rank 0 
2024-05-15 06:52:48.632191: predicting CU_52_postCT_16_2_1 
2024-05-15 06:52:48.633270: CU_52_postCT_16_2_1, shape torch.Size([1, 40, 171, 209]), rank 0 
2024-05-15 06:52:48.884216: predicting CU_52_postCT_16_2_2 
2024-05-15 06:52:48.884947: CU_52_postCT_16_2_2, shape torch.Size([1, 40, 171, 209]), rank 0 
2024-05-15 06:52:49.137052: predicting CU_52_postCT_4_2_1 
2024-05-15 06:52:49.137820: CU_52_postCT_4_2_1, shape torch.Size([1, 40, 171, 209]), rank 0 
2024-05-15 06:52:49.388742: predicting CU_52_postCT_8_2_0 
2024-05-15 06:52:49.389506: CU_52_postCT_8_2_0, shape torch.Size([1, 40, 171, 209]), rank 0 
2024-05-15 06:52:49.640889: predicting Lime_01_01_16_1_0 
2024-05-15 06:52:49.641627: Lime_01_01_16_1_0, shape torch.Size([1, 38, 154, 209]), rank 0 
2024-05-15 06:52:49.892373: predicting Lime_01_01_4_2_0 
2024-05-15 06:52:49.893430: Lime_01_01_4_2_0, shape torch.Size([1, 38, 154, 209]), rank 0 
2024-05-15 06:52:50.144371: predicting Lime_01_01_4_2_1 
2024-05-15 06:52:50.145128: Lime_01_01_4_2_1, shape torch.Size([1, 38, 154, 209]), rank 0 
2024-05-15 06:52:50.396259: predicting Lime_01_01_4_3_0 
2024-05-15 06:52:50.396981: Lime_01_01_4_3_0, shape torch.Size([1, 38, 154, 209]), rank 0 
2024-05-15 06:52:50.647933: predicting Lime_01_01_8_2_2 
2024-05-15 06:52:50.648657: Lime_01_01_8_2_2, shape torch.Size([1, 38, 154, 209]), rank 0 
2024-05-15 06:52:50.899638: predicting Lime_05_00_16_1_2 
2024-05-15 06:52:50.900383: Lime_05_00_16_1_2, shape torch.Size([1, 46, 207, 308]), rank 0 
2024-05-15 06:52:51.866868: predicting Lime_05_00_16_2_1 
2024-05-15 06:52:51.867963: Lime_05_00_16_2_1, shape torch.Size([1, 46, 207, 308]), rank 0 
2024-05-15 06:52:52.835232: predicting Lime_05_00_4_3_0 
2024-05-15 06:52:52.836072: Lime_05_00_4_3_0, shape torch.Size([1, 46, 207, 308]), rank 0 
2024-05-15 06:52:53.802224: predicting Lime_05_00_8_2_0 
2024-05-15 06:52:53.803049: Lime_05_00_8_2_0, shape torch.Size([1, 46, 207, 308]), rank 0 
2024-05-15 06:52:54.769294: predicting Lime_05_01_16_1_0 
2024-05-15 06:52:54.770142: Lime_05_01_16_1_0, shape torch.Size([1, 40, 198, 299]), rank 0 
2024-05-15 06:52:55.734498: predicting Lime_05_01_16_2_1 
2024-05-15 06:52:55.735330: Lime_05_01_16_2_1, shape torch.Size([1, 40, 198, 299]), rank 0 
2024-05-15 06:52:56.703132: predicting Lime_05_01_16_2_2 
2024-05-15 06:52:56.704244: Lime_05_01_16_2_2, shape torch.Size([1, 40, 198, 299]), rank 0 
2024-05-15 06:52:57.669678: predicting Lime_05_01_4_2_1 
2024-05-15 06:52:57.670787: Lime_05_01_4_2_1, shape torch.Size([1, 40, 198, 299]), rank 0 
2024-05-15 06:52:58.636888: predicting Lime_05_01_4_2_2 
2024-05-15 06:52:58.637683: Lime_05_01_4_2_2, shape torch.Size([1, 40, 198, 299]), rank 0 
2024-05-15 06:52:59.603598: predicting Lime_05_01_4_3_0 
2024-05-15 06:52:59.604421: Lime_05_01_4_3_0, shape torch.Size([1, 40, 198, 299]), rank 0 
2024-05-15 06:53:00.570081: predicting Lime_05_01_8_2_2 
2024-05-15 06:53:00.571181: Lime_05_01_8_2_2, shape torch.Size([1, 40, 198, 299]), rank 0 
2024-05-15 06:53:01.535581: predicting Lime_05_01_8_3_0 
2024-05-15 06:53:01.536668: Lime_05_01_8_3_0, shape torch.Size([1, 40, 198, 299]), rank 0 
2024-05-15 06:53:02.504505: predicting Lime_05_01_8_3_2 
2024-05-15 06:53:02.505331: Lime_05_01_8_3_2, shape torch.Size([1, 40, 198, 299]), rank 0 
2024-05-15 06:53:03.471430: predicting Lime_06_00_8_2_2 
2024-05-15 06:53:03.472244: Lime_06_00_8_2_2, shape torch.Size([1, 40, 158, 217]), rank 0 
2024-05-15 06:53:03.721941: predicting Lime_06_00_8_3_1 
2024-05-15 06:53:03.722708: Lime_06_00_8_3_1, shape torch.Size([1, 40, 158, 217]), rank 0 
2024-05-15 06:53:03.973709: predicting Lime_06_01_4_2_1 
2024-05-15 06:53:03.974493: Lime_06_01_4_2_1, shape torch.Size([1, 38, 155, 212]), rank 0 
2024-05-15 06:53:04.225551: predicting Lime_06_01_8_2_0 
2024-05-15 06:53:04.226290: Lime_06_01_8_2_0, shape torch.Size([1, 38, 155, 212]), rank 0 
2024-05-15 06:53:04.478056: predicting Lime_06_01_8_3_0 
2024-05-15 06:53:04.478781: Lime_06_01_8_3_0, shape torch.Size([1, 38, 155, 212]), rank 0 
2024-05-15 06:53:04.729882: predicting Lime_14_01_8_2_0 
2024-05-15 06:53:04.730622: Lime_14_01_8_2_0, shape torch.Size([1, 40, 186, 277]), rank 0 
2024-05-15 06:53:05.220998: predicting Lime_14_01_8_2_1 
2024-05-15 06:53:05.221743: Lime_14_01_8_2_1, shape torch.Size([1, 40, 186, 277]), rank 0 
2024-05-15 06:53:05.711729: predicting Lime_14_01_8_2_2 
2024-05-15 06:53:05.712515: Lime_14_01_8_2_2, shape torch.Size([1, 40, 186, 277]), rank 0 
2024-05-15 06:53:06.201922: predicting Lime_14_01_8_3_0 
2024-05-15 06:53:06.202745: Lime_14_01_8_3_0, shape torch.Size([1, 40, 186, 277]), rank 0 
2024-05-15 06:53:06.692278: predicting Lime_16_00_4_3_1 
2024-05-15 06:53:06.693092: Lime_16_00_4_3_1, shape torch.Size([1, 47, 235, 314]), rank 0 
2024-05-15 06:53:07.661890: predicting Lime_16_00_8_3_2 
2024-05-15 06:53:07.662716: Lime_16_00_8_3_2, shape torch.Size([1, 47, 235, 314]), rank 0 
2024-05-15 06:53:08.630106: predicting Lime_20_01_16_2_2 
2024-05-15 06:53:08.630929: Lime_20_01_16_2_2, shape torch.Size([1, 46, 159, 264]), rank 0 
2024-05-15 06:53:09.118045: predicting Lime_20_01_4_2_1 
2024-05-15 06:53:09.119220: Lime_20_01_4_2_1, shape torch.Size([1, 46, 159, 264]), rank 0 
2024-05-15 06:53:09.608673: predicting Lime_20_01_4_2_2 
2024-05-15 06:53:09.609423: Lime_20_01_4_2_2, shape torch.Size([1, 46, 159, 264]), rank 0 
2024-05-15 06:53:10.099533: predicting Lime_20_01_4_3_1 
2024-05-15 06:53:10.100309: Lime_20_01_4_3_1, shape torch.Size([1, 46, 159, 264]), rank 0 
2024-05-15 06:53:10.590156: predicting Lime_20_01_4_3_2 
2024-05-15 06:53:10.590921: Lime_20_01_4_3_2, shape torch.Size([1, 46, 159, 264]), rank 0 
2024-05-15 06:53:11.080735: predicting Lime_20_01_8_2_2 
2024-05-15 06:53:11.081523: Lime_20_01_8_2_2, shape torch.Size([1, 46, 159, 264]), rank 0 
2024-05-15 06:53:11.570355: predicting Lime_20_01_8_3_0 
2024-05-15 06:53:11.571145: Lime_20_01_8_3_0, shape torch.Size([1, 46, 159, 264]), rank 0 
2024-05-15 06:53:12.059630: predicting Lime_28_01_16_1_0 
2024-05-15 06:53:12.060360: Lime_28_01_16_1_0, shape torch.Size([1, 46, 163, 228]), rank 0 
2024-05-15 06:53:12.310789: predicting Lime_28_01_4_3_1 
2024-05-15 06:53:12.311512: Lime_28_01_4_3_1, shape torch.Size([1, 46, 163, 228]), rank 0 
2024-05-15 06:53:12.562088: predicting Lime_28_01_8_3_2 
2024-05-15 06:53:12.562839: Lime_28_01_8_3_2, shape torch.Size([1, 46, 163, 228]), rank 0 
2024-05-15 06:53:12.814098: predicting Lime_34_00_4_2_0 
2024-05-15 06:53:12.814831: Lime_34_00_4_2_0, shape torch.Size([1, 40, 159, 225]), rank 0 
2024-05-15 06:53:13.065664: predicting Lime_34_00_4_3_2 
2024-05-15 06:53:13.066622: Lime_34_00_4_3_2, shape torch.Size([1, 40, 159, 225]), rank 0 
2024-05-15 06:53:13.318305: predicting Lime_34_00_8_2_0 
2024-05-15 06:53:13.319008: Lime_34_00_8_2_0, shape torch.Size([1, 40, 159, 225]), rank 0 
2024-05-15 06:53:13.570060: predicting Lime_34_00_8_3_2 
2024-05-15 06:53:13.571109: Lime_34_00_8_3_2, shape torch.Size([1, 40, 159, 225]), rank 0 
2024-05-15 06:53:13.822573: predicting Lime_35_00_4_3_0 
2024-05-15 06:53:13.823281: Lime_35_00_4_3_0, shape torch.Size([1, 44, 130, 223]), rank 0 
2024-05-15 06:53:14.073837: predicting Lime_35_00_8_2_0 
2024-05-15 06:53:14.074840: Lime_35_00_8_2_0, shape torch.Size([1, 44, 130, 223]), rank 0 
2024-05-15 06:53:14.325345: predicting Lime_35_00_8_2_2 
2024-05-15 06:53:14.326075: Lime_35_00_8_2_2, shape torch.Size([1, 46, 130, 223]), rank 0 
2024-05-15 06:53:14.576433: predicting P3_16_2_2 
2024-05-15 06:53:14.577127: P3_16_2_2, shape torch.Size([1, 68, 194, 309]), rank 0 
2024-05-15 06:53:16.493251: predicting P3_4_3_0 
2024-05-15 06:53:16.494112: P3_4_3_0, shape torch.Size([1, 68, 194, 309]), rank 0 
2024-05-15 06:53:18.410133: predicting P3_8_2_2 
2024-05-15 06:53:18.411294: P3_8_2_2, shape torch.Size([1, 67, 194, 309]), rank 0 
2024-05-15 06:53:20.326403: predicting P6_4_3_1 
2024-05-15 06:53:20.327308: P6_4_3_1, shape torch.Size([1, 63, 175, 217]), rank 0 
2024-05-15 06:53:20.815269: predicting P6_4_3_2 
2024-05-15 06:53:20.816061: P6_4_3_2, shape torch.Size([1, 63, 175, 217]), rank 0 
2024-05-15 06:53:21.305893: predicting P6_8_2_1 
2024-05-15 06:53:21.306987: P6_8_2_1, shape torch.Size([1, 61, 175, 217]), rank 0 
2024-05-15 06:53:21.798614: predicting Pt10_4_3_1 
2024-05-15 06:53:21.799788: Pt10_4_3_1, shape torch.Size([1, 53, 179, 233]), rank 0 
2024-05-15 06:53:22.289165: predicting Pt10_8_3_2 
2024-05-15 06:53:22.289944: Pt10_8_3_2, shape torch.Size([1, 53, 179, 233]), rank 0 
2024-05-15 06:53:22.779488: predicting Pt13_16_2_2 
2024-05-15 06:53:22.780493: Pt13_16_2_2, shape torch.Size([1, 52, 157, 236]), rank 0 
2024-05-15 06:53:23.270380: predicting Pt13_8_3_1 
2024-05-15 06:53:23.271166: Pt13_8_3_1, shape torch.Size([1, 52, 157, 236]), rank 0 
2024-05-15 06:53:23.760412: predicting Pt15_4_2_1 
2024-05-15 06:53:23.761415: Pt15_4_2_1, shape torch.Size([1, 46, 155, 235]), rank 0 
2024-05-15 06:53:24.011961: predicting Pt15_4_3_0 
2024-05-15 06:53:24.012677: Pt15_4_3_0, shape torch.Size([1, 46, 155, 235]), rank 0 
2024-05-15 06:53:24.263732: predicting Pt15_8_3_1 
2024-05-15 06:53:24.264733: Pt15_8_3_1, shape torch.Size([1, 45, 155, 235]), rank 0 
2024-05-15 06:53:24.516060: predicting Pt17_16_1_2 
2024-05-15 06:53:24.516787: Pt17_16_1_2, shape torch.Size([1, 52, 167, 225]), rank 0 
2024-05-15 06:53:25.007199: predicting Pt17_4_3_0 
2024-05-15 06:53:25.007926: Pt17_4_3_0, shape torch.Size([1, 52, 167, 225]), rank 0 
2024-05-15 06:53:25.497955: predicting Pt17_8_3_1 
2024-05-15 06:53:25.498752: Pt17_8_3_1, shape torch.Size([1, 52, 167, 225]), rank 0 
2024-05-15 06:53:25.988273: predicting Pt18_4_3_1 
2024-05-15 06:53:25.989065: Pt18_4_3_1, shape torch.Size([1, 50, 142, 201]), rank 0 
2024-05-15 06:53:26.478130: predicting Pt18_8_2_1 
2024-05-15 06:53:26.478904: Pt18_8_2_1, shape torch.Size([1, 50, 142, 201]), rank 0 
2024-05-15 06:53:26.967595: predicting Pt18_8_2_2 
2024-05-15 06:53:26.968653: Pt18_8_2_2, shape torch.Size([1, 50, 142, 201]), rank 0 
2024-05-15 06:53:27.457636: predicting Pt18_8_3_0 
2024-05-15 06:53:27.458695: Pt18_8_3_0, shape torch.Size([1, 50, 142, 201]), rank 0 
2024-05-15 06:53:27.946983: predicting Pt20_16_2_1 
2024-05-15 06:53:27.947698: Pt20_16_2_1, shape torch.Size([1, 58, 158, 229]), rank 0 
2024-05-15 06:53:28.437750: predicting Pt20_4_2_1 
2024-05-15 06:53:28.438550: Pt20_4_2_1, shape torch.Size([1, 58, 158, 229]), rank 0 
2024-05-15 06:53:28.928774: predicting Pt20_4_3_0 
2024-05-15 06:53:28.929559: Pt20_4_3_0, shape torch.Size([1, 58, 158, 229]), rank 0 
2024-05-15 06:53:29.418988: predicting Pt20_8_3_0 
2024-05-15 06:53:29.419763: Pt20_8_3_0, shape torch.Size([1, 58, 158, 229]), rank 0 
2024-05-15 06:53:29.909267: predicting Pt20_8_3_2 
2024-05-15 06:53:29.909995: Pt20_8_3_2, shape torch.Size([1, 58, 158, 229]), rank 0 
2024-05-15 06:53:30.398824: predicting Pt9_0_0_0 
2024-05-15 06:53:30.399515: Pt9_0_0_0, shape torch.Size([1, 46, 151, 189]), rank 0 
2024-05-15 06:53:30.649885: predicting case1_4_2_0 
2024-05-15 06:53:30.650895: case1_4_2_0, shape torch.Size([1, 41, 157, 242]), rank 0 
2024-05-15 06:53:30.902229: predicting case1_4_2_1 
2024-05-15 06:53:30.902950: case1_4_2_1, shape torch.Size([1, 41, 157, 242]), rank 0 
2024-05-15 06:53:31.153298: predicting case1_4_2_2 
2024-05-15 06:53:31.154319: case1_4_2_2, shape torch.Size([1, 41, 157, 242]), rank 0 
2024-05-15 06:53:31.405881: predicting case1_4_3_0 
2024-05-15 06:53:31.406622: case1_4_3_0, shape torch.Size([1, 41, 157, 242]), rank 0 
2024-05-15 06:53:31.657343: predicting case1_4_3_2 
2024-05-15 06:53:31.658354: case1_4_3_2, shape torch.Size([1, 41, 157, 242]), rank 0 
2024-05-15 06:53:31.910038: predicting case1_8_3_1 
2024-05-15 06:53:31.910777: case1_8_3_1, shape torch.Size([1, 41, 157, 242]), rank 0 
2024-05-15 06:53:32.161767: predicting case4_4_3_2 
2024-05-15 06:53:32.162781: case4_4_3_2, shape torch.Size([1, 44, 165, 252]), rank 0 
2024-05-15 06:53:32.414226: predicting case4_8_3_1 
2024-05-15 06:53:32.415274: case4_8_3_1, shape torch.Size([1, 44, 165, 252]), rank 0 
2024-05-15 06:53:32.667028: predicting case9_16_1_0 
2024-05-15 06:53:32.667748: case9_16_1_0, shape torch.Size([1, 38, 191, 256]), rank 0 
2024-05-15 06:53:32.918808: predicting case9_16_1_1 
2024-05-15 06:53:32.919542: case9_16_1_1, shape torch.Size([1, 39, 191, 256]), rank 0 
2024-05-15 06:53:33.170301: predicting case9_4_2_1 
2024-05-15 06:53:33.171041: case9_4_2_1, shape torch.Size([1, 38, 191, 256]), rank 0 
2024-05-15 06:53:33.422424: predicting case9_4_2_2 
2024-05-15 06:53:33.423164: case9_4_2_2, shape torch.Size([1, 39, 191, 256]), rank 0 
2024-05-15 06:53:33.673856: predicting case9_8_2_0 
2024-05-15 06:53:33.674596: case9_8_2_0, shape torch.Size([1, 39, 191, 256]), rank 0 
2024-05-15 06:53:33.925344: predicting case_03_16_1_1 
2024-05-15 06:53:33.926103: case_03_16_1_1, shape torch.Size([1, 41, 238, 253]), rank 0 
2024-05-15 06:53:34.416945: predicting case_03_16_1_2 
2024-05-15 06:53:34.417732: case_03_16_1_2, shape torch.Size([1, 41, 238, 253]), rank 0 
2024-05-15 06:53:34.907121: predicting case_03_16_2_2 
2024-05-15 06:53:34.907907: case_03_16_2_2, shape torch.Size([1, 41, 238, 253]), rank 0 
2024-05-15 06:53:35.397523: predicting case_03_8_2_1 
2024-05-15 06:53:35.398275: case_03_8_2_1, shape torch.Size([1, 41, 238, 253]), rank 0 
2024-05-15 06:53:35.887606: predicting case_03_8_3_1 
2024-05-15 06:53:35.888363: case_03_8_3_1, shape torch.Size([1, 41, 238, 253]), rank 0 
2024-05-15 06:53:36.379501: predicting case_04_16_1_0 
2024-05-15 06:53:36.380275: case_04_16_1_0, shape torch.Size([1, 44, 190, 303]), rank 0 
2024-05-15 06:53:36.869364: predicting case_04_4_2_0 
2024-05-15 06:53:36.870517: case_04_4_2_0, shape torch.Size([1, 44, 190, 303]), rank 0 
2024-05-15 06:53:37.359666: predicting case_04_8_3_0 
2024-05-15 06:53:37.360428: case_04_8_3_0, shape torch.Size([1, 44, 190, 303]), rank 0 
2024-05-15 06:53:37.849902: predicting case_04_8_3_2 
2024-05-15 06:53:37.851069: case_04_8_3_2, shape torch.Size([1, 44, 190, 303]), rank 0 
2024-05-15 06:53:38.340988: predicting case_06_16_2_2 
2024-05-15 06:53:38.341777: case_06_16_2_2, shape torch.Size([1, 54, 182, 260]), rank 0 
2024-05-15 06:53:39.308834: predicting case_06_4_3_1 
2024-05-15 06:53:39.309627: case_06_4_3_1, shape torch.Size([1, 54, 182, 260]), rank 0 
2024-05-15 06:53:40.274166: predicting case_06_8_2_0 
2024-05-15 06:53:40.274963: case_06_8_2_0, shape torch.Size([1, 54, 182, 260]), rank 0 
2024-05-15 06:53:41.241843: predicting case_11_4_3_1 
2024-05-15 06:53:41.242687: case_11_4_3_1, shape torch.Size([1, 59, 162, 252]), rank 0 
2024-05-15 06:53:41.731913: predicting case_11_4_3_2 
2024-05-15 06:53:41.732696: case_11_4_3_2, shape torch.Size([1, 59, 162, 252]), rank 0 
2024-05-15 06:53:42.222548: predicting case_11_8_2_2 
2024-05-15 06:53:42.223315: case_11_8_2_2, shape torch.Size([1, 59, 162, 252]), rank 0 
2024-05-15 06:53:42.712475: predicting case_11_8_3_2 
2024-05-15 06:53:42.713593: case_11_8_3_2, shape torch.Size([1, 59, 162, 252]), rank 0 
2024-05-15 06:53:43.203036: predicting case_15_16_2_2 
2024-05-15 06:53:43.203809: case_15_16_2_2, shape torch.Size([1, 40, 203, 317]), rank 0 
2024-05-15 06:53:44.168808: predicting case_16_0_0_0 
2024-05-15 06:53:44.169513: case_16_0_0_0, shape torch.Size([1, 48, 182, 273]), rank 0 
2024-05-15 06:53:44.658440: predicting neg_01_4_3_2 
2024-05-15 06:53:44.659201: neg_01_4_3_2, shape torch.Size([1, 44, 223, 305]), rank 0 
2024-05-15 06:53:45.625876: predicting neg_02_4_2_1 
2024-05-15 06:53:45.626695: neg_02_4_2_1, shape torch.Size([1, 46, 211, 277]), rank 0 
2024-05-15 06:53:46.593544: predicting neg_02_4_3_2 
2024-05-15 06:53:46.594623: neg_02_4_3_2, shape torch.Size([1, 46, 211, 277]), rank 0 
2024-05-15 06:53:47.561495: predicting neg_02_8_2_2 
2024-05-15 06:53:47.562322: neg_02_8_2_2, shape torch.Size([1, 46, 211, 277]), rank 0 
2024-05-15 06:53:48.527824: predicting neg_09_16_2_0 
2024-05-15 06:53:48.528590: neg_09_16_2_0, shape torch.Size([1, 41, 197, 256]), rank 0 
2024-05-15 06:53:49.016424: predicting neg_09_4_2_0 
2024-05-15 06:53:49.017185: neg_09_4_2_0, shape torch.Size([1, 41, 197, 256]), rank 0 
2024-05-15 06:53:49.505815: predicting neg_09_4_3_0 
2024-05-15 06:53:49.506551: neg_09_4_3_0, shape torch.Size([1, 41, 197, 256]), rank 0 
2024-05-15 06:53:49.996475: predicting neg_11_4_2_1 
2024-05-15 06:53:49.997234: neg_11_4_2_1, shape torch.Size([1, 48, 183, 301]), rank 0 
2024-05-15 06:53:50.487763: predicting neg_11_4_3_1 
2024-05-15 06:53:50.488528: neg_11_4_3_1, shape torch.Size([1, 48, 183, 301]), rank 0 
2024-05-15 06:53:50.976908: predicting neg_11_4_3_2 
2024-05-15 06:53:50.977638: neg_11_4_3_2, shape torch.Size([1, 48, 183, 301]), rank 0 
2024-05-15 06:53:51.468117: predicting neg_11_8_2_0 
2024-05-15 06:53:51.468911: neg_11_8_2_0, shape torch.Size([1, 48, 183, 301]), rank 0 
2024-05-15 06:53:51.959111: predicting neg_17_16_1_1 
2024-05-15 06:53:51.959943: neg_17_16_1_1, shape torch.Size([1, 48, 171, 252]), rank 0 
2024-05-15 06:53:52.212238: predicting neg_18_16_2_2 
2024-05-15 06:53:52.213316: neg_18_16_2_2, shape torch.Size([1, 48, 203, 272]), rank 0 
2024-05-15 06:53:53.178910: predicting neg_18_4_2_1 
2024-05-15 06:53:53.179713: neg_18_4_2_1, shape torch.Size([1, 48, 203, 272]), rank 0 
2024-05-15 06:53:54.145682: predicting neg_18_8_2_0 
2024-05-15 06:53:54.146853: neg_18_8_2_0, shape torch.Size([1, 48, 203, 272]), rank 0 
2024-05-15 06:53:55.111084: predicting neg_24_16_2_2 
2024-05-15 06:53:55.112185: neg_24_16_2_2, shape torch.Size([1, 41, 182, 277]), rank 0 
2024-05-15 06:53:55.600871: predicting neg_24_8_2_1 
2024-05-15 06:53:55.601648: neg_24_8_2_1, shape torch.Size([1, 41, 182, 277]), rank 0 
2024-05-15 06:53:56.090404: predicting neg_24_8_3_2 
2024-05-15 06:53:56.091460: neg_24_8_3_2, shape torch.Size([1, 41, 182, 277]), rank 0 
2024-05-15 06:53:56.581682: predicting neg_28_4_2_0 
2024-05-15 06:53:56.582485: neg_28_4_2_0, shape torch.Size([1, 46, 207, 277]), rank 0 
2024-05-15 06:53:57.547664: predicting neg_28_8_2_1 
2024-05-15 06:53:57.548759: neg_28_8_2_1, shape torch.Size([1, 46, 207, 277]), rank 0 
2024-05-15 06:53:58.513947: predicting neg_28_8_3_0 
2024-05-15 06:53:58.514788: neg_28_8_3_0, shape torch.Size([1, 46, 207, 277]), rank 0 
2024-05-15 06:53:59.480232: predicting neg_28_8_3_1 
2024-05-15 06:53:59.481026: neg_28_8_3_1, shape torch.Size([1, 46, 207, 277]), rank 0 
2024-05-15 06:54:00.444550: predicting neg_36_4_2_2 
2024-05-15 06:54:00.445396: neg_36_4_2_2, shape torch.Size([1, 48, 207, 328]), rank 0 
2024-05-15 06:54:01.410649: predicting neg_36_8_2_2 
2024-05-15 06:54:01.411447: neg_36_8_2_2, shape torch.Size([1, 47, 207, 328]), rank 0 
2024-05-15 06:54:02.377452: predicting neg_38_16_2_0 
2024-05-15 06:54:02.378634: neg_38_16_2_0, shape torch.Size([1, 46, 171, 269]), rank 0 
2024-05-15 06:54:02.866025: predicting neg_38_4_2_1 
2024-05-15 06:54:02.866793: neg_38_4_2_1, shape torch.Size([1, 46, 171, 269]), rank 0 
2024-05-15 06:54:03.356833: predicting neg_38_8_2_2 
2024-05-15 06:54:03.357620: neg_38_8_2_2, shape torch.Size([1, 46, 171, 269]), rank 0 
2024-05-15 06:54:03.850688: predicting neg_38_8_3_0 
2024-05-15 06:54:03.851491: neg_38_8_3_0, shape torch.Size([1, 46, 171, 269]), rank 0 
2024-05-15 06:54:04.343723: predicting neg_38_8_3_1 
2024-05-15 06:54:04.344506: neg_38_8_3_1, shape torch.Size([1, 46, 171, 269]), rank 0 
2024-05-15 06:54:04.833013: predicting neg_47_4_2_1 
2024-05-15 06:54:04.833832: neg_47_4_2_1, shape torch.Size([1, 40, 175, 277]), rank 0 
2024-05-15 06:54:05.322581: predicting neg_47_4_3_1 
2024-05-15 06:54:05.323796: neg_47_4_3_1, shape torch.Size([1, 40, 175, 277]), rank 0 
2024-05-15 06:54:05.814122: predicting neg_47_8_2_1 
2024-05-15 06:54:05.814882: neg_47_8_2_1, shape torch.Size([1, 40, 175, 277]), rank 0 
2024-05-15 06:54:06.304384: predicting neg_47_8_3_2 
2024-05-15 06:54:06.305187: neg_47_8_3_2, shape torch.Size([1, 40, 175, 277]), rank 0 
2024-05-15 06:54:06.794260: predicting neg_56_4_2_0 
2024-05-15 06:54:06.795065: neg_56_4_2_0, shape torch.Size([1, 48, 195, 296]), rank 0 
2024-05-15 06:54:07.760780: predicting neg_56_8_2_2 
2024-05-15 06:54:07.761573: neg_56_8_2_2, shape torch.Size([1, 48, 195, 296]), rank 0 
2024-05-15 06:54:08.727206: predicting petGas_10_16_1_1 
2024-05-15 06:54:08.728023: petGas_10_16_1_1, shape torch.Size([1, 70, 223, 313]), rank 0 
2024-05-15 06:54:10.643269: predicting petGas_10_4_2_1 
2024-05-15 06:54:10.644187: petGas_10_4_2_1, shape torch.Size([1, 70, 223, 313]), rank 0 
2024-05-15 06:54:12.556410: predicting petGas_10_8_2_0 
2024-05-15 06:54:12.557314: petGas_10_8_2_0, shape torch.Size([1, 70, 223, 313]), rank 0 
2024-05-15 06:54:14.470782: predicting petGas_10_8_3_0 
2024-05-15 06:54:14.471705: petGas_10_8_3_0, shape torch.Size([1, 63, 223, 313]), rank 0 
2024-05-15 06:54:16.383761: predicting petGas_11_8_3_1 
2024-05-15 06:54:16.384997: petGas_11_8_3_1, shape torch.Size([1, 74, 161, 265]), rank 0 
2024-05-15 06:54:17.826137: predicting petGas_13_16_1_2 
2024-05-15 06:54:17.826958: petGas_13_16_1_2, shape torch.Size([1, 78, 165, 263]), rank 0 
2024-05-15 06:54:19.269558: predicting petGas_13_16_2_0 
2024-05-15 06:54:19.270408: petGas_13_16_2_0, shape torch.Size([1, 78, 165, 263]), rank 0 
2024-05-15 06:54:20.712314: predicting petGas_13_16_2_1 
2024-05-15 06:54:20.713125: petGas_13_16_2_1, shape torch.Size([1, 78, 165, 263]), rank 0 
2024-05-15 06:54:22.155545: predicting petGas_13_16_2_2 
2024-05-15 06:54:22.156376: petGas_13_16_2_2, shape torch.Size([1, 78, 165, 263]), rank 0 
2024-05-15 06:54:23.597749: predicting petGas_13_4_2_0 
2024-05-15 06:54:23.598908: petGas_13_4_2_0, shape torch.Size([1, 78, 165, 263]), rank 0 
2024-05-15 06:54:25.040658: predicting petGas_13_8_3_0 
2024-05-15 06:54:25.041487: petGas_13_8_3_0, shape torch.Size([1, 78, 165, 263]), rank 0 
2024-05-15 06:54:26.482915: predicting petGas_13_8_3_2 
2024-05-15 06:54:26.484102: petGas_13_8_3_2, shape torch.Size([1, 78, 165, 263]), rank 0 
2024-05-15 06:54:27.926564: predicting petGas_14_16_1_2 
2024-05-15 06:54:27.927752: petGas_14_16_1_2, shape torch.Size([1, 70, 195, 271]), rank 0 
2024-05-15 06:54:29.845576: predicting petGas_14_8_2_1 
2024-05-15 06:54:29.846411: petGas_14_8_2_1, shape torch.Size([1, 70, 195, 271]), rank 0 
2024-05-15 06:54:31.764035: predicting petGas_14_8_3_1 
2024-05-15 06:54:31.764877: petGas_14_8_3_1, shape torch.Size([1, 70, 195, 271]), rank 0 
2024-05-15 06:54:33.682023: predicting petGas_17_4_3_0 
2024-05-15 06:54:33.683170: petGas_17_4_3_0, shape torch.Size([1, 70, 183, 257]), rank 0 
2024-05-15 06:54:34.648561: predicting petGas_17_8_3_1 
2024-05-15 06:54:34.649684: petGas_17_8_3_1, shape torch.Size([1, 62, 183, 257]), rank 0 
2024-05-15 06:54:35.614821: predicting petGas_18_16_1_2 
2024-05-15 06:54:35.615668: petGas_18_16_1_2, shape torch.Size([1, 68, 223, 332]), rank 0 
2024-05-15 06:54:37.533442: predicting petGas_18_16_2_2 
2024-05-15 06:54:37.534343: petGas_18_16_2_2, shape torch.Size([1, 68, 223, 332]), rank 0 
2024-05-15 06:54:39.451835: predicting petGas_18_4_2_0 
2024-05-15 06:54:39.453102: petGas_18_4_2_0, shape torch.Size([1, 68, 223, 332]), rank 0 
2024-05-15 06:54:41.371049: predicting petGas_18_4_2_1 
2024-05-15 06:54:41.371953: petGas_18_4_2_1, shape torch.Size([1, 68, 223, 332]), rank 0 
2024-05-15 06:54:43.290904: predicting petGas_18_8_2_2 
2024-05-15 06:54:43.291791: petGas_18_8_2_2, shape torch.Size([1, 68, 223, 332]), rank 0 
2024-05-15 06:54:45.209098: predicting petGas_19_16_2_2 
2024-05-15 06:54:45.210342: petGas_19_16_2_2, shape torch.Size([1, 61, 163, 254]), rank 0 
2024-05-15 06:54:45.698364: predicting petGas_19_4_3_0 
2024-05-15 06:54:45.699149: petGas_19_4_3_0, shape torch.Size([1, 72, 163, 260]), rank 0 
2024-05-15 06:54:46.664581: predicting petGas_1_16_2_2 
2024-05-15 06:54:46.665367: petGas_1_16_2_2, shape torch.Size([1, 70, 199, 285]), rank 0 
2024-05-15 06:54:48.579597: predicting petGas_1_4_3_0 
2024-05-15 06:54:48.580766: petGas_1_4_3_0, shape torch.Size([1, 70, 199, 285]), rank 0 
2024-05-15 06:54:50.494479: predicting petGas_1_8_3_2 
2024-05-15 06:54:50.495646: petGas_1_8_3_2, shape torch.Size([1, 63, 199, 285]), rank 0 
2024-05-15 06:54:52.409531: predicting petGas_2_16_1_2 
2024-05-15 06:54:52.410785: petGas_2_16_1_2, shape torch.Size([1, 69, 227, 285]), rank 0 
2024-05-15 06:54:54.326581: predicting petGas_2_16_2_1 
2024-05-15 06:54:54.327486: petGas_2_16_2_1, shape torch.Size([1, 69, 227, 285]), rank 0 
2024-05-15 06:54:56.245652: predicting petGas_2_4_2_0 
2024-05-15 06:54:56.246516: petGas_2_4_2_0, shape torch.Size([1, 69, 227, 285]), rank 0 
2024-05-15 06:54:58.163442: predicting petGas_2_4_2_1 
2024-05-15 06:54:58.164297: petGas_2_4_2_1, shape torch.Size([1, 69, 227, 285]), rank 0 
2024-05-15 06:55:00.080616: predicting petGas_6_4_2_0 
2024-05-15 06:55:00.081873: petGas_6_4_2_0, shape torch.Size([1, 70, 227, 317]), rank 0 
2024-05-15 06:55:01.999361: predicting petGas_6_4_3_2 
2024-05-15 06:55:02.000251: petGas_6_4_3_2, shape torch.Size([1, 70, 227, 317]), rank 0 
2024-05-15 06:55:03.918133: predicting petGas_7_16_1_0 
2024-05-15 06:55:03.919025: petGas_7_16_1_0, shape torch.Size([1, 74, 202, 311]), rank 0 
2024-05-15 06:55:06.785510: predicting petGas_7_16_2_0 
2024-05-15 06:55:06.786402: petGas_7_16_2_0, shape torch.Size([1, 74, 202, 311]), rank 0 
2024-05-15 06:55:09.653944: predicting petGas_7_4_2_0 
2024-05-15 06:55:09.654812: petGas_7_4_2_0, shape torch.Size([1, 74, 202, 311]), rank 0 
2024-05-15 06:55:12.521636: predicting petGas_7_4_3_1 
2024-05-15 06:55:12.522883: petGas_7_4_3_1, shape torch.Size([1, 74, 202, 311]), rank 0 
2024-05-15 06:55:15.390646: predicting petGas_7_8_2_1 
2024-05-15 06:55:15.391531: petGas_7_8_2_1, shape torch.Size([1, 74, 202, 311]), rank 0 
2024-05-15 06:55:18.258474: predicting petGas_8_16_1_0 
2024-05-15 06:55:18.259673: petGas_8_16_1_0, shape torch.Size([1, 74, 183, 279]), rank 0 
2024-05-15 06:55:19.702845: predicting petGas_8_16_1_2 
2024-05-15 06:55:19.703688: petGas_8_16_1_2, shape torch.Size([1, 74, 183, 279]), rank 0 
2024-05-15 06:55:21.147551: predicting petGas_8_16_2_1 
2024-05-15 06:55:21.148382: petGas_8_16_2_1, shape torch.Size([1, 74, 183, 279]), rank 0 
2024-05-15 06:55:22.591541: predicting petGas_8_8_2_1 
2024-05-15 06:55:22.592360: petGas_8_8_2_1, shape torch.Size([1, 74, 183, 279]), rank 0 
2024-05-15 06:55:24.035718: predicting petGas_8_8_3_0 
2024-05-15 06:55:24.036588: petGas_8_8_3_0, shape torch.Size([1, 74, 183, 279]), rank 0 
2024-05-15 06:55:25.481386: predicting petGas_9_16_1_1 
2024-05-15 06:55:25.482247: petGas_9_16_1_1, shape torch.Size([1, 65, 171, 279]), rank 0 
2024-05-15 06:55:26.448357: predicting petGas_9_4_2_2 
2024-05-15 06:55:26.449568: petGas_9_4_2_2, shape torch.Size([1, 66, 171, 279]), rank 0 
2024-05-15 06:55:27.415526: predicting petGas_9_8_3_1 
2024-05-15 06:55:27.416347: petGas_9_8_3_1, shape torch.Size([1, 66, 171, 279]), rank 0 
2024-05-15 06:55:28.382414: predicting pos_01_8_3_2 
2024-05-15 06:55:28.383272: pos_01_8_3_2, shape torch.Size([1, 41, 178, 272]), rank 0 
2024-05-15 06:55:28.870586: predicting pos_12_4_3_2 
2024-05-15 06:55:28.871681: pos_12_4_3_2, shape torch.Size([1, 48, 191, 273]), rank 0 
2024-05-15 06:55:29.361883: predicting pos_13_16_1_1 
2024-05-15 06:55:29.362688: pos_13_16_1_1, shape torch.Size([1, 46, 203, 288]), rank 0 
2024-05-15 06:55:30.328233: predicting pos_13_16_1_2 
2024-05-15 06:55:30.329004: pos_13_16_1_2, shape torch.Size([1, 46, 203, 288]), rank 0 
2024-05-15 06:55:31.293069: predicting pos_13_16_2_2 
2024-05-15 06:55:31.294096: pos_13_16_2_2, shape torch.Size([1, 46, 203, 288]), rank 0 
2024-05-15 06:55:32.259177: predicting pos_13_8_2_1 
2024-05-15 06:55:32.260000: pos_13_8_2_1, shape torch.Size([1, 46, 203, 288]), rank 0 
2024-05-15 06:55:33.223926: predicting pos_13_8_3_1 
2024-05-15 06:55:33.224723: pos_13_8_3_1, shape torch.Size([1, 46, 203, 288]), rank 0 
2024-05-15 06:55:34.188158: predicting pos_13_8_3_2 
2024-05-15 06:55:34.188943: pos_13_8_3_2, shape torch.Size([1, 46, 203, 288]), rank 0 
2024-05-15 06:55:35.152844: predicting pos_14_4_2_0 
2024-05-15 06:55:35.153645: pos_14_4_2_0, shape torch.Size([1, 40, 210, 340]), rank 0 
2024-05-15 06:55:36.118533: predicting pos_14_4_3_2 
2024-05-15 06:55:36.119359: pos_14_4_3_2, shape torch.Size([1, 40, 210, 340]), rank 0 
2024-05-15 06:55:37.084277: predicting pos_17_16_2_0 
2024-05-15 06:55:37.085068: pos_17_16_2_0, shape torch.Size([1, 48, 203, 306]), rank 0 
2024-05-15 06:55:38.049420: predicting pos_17_16_2_2 
2024-05-15 06:55:38.050209: pos_17_16_2_2, shape torch.Size([1, 48, 203, 306]), rank 0 
2024-05-15 06:55:39.014206: predicting pos_17_8_3_0 
2024-05-15 06:55:39.015310: pos_17_8_3_0, shape torch.Size([1, 48, 203, 306]), rank 0 
2024-05-15 06:55:39.979502: predicting pos_17_8_3_2 
2024-05-15 06:55:39.980359: pos_17_8_3_2, shape torch.Size([1, 48, 203, 306]), rank 0 
2024-05-15 06:55:40.944829: predicting pos_20_16_2_0 
2024-05-15 06:55:40.946016: pos_20_16_2_0, shape torch.Size([1, 48, 174, 275]), rank 0 
2024-05-15 06:55:41.434128: predicting pos_20_4_2_0 
2024-05-15 06:55:41.434914: pos_20_4_2_0, shape torch.Size([1, 48, 174, 275]), rank 0 
2024-05-15 06:55:41.924208: predicting pos_20_8_2_1 
2024-05-15 06:55:41.924979: pos_20_8_2_1, shape torch.Size([1, 48, 174, 275]), rank 0 
2024-05-15 06:55:42.414161: predicting pos_20_8_3_0 
2024-05-15 06:55:42.414926: pos_20_8_3_0, shape torch.Size([1, 48, 174, 275]), rank 0 
2024-05-15 06:55:42.903662: predicting pos_20_8_3_1 
2024-05-15 06:55:42.904455: pos_20_8_3_1, shape torch.Size([1, 48, 174, 275]), rank 0 
2024-05-15 06:55:43.393454: predicting pos_21_16_1_1 
2024-05-15 06:55:43.394205: pos_21_16_1_1, shape torch.Size([1, 48, 194, 281]), rank 0 
2024-05-15 06:55:44.359727: predicting pos_21_8_2_0 
2024-05-15 06:55:44.360491: pos_21_8_2_0, shape torch.Size([1, 48, 194, 281]), rank 0 
2024-05-15 06:55:45.325953: predicting pos_21_8_2_2 
2024-05-15 06:55:45.327065: pos_21_8_2_2, shape torch.Size([1, 48, 194, 281]), rank 0 
2024-05-15 06:55:46.291672: predicting pos_21_8_3_2 
2024-05-15 06:55:46.292471: pos_21_8_3_2, shape torch.Size([1, 48, 194, 281]), rank 0 
2024-05-15 06:55:47.257189: predicting pos_23_16_1_1 
2024-05-15 06:55:47.257956: pos_23_16_1_1, shape torch.Size([1, 48, 174, 263]), rank 0 
2024-05-15 06:55:47.745657: predicting pos_23_16_2_1 
2024-05-15 06:55:47.746446: pos_23_16_2_1, shape torch.Size([1, 48, 174, 263]), rank 0 
2024-05-15 06:55:48.234578: predicting pos_23_16_2_2 
2024-05-15 06:55:48.235342: pos_23_16_2_2, shape torch.Size([1, 48, 174, 263]), rank 0 
2024-05-15 06:55:48.724697: predicting pos_27_4_2_2 
2024-05-15 06:55:48.725471: pos_27_4_2_2, shape torch.Size([1, 48, 175, 261]), rank 0 
2024-05-15 06:55:49.215135: predicting pos_27_8_2_1 
2024-05-15 06:55:49.215906: pos_27_8_2_1, shape torch.Size([1, 48, 175, 261]), rank 0 
2024-05-15 06:55:49.705291: predicting pos_27_8_2_2 
2024-05-15 06:55:49.706052: pos_27_8_2_2, shape torch.Size([1, 48, 175, 261]), rank 0 
2024-05-15 06:55:50.196199: predicting pos_37_16_1_1 
2024-05-15 06:55:50.196972: pos_37_16_1_1, shape torch.Size([1, 43, 201, 257]), rank 0 
2024-05-15 06:55:51.162226: predicting pos_37_16_2_0 
2024-05-15 06:55:51.163047: pos_37_16_2_0, shape torch.Size([1, 43, 201, 257]), rank 0 
2024-05-15 06:55:52.128318: predicting pos_37_4_2_2 
2024-05-15 06:55:52.129112: pos_37_4_2_2, shape torch.Size([1, 43, 201, 257]), rank 0 
2024-05-15 06:55:53.094586: predicting pos_47_16_1_0 
2024-05-15 06:55:53.095375: pos_47_16_1_0, shape torch.Size([1, 48, 207, 257]), rank 0 
2024-05-15 06:55:54.059794: predicting pos_47_4_2_2 
2024-05-15 06:55:54.060606: pos_47_4_2_2, shape torch.Size([1, 48, 207, 257]), rank 0 
2024-05-15 06:55:55.025063: predicting pos_47_4_3_1 
2024-05-15 06:55:55.025863: pos_47_4_3_1, shape torch.Size([1, 48, 207, 257]), rank 0 
2024-05-15 06:55:55.990552: predicting pos_47_8_2_1 
2024-05-15 06:55:55.991369: pos_47_8_2_1, shape torch.Size([1, 48, 207, 257]), rank 0 
2024-05-15 06:55:56.956530: predicting pos_47_8_2_2 
2024-05-15 06:55:56.957303: pos_47_8_2_2, shape torch.Size([1, 48, 207, 257]), rank 0 
2024-05-15 06:55:57.920966: predicting pos_47_8_3_0 
2024-05-15 06:55:57.922029: pos_47_8_3_0, shape torch.Size([1, 48, 207, 257]), rank 0 
2024-05-15 06:55:58.886128: predicting pos_48_0_0_0 
2024-05-15 06:55:58.886864: pos_48_0_0_0, shape torch.Size([1, 47, 226, 312]), rank 0 
2024-05-15 06:55:59.854039: predicting pos_50_4_3_2 
2024-05-15 06:55:59.854892: pos_50_4_3_2, shape torch.Size([1, 48, 231, 317]), rank 0 
2024-05-15 06:56:00.819560: predicting pos_50_8_3_1 
2024-05-15 06:56:00.820400: pos_50_8_3_1, shape torch.Size([1, 48, 231, 317]), rank 0 
2024-05-15 06:56:07.311626: Validation complete 
2024-05-15 06:56:07.311723: Mean Validation Dice:  0.8620702520351569 
