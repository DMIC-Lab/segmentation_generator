
#######################################################################
Please cite the following paper when using nnU-Net:
Isensee, F., Jaeger, P. F., Kohl, S. A., Petersen, J., & Maier-Hein, K. H. (2021). nnU-Net: a self-configuring method for deep learning-based biomedical image segmentation. Nature methods, 18(2), 203-211.
#######################################################################
 

This is the configuration used by this training:
Configuration name: 3d_fullres
 {'data_identifier': 'nnUNetPlans_3d_fullres', 'preprocessor_name': 'DefaultPreprocessor', 'batch_size': 2, 'patch_size': [48, 192, 256], 'median_image_size_in_voxels': [43.0, 193.0, 264.0], 'spacing': [1.0, 1.0, 1.0], 'normalization_schemes': ['CTNormalization'], 'use_mask_for_norm': [False], 'resampling_fn_data': 'resample_data_or_seg_to_shape', 'resampling_fn_seg': 'resample_data_or_seg_to_shape', 'resampling_fn_data_kwargs': {'is_seg': False, 'order': 3, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_seg_kwargs': {'is_seg': True, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_probabilities': 'resample_data_or_seg_to_shape', 'resampling_fn_probabilities_kwargs': {'is_seg': False, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'architecture': {'network_class_name': 'dynamic_network_architectures.architectures.unet.PlainConvUNet', 'arch_kwargs': {'n_stages': 6, 'features_per_stage': [32, 64, 128, 256, 320, 320], 'conv_op': 'torch.nn.modules.conv.Conv3d', 'kernel_sizes': [[3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3]], 'strides': [[1, 1, 1], [2, 2, 2], [2, 2, 2], [2, 2, 2], [1, 2, 2], [1, 2, 2]], 'n_conv_per_stage': [2, 2, 2, 2, 2, 2], 'n_conv_per_stage_decoder': [2, 2, 2, 2, 2], 'conv_bias': True, 'norm_op': 'torch.nn.modules.instancenorm.InstanceNorm3d', 'norm_op_kwargs': {'eps': 1e-05, 'affine': True}, 'dropout_op': None, 'dropout_op_kwargs': None, 'nonlin': 'torch.nn.LeakyReLU', 'nonlin_kwargs': {'inplace': True}, 'deep_supervision': True}, '_kw_requires_import': ['conv_op', 'norm_op', 'dropout_op', 'nonlin']}, 'batch_dice': False} 
 
These are the global plan.json settings:
 {'dataset_name': 'Dataset022_COPD_addedArt', 'plans_name': 'nnUNetPlans', 'original_median_spacing_after_transp': [1.0, 1.0, 1.0], 'original_median_shape_after_transp': [43, 193, 264], 'image_reader_writer': 'SimpleITKIO', 'transpose_forward': [0, 1, 2], 'transpose_backward': [0, 1, 2], 'experiment_planner_used': 'ExperimentPlanner', 'label_manager': 'LabelManager', 'foreground_intensity_properties_per_channel': {'0': {'max': 1.0, 'mean': 0.4884437620639801, 'median': 0.5216540098190308, 'min': 0.0001169832466985099, 'percentile_00_5': 0.07381678372621536, 'percentile_99_5': 0.9124783784151074, 'std': 0.15860603749752045}}} 
 
2024-05-08 12:12:18.057485: unpacking dataset... 
2024-05-08 12:13:19.074055: unpacking done... 
2024-05-08 12:13:19.074777: do_dummy_2d_data_aug: True 
2024-05-08 12:13:19.113703: Creating new 5-fold cross-validation split... 
2024-05-08 12:13:19.162873: Desired fold for training: 0 
2024-05-08 12:13:19.162940: This split has 7033 training and 1759 validation cases. 
2024-05-08 12:13:19.371072: Unable to plot network architecture: 
2024-05-08 12:13:19.371149: No module named 'hiddenlayer' 
2024-05-08 12:13:19.432749:  
2024-05-08 12:13:19.432823: Epoch 0 
2024-05-08 12:13:19.432932: Current learning rate: 0.01 
2024-05-08 12:14:07.648024: train_loss -0.2842 
2024-05-08 12:14:07.648347: val_loss -0.417 
2024-05-08 12:14:07.648403: Pseudo dice [0.6626] 
2024-05-08 12:14:07.648467: Epoch time: 48.22 s 
2024-05-08 12:14:07.648513: Yayy! New best EMA pseudo Dice: 0.6626 
2024-05-08 12:14:08.820668:  
2024-05-08 12:14:08.820796: Epoch 1 
2024-05-08 12:14:08.820886: Current learning rate: 0.00999 
2024-05-08 12:14:49.420003: train_loss -0.4687 
2024-05-08 12:14:49.420173: val_loss -0.4154 
2024-05-08 12:14:49.420221: Pseudo dice [0.6417] 
2024-05-08 12:14:49.420273: Epoch time: 40.6 s 
2024-05-08 12:14:50.657733:  
2024-05-08 12:14:50.657855: Epoch 2 
2024-05-08 12:14:50.657949: Current learning rate: 0.00998 
2024-05-08 12:15:31.256509: train_loss -0.4811 
2024-05-08 12:15:31.256693: val_loss -0.5171 
2024-05-08 12:15:31.256742: Pseudo dice [0.7098] 
2024-05-08 12:15:31.256795: Epoch time: 40.6 s 
2024-05-08 12:15:31.256836: Yayy! New best EMA pseudo Dice: 0.6655 
2024-05-08 12:15:32.636592:  
2024-05-08 12:15:32.636735: Epoch 3 
2024-05-08 12:15:32.636831: Current learning rate: 0.00997 
2024-05-08 12:16:13.272516: train_loss -0.5009 
2024-05-08 12:16:13.272689: val_loss -0.5337 
2024-05-08 12:16:13.272738: Pseudo dice [0.7488] 
2024-05-08 12:16:13.272792: Epoch time: 40.64 s 
2024-05-08 12:16:13.272837: Yayy! New best EMA pseudo Dice: 0.6738 
2024-05-08 12:16:14.607107:  
2024-05-08 12:16:14.607241: Epoch 4 
2024-05-08 12:16:14.607334: Current learning rate: 0.00996 
2024-05-08 12:16:55.179938: train_loss -0.523 
2024-05-08 12:16:55.180113: val_loss -0.5836 
2024-05-08 12:16:55.180161: Pseudo dice [0.7728] 
2024-05-08 12:16:55.180213: Epoch time: 40.57 s 
2024-05-08 12:16:55.180255: Yayy! New best EMA pseudo Dice: 0.6837 
2024-05-08 12:16:56.585568:  
2024-05-08 12:16:56.585703: Epoch 5 
2024-05-08 12:16:56.585798: Current learning rate: 0.00995 
2024-05-08 12:17:37.602977: train_loss -0.5571 
2024-05-08 12:17:37.603171: val_loss -0.5759 
2024-05-08 12:17:37.603219: Pseudo dice [0.7581] 
2024-05-08 12:17:37.603273: Epoch time: 41.02 s 
2024-05-08 12:17:37.603314: Yayy! New best EMA pseudo Dice: 0.6911 
2024-05-08 12:17:39.259869:  
2024-05-08 12:17:39.259993: Epoch 6 
2024-05-08 12:17:39.260077: Current learning rate: 0.00995 
2024-05-08 12:18:20.125634: train_loss -0.5594 
2024-05-08 12:18:20.125803: val_loss -0.6208 
2024-05-08 12:18:20.125905: Pseudo dice [0.7915] 
2024-05-08 12:18:20.125957: Epoch time: 40.87 s 
2024-05-08 12:18:20.126001: Yayy! New best EMA pseudo Dice: 0.7012 
2024-05-08 12:18:21.509784:  
2024-05-08 12:18:21.510035: Epoch 7 
2024-05-08 12:18:21.510132: Current learning rate: 0.00994 
2024-05-08 12:19:02.260844: train_loss -0.5779 
2024-05-08 12:19:02.261004: val_loss -0.5826 
2024-05-08 12:19:02.261052: Pseudo dice [0.7745] 
2024-05-08 12:19:02.261103: Epoch time: 40.75 s 
2024-05-08 12:19:02.261145: Yayy! New best EMA pseudo Dice: 0.7085 
2024-05-08 12:19:03.651962:  
2024-05-08 12:19:03.652102: Epoch 8 
2024-05-08 12:19:03.652198: Current learning rate: 0.00993 
2024-05-08 12:19:44.936132: train_loss -0.595 
2024-05-08 12:19:44.936671: val_loss -0.6093 
2024-05-08 12:19:44.936735: Pseudo dice [0.7899] 
2024-05-08 12:19:44.936811: Epoch time: 41.29 s 
2024-05-08 12:19:44.936859: Yayy! New best EMA pseudo Dice: 0.7166 
2024-05-08 12:19:46.881554:  
2024-05-08 12:19:46.881924: Epoch 9 
2024-05-08 12:19:46.882029: Current learning rate: 0.00992 
2024-05-08 12:20:27.745756: train_loss -0.6025 
2024-05-08 12:20:27.745915: val_loss -0.6359 
2024-05-08 12:20:27.745962: Pseudo dice [0.7937] 
2024-05-08 12:20:27.746014: Epoch time: 40.87 s 
2024-05-08 12:20:27.746055: Yayy! New best EMA pseudo Dice: 0.7243 
2024-05-08 12:20:29.302845:  
2024-05-08 12:20:29.303040: Epoch 10 
2024-05-08 12:20:29.303132: Current learning rate: 0.00991 
2024-05-08 12:21:10.102858: train_loss -0.6299 
2024-05-08 12:21:10.103018: val_loss -0.6142 
2024-05-08 12:21:10.103065: Pseudo dice [0.7811] 
2024-05-08 12:21:10.103117: Epoch time: 40.8 s 
2024-05-08 12:21:10.103158: Yayy! New best EMA pseudo Dice: 0.73 
2024-05-08 12:21:11.474273:  
2024-05-08 12:21:11.474404: Epoch 11 
2024-05-08 12:21:11.474494: Current learning rate: 0.0099 
2024-05-08 12:21:52.281910: train_loss -0.6251 
2024-05-08 12:21:52.282079: val_loss -0.6468 
2024-05-08 12:21:52.282125: Pseudo dice [0.7972] 
2024-05-08 12:21:52.282176: Epoch time: 40.81 s 
2024-05-08 12:21:52.282218: Yayy! New best EMA pseudo Dice: 0.7367 
2024-05-08 12:21:53.688162:  
2024-05-08 12:21:53.688339: Epoch 12 
2024-05-08 12:21:53.688451: Current learning rate: 0.00989 
2024-05-08 12:22:34.509065: train_loss -0.6288 
2024-05-08 12:22:34.509237: val_loss -0.6654 
2024-05-08 12:22:34.509284: Pseudo dice [0.8163] 
2024-05-08 12:22:34.509336: Epoch time: 40.82 s 
2024-05-08 12:22:34.509377: Yayy! New best EMA pseudo Dice: 0.7447 
2024-05-08 12:22:35.884407:  
2024-05-08 12:22:35.884602: Epoch 13 
2024-05-08 12:22:35.884700: Current learning rate: 0.00988 
2024-05-08 12:23:16.698970: train_loss -0.6364 
2024-05-08 12:23:16.699140: val_loss -0.6719 
2024-05-08 12:23:16.699187: Pseudo dice [0.8221] 
2024-05-08 12:23:16.699238: Epoch time: 40.82 s 
2024-05-08 12:23:16.699280: Yayy! New best EMA pseudo Dice: 0.7524 
2024-05-08 12:23:18.236722:  
2024-05-08 12:23:18.236845: Epoch 14 
2024-05-08 12:23:18.236934: Current learning rate: 0.00987 
2024-05-08 12:23:59.066187: train_loss -0.648 
2024-05-08 12:23:59.066346: val_loss -0.6374 
2024-05-08 12:23:59.066396: Pseudo dice [0.7906] 
2024-05-08 12:23:59.066448: Epoch time: 40.83 s 
2024-05-08 12:23:59.066491: Yayy! New best EMA pseudo Dice: 0.7562 
2024-05-08 12:24:00.443325:  
2024-05-08 12:24:00.443454: Epoch 15 
2024-05-08 12:24:00.443542: Current learning rate: 0.00986 
2024-05-08 12:24:41.266450: train_loss -0.651 
2024-05-08 12:24:41.266621: val_loss -0.6842 
2024-05-08 12:24:41.266670: Pseudo dice [0.8226] 
2024-05-08 12:24:41.266721: Epoch time: 40.82 s 
2024-05-08 12:24:41.266763: Yayy! New best EMA pseudo Dice: 0.7629 
2024-05-08 12:24:42.670742:  
2024-05-08 12:24:42.670860: Epoch 16 
2024-05-08 12:24:42.670948: Current learning rate: 0.00986 
2024-05-08 12:25:23.543595: train_loss -0.6485 
2024-05-08 12:25:23.543760: val_loss -0.6875 
2024-05-08 12:25:23.543808: Pseudo dice [0.8313] 
2024-05-08 12:25:23.543861: Epoch time: 40.87 s 
2024-05-08 12:25:23.543903: Yayy! New best EMA pseudo Dice: 0.7697 
2024-05-08 12:25:24.962079:  
2024-05-08 12:25:24.962210: Epoch 17 
2024-05-08 12:25:24.962300: Current learning rate: 0.00985 
2024-05-08 12:26:05.835337: train_loss -0.6709 
2024-05-08 12:26:05.835514: val_loss -0.6884 
2024-05-08 12:26:05.835560: Pseudo dice [0.83] 
2024-05-08 12:26:05.835612: Epoch time: 40.87 s 
2024-05-08 12:26:05.835657: Yayy! New best EMA pseudo Dice: 0.7758 
2024-05-08 12:26:07.247829:  
2024-05-08 12:26:07.247941: Epoch 18 
2024-05-08 12:26:07.248029: Current learning rate: 0.00984 
2024-05-08 12:26:48.546572: train_loss -0.683 
2024-05-08 12:26:48.546813: val_loss -0.6723 
2024-05-08 12:26:48.546865: Pseudo dice [0.8212] 
2024-05-08 12:26:48.546925: Epoch time: 41.3 s 
2024-05-08 12:26:48.546968: Yayy! New best EMA pseudo Dice: 0.7803 
2024-05-08 12:26:50.071606:  
2024-05-08 12:26:50.071725: Epoch 19 
2024-05-08 12:26:50.071811: Current learning rate: 0.00983 
2024-05-08 12:27:31.022480: train_loss -0.677 
2024-05-08 12:27:31.022647: val_loss -0.6945 
2024-05-08 12:27:31.022694: Pseudo dice [0.8363] 
2024-05-08 12:27:31.022746: Epoch time: 40.95 s 
2024-05-08 12:27:31.022788: Yayy! New best EMA pseudo Dice: 0.7859 
2024-05-08 12:27:32.614517:  
2024-05-08 12:27:32.614702: Epoch 20 
2024-05-08 12:27:32.614793: Current learning rate: 0.00982 
2024-05-08 12:28:14.011976: train_loss -0.6876 
2024-05-08 12:28:14.012137: val_loss -0.6977 
2024-05-08 12:28:14.012184: Pseudo dice [0.8372] 
2024-05-08 12:28:14.012235: Epoch time: 41.4 s 
2024-05-08 12:28:14.012275: Yayy! New best EMA pseudo Dice: 0.791 
2024-05-08 12:28:15.446391:  
2024-05-08 12:28:15.446520: Epoch 21 
2024-05-08 12:28:15.446608: Current learning rate: 0.00981 
2024-05-08 12:28:56.395514: train_loss -0.6999 
2024-05-08 12:28:56.395703: val_loss -0.7036 
2024-05-08 12:28:56.395752: Pseudo dice [0.8478] 
2024-05-08 12:28:56.395809: Epoch time: 40.95 s 
2024-05-08 12:28:56.395857: Yayy! New best EMA pseudo Dice: 0.7967 
2024-05-08 12:28:57.767562:  
2024-05-08 12:28:57.767693: Epoch 22 
2024-05-08 12:28:57.767786: Current learning rate: 0.0098 
2024-05-08 12:29:38.725294: train_loss -0.6954 
2024-05-08 12:29:38.725468: val_loss -0.7045 
2024-05-08 12:29:38.725517: Pseudo dice [0.8315] 
2024-05-08 12:29:38.725572: Epoch time: 40.96 s 
2024-05-08 12:29:38.725615: Yayy! New best EMA pseudo Dice: 0.8002 
2024-05-08 12:29:40.055623:  
2024-05-08 12:29:40.055747: Epoch 23 
2024-05-08 12:29:40.055838: Current learning rate: 0.00979 
2024-05-08 12:30:21.031768: train_loss -0.7034 
2024-05-08 12:30:21.031949: val_loss -0.708 
2024-05-08 12:30:21.031998: Pseudo dice [0.8407] 
2024-05-08 12:30:21.032051: Epoch time: 40.98 s 
2024-05-08 12:30:21.032092: Yayy! New best EMA pseudo Dice: 0.8042 
2024-05-08 12:30:22.372011:  
2024-05-08 12:30:22.372140: Epoch 24 
2024-05-08 12:30:22.372232: Current learning rate: 0.00978 
2024-05-08 12:31:03.320802: train_loss -0.6986 
2024-05-08 12:31:03.320971: val_loss -0.7261 
2024-05-08 12:31:03.321023: Pseudo dice [0.854] 
2024-05-08 12:31:03.321079: Epoch time: 40.95 s 
2024-05-08 12:31:03.321121: Yayy! New best EMA pseudo Dice: 0.8092 
2024-05-08 12:31:04.670369:  
2024-05-08 12:31:04.670536: Epoch 25 
2024-05-08 12:31:04.670631: Current learning rate: 0.00977 
2024-05-08 12:31:45.609818: train_loss -0.7166 
2024-05-08 12:31:45.609995: val_loss -0.7101 
2024-05-08 12:31:45.610043: Pseudo dice [0.8434] 
2024-05-08 12:31:45.610095: Epoch time: 40.94 s 
2024-05-08 12:31:45.610137: Yayy! New best EMA pseudo Dice: 0.8126 
2024-05-08 12:31:47.117607:  
2024-05-08 12:31:47.117737: Epoch 26 
2024-05-08 12:31:47.117826: Current learning rate: 0.00977 
2024-05-08 12:32:28.043660: train_loss -0.7184 
2024-05-08 12:32:28.043836: val_loss -0.7522 
2024-05-08 12:32:28.043885: Pseudo dice [0.864] 
2024-05-08 12:32:28.043937: Epoch time: 40.93 s 
2024-05-08 12:32:28.043978: Yayy! New best EMA pseudo Dice: 0.8178 
2024-05-08 12:32:29.366763:  
2024-05-08 12:32:29.366995: Epoch 27 
2024-05-08 12:32:29.367123: Current learning rate: 0.00976 
2024-05-08 12:33:10.884736: train_loss -0.7135 
2024-05-08 12:33:10.884913: val_loss -0.724 
2024-05-08 12:33:10.884962: Pseudo dice [0.8549] 
2024-05-08 12:33:10.885016: Epoch time: 41.52 s 
2024-05-08 12:33:10.885057: Yayy! New best EMA pseudo Dice: 0.8215 
2024-05-08 12:33:12.243843:  
2024-05-08 12:33:12.244042: Epoch 28 
2024-05-08 12:33:12.244153: Current learning rate: 0.00975 
2024-05-08 12:33:53.571850: train_loss -0.7484 
2024-05-08 12:33:53.572116: val_loss -0.7331 
2024-05-08 12:33:53.572260: Pseudo dice [0.8499] 
2024-05-08 12:33:53.572375: Epoch time: 41.33 s 
2024-05-08 12:33:53.572421: Yayy! New best EMA pseudo Dice: 0.8243 
2024-05-08 12:33:54.955673:  
2024-05-08 12:33:54.956015: Epoch 29 
2024-05-08 12:33:54.956172: Current learning rate: 0.00974 
2024-05-08 12:34:36.020293: train_loss -0.7271 
2024-05-08 12:34:36.020472: val_loss -0.7446 
2024-05-08 12:34:36.020520: Pseudo dice [0.858] 
2024-05-08 12:34:36.020573: Epoch time: 41.07 s 
2024-05-08 12:34:36.020616: Yayy! New best EMA pseudo Dice: 0.8277 
2024-05-08 12:34:37.409087:  
2024-05-08 12:34:37.409219: Epoch 30 
2024-05-08 12:34:37.409311: Current learning rate: 0.00973 
2024-05-08 12:35:18.322443: train_loss -0.7375 
2024-05-08 12:35:18.322622: val_loss -0.7521 
2024-05-08 12:35:18.322671: Pseudo dice [0.8632] 
2024-05-08 12:35:18.322724: Epoch time: 40.91 s 
2024-05-08 12:35:18.322766: Yayy! New best EMA pseudo Dice: 0.8312 
2024-05-08 12:35:19.698720:  
2024-05-08 12:35:19.698848: Epoch 31 
2024-05-08 12:35:19.698946: Current learning rate: 0.00972 
2024-05-08 12:36:00.946428: train_loss -0.7402 
2024-05-08 12:36:00.946610: val_loss -0.7464 
2024-05-08 12:36:00.946660: Pseudo dice [0.8576] 
2024-05-08 12:36:00.946715: Epoch time: 41.25 s 
2024-05-08 12:36:00.946758: Yayy! New best EMA pseudo Dice: 0.8339 
2024-05-08 12:36:02.656497:  
2024-05-08 12:36:02.656747: Epoch 32 
2024-05-08 12:36:02.656850: Current learning rate: 0.00971 
2024-05-08 12:36:43.645313: train_loss -0.7387 
2024-05-08 12:36:43.645494: val_loss -0.7553 
2024-05-08 12:36:43.645544: Pseudo dice [0.8672] 
2024-05-08 12:36:43.645597: Epoch time: 40.99 s 
2024-05-08 12:36:43.645638: Yayy! New best EMA pseudo Dice: 0.8372 
2024-05-08 12:36:45.034827:  
2024-05-08 12:36:45.034966: Epoch 33 
2024-05-08 12:36:45.035057: Current learning rate: 0.0097 
2024-05-08 12:37:25.932941: train_loss -0.7266 
2024-05-08 12:37:25.933113: val_loss -0.7048 
2024-05-08 12:37:25.933164: Pseudo dice [0.8498] 
2024-05-08 12:37:25.933219: Epoch time: 40.9 s 
2024-05-08 12:37:25.933262: Yayy! New best EMA pseudo Dice: 0.8385 
2024-05-08 12:37:27.318556:  
2024-05-08 12:37:27.318692: Epoch 34 
2024-05-08 12:37:27.318783: Current learning rate: 0.00969 
2024-05-08 12:38:08.763025: train_loss -0.7373 
2024-05-08 12:38:08.763197: val_loss -0.7605 
2024-05-08 12:38:08.763245: Pseudo dice [0.8722] 
2024-05-08 12:38:08.763298: Epoch time: 41.45 s 
2024-05-08 12:38:08.763340: Yayy! New best EMA pseudo Dice: 0.8418 
2024-05-08 12:38:10.143374:  
2024-05-08 12:38:10.143507: Epoch 35 
2024-05-08 12:38:10.143598: Current learning rate: 0.00968 
2024-05-08 12:38:51.065479: train_loss -0.7424 
2024-05-08 12:38:51.065653: val_loss -0.7372 
2024-05-08 12:38:51.065701: Pseudo dice [0.8569] 
2024-05-08 12:38:51.065752: Epoch time: 40.92 s 
2024-05-08 12:38:51.065794: Yayy! New best EMA pseudo Dice: 0.8433 
2024-05-08 12:38:52.446709:  
2024-05-08 12:38:52.446844: Epoch 36 
2024-05-08 12:38:52.446940: Current learning rate: 0.00968 
2024-05-08 12:39:33.352672: train_loss -0.7342 
2024-05-08 12:39:33.352860: val_loss -0.7729 
2024-05-08 12:39:33.352919: Pseudo dice [0.8698] 
2024-05-08 12:39:33.352985: Epoch time: 40.91 s 
2024-05-08 12:39:33.353031: Yayy! New best EMA pseudo Dice: 0.846 
2024-05-08 12:39:34.730891:  
2024-05-08 12:39:34.731058: Epoch 37 
2024-05-08 12:39:34.731152: Current learning rate: 0.00967 
2024-05-08 12:40:15.657853: train_loss -0.7535 
2024-05-08 12:40:15.658046: val_loss -0.7437 
2024-05-08 12:40:15.658105: Pseudo dice [0.8605] 
2024-05-08 12:40:15.658160: Epoch time: 40.93 s 
2024-05-08 12:40:15.658202: Yayy! New best EMA pseudo Dice: 0.8474 
2024-05-08 12:40:17.098086:  
2024-05-08 12:40:17.098310: Epoch 38 
2024-05-08 12:40:17.098416: Current learning rate: 0.00966 
2024-05-08 12:40:58.024710: train_loss -0.75 
2024-05-08 12:40:58.024883: val_loss -0.7363 
2024-05-08 12:40:58.024931: Pseudo dice [0.861] 
2024-05-08 12:40:58.024986: Epoch time: 40.93 s 
2024-05-08 12:40:58.025027: Yayy! New best EMA pseudo Dice: 0.8488 
2024-05-08 12:40:59.595919:  
2024-05-08 12:40:59.596070: Epoch 39 
2024-05-08 12:40:59.596171: Current learning rate: 0.00965 
2024-05-08 12:41:40.577658: train_loss -0.7401 
2024-05-08 12:41:40.577963: val_loss -0.7678 
2024-05-08 12:41:40.578012: Pseudo dice [0.8777] 
2024-05-08 12:41:40.578065: Epoch time: 40.98 s 
2024-05-08 12:41:40.578108: Yayy! New best EMA pseudo Dice: 0.8517 
2024-05-08 12:41:42.036497:  
2024-05-08 12:41:42.036639: Epoch 40 
2024-05-08 12:41:42.036737: Current learning rate: 0.00964 
2024-05-08 12:42:23.009282: train_loss -0.7486 
2024-05-08 12:42:23.009477: val_loss -0.7566 
2024-05-08 12:42:23.009526: Pseudo dice [0.8626] 
2024-05-08 12:42:23.009579: Epoch time: 40.97 s 
2024-05-08 12:42:23.009621: Yayy! New best EMA pseudo Dice: 0.8528 
2024-05-08 12:42:24.428898:  
2024-05-08 12:42:24.429112: Epoch 41 
2024-05-08 12:42:24.429208: Current learning rate: 0.00963 
2024-05-08 12:43:05.386010: train_loss -0.7531 
2024-05-08 12:43:05.386302: val_loss -0.7586 
2024-05-08 12:43:05.386356: Pseudo dice [0.8728] 
2024-05-08 12:43:05.386409: Epoch time: 40.96 s 
2024-05-08 12:43:05.386451: Yayy! New best EMA pseudo Dice: 0.8548 
2024-05-08 12:43:06.737092:  
2024-05-08 12:43:06.737225: Epoch 42 
2024-05-08 12:43:06.737316: Current learning rate: 0.00962 
2024-05-08 12:43:47.720927: train_loss -0.7451 
2024-05-08 12:43:47.721111: val_loss -0.7619 
2024-05-08 12:43:47.721159: Pseudo dice [0.8699] 
2024-05-08 12:43:47.721213: Epoch time: 40.98 s 
2024-05-08 12:43:47.721255: Yayy! New best EMA pseudo Dice: 0.8563 
2024-05-08 12:43:49.069581:  
2024-05-08 12:43:49.069771: Epoch 43 
2024-05-08 12:43:49.069864: Current learning rate: 0.00961 
2024-05-08 12:44:30.047402: train_loss -0.7543 
2024-05-08 12:44:30.047566: val_loss -0.7274 
2024-05-08 12:44:30.047614: Pseudo dice [0.8546] 
2024-05-08 12:44:30.047666: Epoch time: 40.98 s 
2024-05-08 12:44:31.068807:  
2024-05-08 12:44:31.069006: Epoch 44 
2024-05-08 12:44:31.069098: Current learning rate: 0.0096 
2024-05-08 12:45:12.056157: train_loss -0.7476 
2024-05-08 12:45:12.056331: val_loss -0.739 
2024-05-08 12:45:12.056381: Pseudo dice [0.8634] 
2024-05-08 12:45:12.056445: Epoch time: 40.99 s 
2024-05-08 12:45:12.056488: Yayy! New best EMA pseudo Dice: 0.8568 
2024-05-08 12:45:13.578047:  
2024-05-08 12:45:13.578191: Epoch 45 
2024-05-08 12:45:13.578285: Current learning rate: 0.00959 
2024-05-08 12:45:54.585929: train_loss -0.764 
2024-05-08 12:45:54.586118: val_loss -0.7738 
2024-05-08 12:45:54.586167: Pseudo dice [0.8787] 
2024-05-08 12:45:54.586222: Epoch time: 41.01 s 
2024-05-08 12:45:54.586265: Yayy! New best EMA pseudo Dice: 0.859 
2024-05-08 12:45:55.925416:  
2024-05-08 12:45:55.925549: Epoch 46 
2024-05-08 12:45:55.925644: Current learning rate: 0.00959 
2024-05-08 12:46:36.910777: train_loss -0.7599 
2024-05-08 12:46:36.910955: val_loss -0.7731 
2024-05-08 12:46:36.911004: Pseudo dice [0.8744] 
2024-05-08 12:46:36.911056: Epoch time: 40.99 s 
2024-05-08 12:46:36.911099: Yayy! New best EMA pseudo Dice: 0.8606 
2024-05-08 12:46:38.262795:  
2024-05-08 12:46:38.262979: Epoch 47 
2024-05-08 12:46:38.263070: Current learning rate: 0.00958 
2024-05-08 12:47:19.240655: train_loss -0.7636 
2024-05-08 12:47:19.240822: val_loss -0.7752 
2024-05-08 12:47:19.240870: Pseudo dice [0.88] 
2024-05-08 12:47:19.240923: Epoch time: 40.98 s 
2024-05-08 12:47:19.240967: Yayy! New best EMA pseudo Dice: 0.8625 
2024-05-08 12:47:20.582227:  
2024-05-08 12:47:20.582367: Epoch 48 
2024-05-08 12:47:20.582457: Current learning rate: 0.00957 
2024-05-08 12:48:01.543497: train_loss -0.7702 
2024-05-08 12:48:01.543664: val_loss -0.7848 
2024-05-08 12:48:01.543713: Pseudo dice [0.8786] 
2024-05-08 12:48:01.543767: Epoch time: 40.96 s 
2024-05-08 12:48:01.543811: Yayy! New best EMA pseudo Dice: 0.8641 
2024-05-08 12:48:02.904059:  
2024-05-08 12:48:02.904233: Epoch 49 
2024-05-08 12:48:02.904373: Current learning rate: 0.00956 
2024-05-08 12:48:43.848844: train_loss -0.7682 
2024-05-08 12:48:43.849014: val_loss -0.7673 
2024-05-08 12:48:43.849062: Pseudo dice [0.8748] 
2024-05-08 12:48:43.849114: Epoch time: 40.95 s 
2024-05-08 12:48:44.104948: Yayy! New best EMA pseudo Dice: 0.8652 
2024-05-08 12:48:45.454358:  
2024-05-08 12:48:45.454601: Epoch 50 
2024-05-08 12:48:45.454805: Current learning rate: 0.00955 
2024-05-08 12:49:26.398487: train_loss -0.7471 
2024-05-08 12:49:26.398659: val_loss -0.7498 
2024-05-08 12:49:26.398708: Pseudo dice [0.8651] 
2024-05-08 12:49:26.398761: Epoch time: 40.95 s 
2024-05-08 12:49:27.634414:  
2024-05-08 12:49:27.634551: Epoch 51 
2024-05-08 12:49:27.634642: Current learning rate: 0.00954 
2024-05-08 12:50:08.576469: train_loss -0.7528 
2024-05-08 12:50:08.576657: val_loss -0.7905 
2024-05-08 12:50:08.576709: Pseudo dice [0.8839] 
2024-05-08 12:50:08.576763: Epoch time: 40.94 s 
2024-05-08 12:50:08.576805: Yayy! New best EMA pseudo Dice: 0.867 
2024-05-08 12:50:09.944254:  
2024-05-08 12:50:09.944393: Epoch 52 
2024-05-08 12:50:09.944487: Current learning rate: 0.00953 
2024-05-08 12:50:50.895563: train_loss -0.7597 
2024-05-08 12:50:50.895737: val_loss -0.756 
2024-05-08 12:50:50.895786: Pseudo dice [0.866] 
2024-05-08 12:50:50.895840: Epoch time: 40.95 s 
2024-05-08 12:50:51.927001:  
2024-05-08 12:50:51.927134: Epoch 53 
2024-05-08 12:50:51.927224: Current learning rate: 0.00952 
2024-05-08 12:51:32.887521: train_loss -0.7593 
2024-05-08 12:51:32.887700: val_loss -0.7772 
2024-05-08 12:51:32.887749: Pseudo dice [0.8831] 
2024-05-08 12:51:32.887802: Epoch time: 40.96 s 
2024-05-08 12:51:32.887844: Yayy! New best EMA pseudo Dice: 0.8686 
2024-05-08 12:51:34.249460:  
2024-05-08 12:51:34.249593: Epoch 54 
2024-05-08 12:51:34.249689: Current learning rate: 0.00951 
2024-05-08 12:52:15.202674: train_loss -0.7577 
2024-05-08 12:52:15.202853: val_loss -0.7987 
2024-05-08 12:52:15.202903: Pseudo dice [0.8959] 
2024-05-08 12:52:15.202956: Epoch time: 40.95 s 
2024-05-08 12:52:15.202999: Yayy! New best EMA pseudo Dice: 0.8713 
2024-05-08 12:52:16.575184:  
2024-05-08 12:52:16.575315: Epoch 55 
2024-05-08 12:52:16.575408: Current learning rate: 0.0095 
2024-05-08 12:52:57.510141: train_loss -0.7677 
2024-05-08 12:52:57.510317: val_loss -0.7578 
2024-05-08 12:52:57.510366: Pseudo dice [0.873] 
2024-05-08 12:52:57.510432: Epoch time: 40.94 s 
2024-05-08 12:52:57.510475: Yayy! New best EMA pseudo Dice: 0.8714 
2024-05-08 12:52:58.878817:  
2024-05-08 12:52:58.878945: Epoch 56 
2024-05-08 12:52:58.879037: Current learning rate: 0.00949 
2024-05-08 12:53:39.838295: train_loss -0.7704 
2024-05-08 12:53:39.838478: val_loss -0.7759 
2024-05-08 12:53:39.838527: Pseudo dice [0.8777] 
2024-05-08 12:53:39.838580: Epoch time: 40.96 s 
2024-05-08 12:53:39.838623: Yayy! New best EMA pseudo Dice: 0.8721 
2024-05-08 12:53:41.375522:  
2024-05-08 12:53:41.375662: Epoch 57 
2024-05-08 12:53:41.375763: Current learning rate: 0.00949 
2024-05-08 12:54:22.355284: train_loss -0.7837 
2024-05-08 12:54:22.355474: val_loss -0.7863 
2024-05-08 12:54:22.355522: Pseudo dice [0.8838] 
2024-05-08 12:54:22.355574: Epoch time: 40.98 s 
2024-05-08 12:54:22.355618: Yayy! New best EMA pseudo Dice: 0.8732 
2024-05-08 12:54:23.744573:  
2024-05-08 12:54:23.744718: Epoch 58 
2024-05-08 12:54:23.744811: Current learning rate: 0.00948 
2024-05-08 12:55:04.738051: train_loss -0.7719 
2024-05-08 12:55:04.738221: val_loss -0.7691 
2024-05-08 12:55:04.738271: Pseudo dice [0.8833] 
2024-05-08 12:55:04.738323: Epoch time: 40.99 s 
2024-05-08 12:55:04.738365: Yayy! New best EMA pseudo Dice: 0.8743 
2024-05-08 12:55:06.131070:  
2024-05-08 12:55:06.131295: Epoch 59 
2024-05-08 12:55:06.131393: Current learning rate: 0.00947 
2024-05-08 12:55:47.128913: train_loss -0.7777 
2024-05-08 12:55:47.129091: val_loss -0.806 
2024-05-08 12:55:47.129142: Pseudo dice [0.898] 
2024-05-08 12:55:47.129197: Epoch time: 41.0 s 
2024-05-08 12:55:47.129244: Yayy! New best EMA pseudo Dice: 0.8766 
2024-05-08 12:55:48.516514:  
2024-05-08 12:55:48.516655: Epoch 60 
2024-05-08 12:55:48.516747: Current learning rate: 0.00946 
2024-05-08 12:56:29.479045: train_loss -0.7821 
2024-05-08 12:56:29.479232: val_loss -0.7925 
2024-05-08 12:56:29.479282: Pseudo dice [0.8891] 
2024-05-08 12:56:29.479334: Epoch time: 40.96 s 
2024-05-08 12:56:29.479376: Yayy! New best EMA pseudo Dice: 0.8779 
2024-05-08 12:56:30.903599:  
2024-05-08 12:56:30.903733: Epoch 61 
2024-05-08 12:56:30.903822: Current learning rate: 0.00945 
2024-05-08 12:57:11.859725: train_loss -0.7861 
2024-05-08 12:57:11.859901: val_loss -0.7937 
2024-05-08 12:57:11.859949: Pseudo dice [0.8882] 
2024-05-08 12:57:11.860002: Epoch time: 40.96 s 
2024-05-08 12:57:11.860044: Yayy! New best EMA pseudo Dice: 0.8789 
2024-05-08 12:57:13.244402:  
2024-05-08 12:57:13.244600: Epoch 62 
2024-05-08 12:57:13.244698: Current learning rate: 0.00944 
2024-05-08 12:57:54.174138: train_loss -0.7768 
2024-05-08 12:57:54.174325: val_loss -0.7582 
2024-05-08 12:57:54.174374: Pseudo dice [0.8693] 
2024-05-08 12:57:54.174426: Epoch time: 40.93 s 
2024-05-08 12:57:55.248543:  
2024-05-08 12:57:55.248675: Epoch 63 
2024-05-08 12:57:55.248765: Current learning rate: 0.00943 
2024-05-08 12:58:36.165109: train_loss -0.7726 
2024-05-08 12:58:36.165280: val_loss -0.7857 
2024-05-08 12:58:36.165329: Pseudo dice [0.8852] 
2024-05-08 12:58:36.165383: Epoch time: 40.92 s 
2024-05-08 12:58:37.440928:  
2024-05-08 12:58:37.441156: Epoch 64 
2024-05-08 12:58:37.441259: Current learning rate: 0.00942 
2024-05-08 12:59:18.348000: train_loss -0.7738 
2024-05-08 12:59:18.348191: val_loss -0.7922 
2024-05-08 12:59:18.348250: Pseudo dice [0.885] 
2024-05-08 12:59:18.348302: Epoch time: 40.91 s 
2024-05-08 12:59:18.348344: Yayy! New best EMA pseudo Dice: 0.8793 
2024-05-08 12:59:19.766916:  
2024-05-08 12:59:19.767054: Epoch 65 
2024-05-08 12:59:19.767160: Current learning rate: 0.00941 
2024-05-08 13:00:00.708684: train_loss -0.7861 
2024-05-08 13:00:00.708879: val_loss -0.7677 
2024-05-08 13:00:00.708942: Pseudo dice [0.8762] 
2024-05-08 13:00:00.709009: Epoch time: 40.94 s 
2024-05-08 13:00:01.765189:  
2024-05-08 13:00:01.765385: Epoch 66 
2024-05-08 13:00:01.765498: Current learning rate: 0.0094 
2024-05-08 13:00:42.715919: train_loss -0.7915 
2024-05-08 13:00:42.716111: val_loss -0.7821 
2024-05-08 13:00:42.716174: Pseudo dice [0.8848] 
2024-05-08 13:00:42.716252: Epoch time: 40.95 s 
2024-05-08 13:00:42.716308: Yayy! New best EMA pseudo Dice: 0.8796 
2024-05-08 13:00:44.111314:  
2024-05-08 13:00:44.111490: Epoch 67 
2024-05-08 13:00:44.111587: Current learning rate: 0.00939 
2024-05-08 13:01:25.490619: train_loss -0.7879 
2024-05-08 13:01:25.490914: val_loss -0.7896 
2024-05-08 13:01:25.490979: Pseudo dice [0.8821] 
2024-05-08 13:01:25.491045: Epoch time: 41.38 s 
2024-05-08 13:01:25.491100: Yayy! New best EMA pseudo Dice: 0.8798 
2024-05-08 13:01:26.903876:  
2024-05-08 13:01:26.904011: Epoch 68 
2024-05-08 13:01:26.904119: Current learning rate: 0.00939 
2024-05-08 13:02:07.855856: train_loss -0.7809 
2024-05-08 13:02:07.856040: val_loss -0.7922 
2024-05-08 13:02:07.856089: Pseudo dice [0.892] 
2024-05-08 13:02:07.856142: Epoch time: 40.95 s 
2024-05-08 13:02:07.856184: Yayy! New best EMA pseudo Dice: 0.881 
2024-05-08 13:02:09.267804:  
2024-05-08 13:02:09.267936: Epoch 69 
2024-05-08 13:02:09.268031: Current learning rate: 0.00938 
2024-05-08 13:02:50.207621: train_loss -0.7925 
2024-05-08 13:02:50.207811: val_loss -0.7841 
2024-05-08 13:02:50.207860: Pseudo dice [0.8879] 
2024-05-08 13:02:50.207912: Epoch time: 40.94 s 
2024-05-08 13:02:50.207955: Yayy! New best EMA pseudo Dice: 0.8817 
2024-05-08 13:02:51.773801:  
2024-05-08 13:02:51.773946: Epoch 70 
2024-05-08 13:02:51.774037: Current learning rate: 0.00937 
2024-05-08 13:03:32.721783: train_loss -0.7738 
2024-05-08 13:03:32.721963: val_loss -0.7924 
2024-05-08 13:03:32.722013: Pseudo dice [0.8899] 
2024-05-08 13:03:32.722067: Epoch time: 40.95 s 
2024-05-08 13:03:32.722110: Yayy! New best EMA pseudo Dice: 0.8826 
2024-05-08 13:03:34.141348:  
2024-05-08 13:03:34.141483: Epoch 71 
2024-05-08 13:03:34.141573: Current learning rate: 0.00936 
2024-05-08 13:04:15.606818: train_loss -0.7821 
2024-05-08 13:04:15.607011: val_loss -0.7962 
2024-05-08 13:04:15.607060: Pseudo dice [0.8917] 
2024-05-08 13:04:15.607115: Epoch time: 41.47 s 
2024-05-08 13:04:15.607158: Yayy! New best EMA pseudo Dice: 0.8835 
2024-05-08 13:04:17.016424:  
2024-05-08 13:04:17.016604: Epoch 72 
2024-05-08 13:04:17.016711: Current learning rate: 0.00935 
2024-05-08 13:04:57.960317: train_loss -0.7934 
2024-05-08 13:04:57.960607: val_loss -0.8018 
2024-05-08 13:04:57.960701: Pseudo dice [0.8872] 
2024-05-08 13:04:57.960757: Epoch time: 40.94 s 
2024-05-08 13:04:57.960800: Yayy! New best EMA pseudo Dice: 0.8838 
2024-05-08 13:04:59.413439:  
2024-05-08 13:04:59.413649: Epoch 73 
2024-05-08 13:04:59.413744: Current learning rate: 0.00934 
2024-05-08 13:05:40.338485: train_loss -0.8004 
2024-05-08 13:05:40.338663: val_loss -0.7996 
2024-05-08 13:05:40.338713: Pseudo dice [0.895] 
2024-05-08 13:05:40.338767: Epoch time: 40.93 s 
2024-05-08 13:05:40.338810: Yayy! New best EMA pseudo Dice: 0.885 
2024-05-08 13:05:41.737319:  
2024-05-08 13:05:41.737465: Epoch 74 
2024-05-08 13:05:41.737640: Current learning rate: 0.00933 
2024-05-08 13:06:22.975595: train_loss -0.788 
2024-05-08 13:06:22.975798: val_loss -0.7953 
2024-05-08 13:06:22.975848: Pseudo dice [0.8862] 
2024-05-08 13:06:22.975902: Epoch time: 41.24 s 
2024-05-08 13:06:22.975945: Yayy! New best EMA pseudo Dice: 0.8851 
2024-05-08 13:06:24.399634:  
2024-05-08 13:06:24.399916: Epoch 75 
2024-05-08 13:06:24.400016: Current learning rate: 0.00932 
2024-05-08 13:07:05.526506: train_loss -0.7876 
2024-05-08 13:07:05.526686: val_loss -0.7967 
2024-05-08 13:07:05.526735: Pseudo dice [0.8853] 
2024-05-08 13:07:05.526788: Epoch time: 41.13 s 
2024-05-08 13:07:05.526831: Yayy! New best EMA pseudo Dice: 0.8851 
2024-05-08 13:07:07.126982:  
2024-05-08 13:07:07.127114: Epoch 76 
2024-05-08 13:07:07.127212: Current learning rate: 0.00931 
2024-05-08 13:07:48.124091: train_loss -0.7852 
2024-05-08 13:07:48.124295: val_loss -0.8061 
2024-05-08 13:07:48.124345: Pseudo dice [0.896] 
2024-05-08 13:07:48.124399: Epoch time: 41.0 s 
2024-05-08 13:07:48.124443: Yayy! New best EMA pseudo Dice: 0.8862 
2024-05-08 13:07:49.542740:  
2024-05-08 13:07:49.542885: Epoch 77 
2024-05-08 13:07:49.542976: Current learning rate: 0.0093 
2024-05-08 13:08:30.518875: train_loss -0.7843 
2024-05-08 13:08:30.519049: val_loss -0.7853 
2024-05-08 13:08:30.519101: Pseudo dice [0.8845] 
2024-05-08 13:08:30.519154: Epoch time: 40.98 s 
2024-05-08 13:08:31.612976:  
2024-05-08 13:08:31.613119: Epoch 78 
2024-05-08 13:08:31.613215: Current learning rate: 0.0093 
2024-05-08 13:09:12.571209: train_loss -0.7908 
2024-05-08 13:09:12.571385: val_loss -0.7903 
2024-05-08 13:09:12.571433: Pseudo dice [0.8854] 
2024-05-08 13:09:12.571486: Epoch time: 40.96 s 
2024-05-08 13:09:13.664935:  
2024-05-08 13:09:13.665079: Epoch 79 
2024-05-08 13:09:13.665171: Current learning rate: 0.00929 
2024-05-08 13:09:55.066787: train_loss -0.79 
2024-05-08 13:09:55.066969: val_loss -0.8034 
2024-05-08 13:09:55.067018: Pseudo dice [0.8934] 
2024-05-08 13:09:55.067070: Epoch time: 41.4 s 
2024-05-08 13:09:55.067111: Yayy! New best EMA pseudo Dice: 0.8867 
2024-05-08 13:09:56.494754:  
2024-05-08 13:09:56.494942: Epoch 80 
2024-05-08 13:09:56.495061: Current learning rate: 0.00928 
2024-05-08 13:10:37.905927: train_loss -0.8086 
2024-05-08 13:10:37.906105: val_loss -0.8134 
2024-05-08 13:10:37.906153: Pseudo dice [0.901] 
2024-05-08 13:10:37.906208: Epoch time: 41.41 s 
2024-05-08 13:10:37.906250: Yayy! New best EMA pseudo Dice: 0.8881 
2024-05-08 13:10:39.348830:  
2024-05-08 13:10:39.349040: Epoch 81 
2024-05-08 13:10:39.349139: Current learning rate: 0.00927 
2024-05-08 13:11:20.296087: train_loss -0.7971 
2024-05-08 13:11:20.296268: val_loss -0.7881 
2024-05-08 13:11:20.296316: Pseudo dice [0.8896] 
2024-05-08 13:11:20.296371: Epoch time: 40.95 s 
2024-05-08 13:11:20.296419: Yayy! New best EMA pseudo Dice: 0.8883 
2024-05-08 13:11:21.887836:  
2024-05-08 13:11:21.888101: Epoch 82 
2024-05-08 13:11:21.888197: Current learning rate: 0.00926 
2024-05-08 13:12:02.853892: train_loss -0.7888 
2024-05-08 13:12:02.854074: val_loss -0.7893 
2024-05-08 13:12:02.854124: Pseudo dice [0.8903] 
2024-05-08 13:12:02.854179: Epoch time: 40.97 s 
2024-05-08 13:12:02.854222: Yayy! New best EMA pseudo Dice: 0.8885 
2024-05-08 13:12:04.227793:  
2024-05-08 13:12:04.227936: Epoch 83 
2024-05-08 13:12:04.228029: Current learning rate: 0.00925 
2024-05-08 13:12:45.182274: train_loss -0.7913 
2024-05-08 13:12:45.182457: val_loss -0.7908 
2024-05-08 13:12:45.182508: Pseudo dice [0.8895] 
2024-05-08 13:12:45.182562: Epoch time: 40.96 s 
2024-05-08 13:12:45.182605: Yayy! New best EMA pseudo Dice: 0.8886 
2024-05-08 13:12:46.556716:  
2024-05-08 13:12:46.556968: Epoch 84 
2024-05-08 13:12:46.557061: Current learning rate: 0.00924 
2024-05-08 13:13:27.484704: train_loss -0.7945 
2024-05-08 13:13:27.484889: val_loss -0.822 
2024-05-08 13:13:27.484940: Pseudo dice [0.9046] 
2024-05-08 13:13:27.484993: Epoch time: 40.93 s 
2024-05-08 13:13:27.485036: Yayy! New best EMA pseudo Dice: 0.8902 
2024-05-08 13:13:28.842459:  
2024-05-08 13:13:28.842618: Epoch 85 
2024-05-08 13:13:28.842717: Current learning rate: 0.00923 
2024-05-08 13:14:10.298142: train_loss -0.7863 
2024-05-08 13:14:10.298326: val_loss -0.7975 
2024-05-08 13:14:10.298376: Pseudo dice [0.8907] 
2024-05-08 13:14:10.298429: Epoch time: 41.46 s 
2024-05-08 13:14:10.298471: Yayy! New best EMA pseudo Dice: 0.8902 
2024-05-08 13:14:11.661474:  
2024-05-08 13:14:11.661708: Epoch 86 
2024-05-08 13:14:11.661857: Current learning rate: 0.00922 
2024-05-08 13:14:52.635561: train_loss -0.7955 
2024-05-08 13:14:52.635760: val_loss -0.7994 
2024-05-08 13:14:52.635810: Pseudo dice [0.8958] 
2024-05-08 13:14:52.635864: Epoch time: 40.98 s 
2024-05-08 13:14:52.635906: Yayy! New best EMA pseudo Dice: 0.8908 
2024-05-08 13:14:54.018893:  
2024-05-08 13:14:54.019135: Epoch 87 
2024-05-08 13:14:54.019233: Current learning rate: 0.00921 
2024-05-08 13:15:35.002062: train_loss -0.8079 
2024-05-08 13:15:35.002245: val_loss -0.8034 
2024-05-08 13:15:35.002294: Pseudo dice [0.8939] 
2024-05-08 13:15:35.002347: Epoch time: 40.98 s 
2024-05-08 13:15:35.002389: Yayy! New best EMA pseudo Dice: 0.8911 
2024-05-08 13:15:36.364466:  
2024-05-08 13:15:36.364602: Epoch 88 
2024-05-08 13:15:36.364709: Current learning rate: 0.0092 
2024-05-08 13:16:17.328101: train_loss -0.7824 
2024-05-08 13:16:17.328286: val_loss -0.7845 
2024-05-08 13:16:17.328333: Pseudo dice [0.8893] 
2024-05-08 13:16:17.328387: Epoch time: 40.96 s 
2024-05-08 13:16:18.551950:  
2024-05-08 13:16:18.552097: Epoch 89 
2024-05-08 13:16:18.552188: Current learning rate: 0.0092 
2024-05-08 13:16:59.471096: train_loss -0.7882 
2024-05-08 13:16:59.471286: val_loss -0.8016 
2024-05-08 13:16:59.471334: Pseudo dice [0.8953] 
2024-05-08 13:16:59.471386: Epoch time: 40.92 s 
2024-05-08 13:16:59.471429: Yayy! New best EMA pseudo Dice: 0.8914 
2024-05-08 13:17:00.838811:  
2024-05-08 13:17:00.838946: Epoch 90 
2024-05-08 13:17:00.839037: Current learning rate: 0.00919 
2024-05-08 13:17:41.721087: train_loss -0.7998 
2024-05-08 13:17:41.721269: val_loss -0.815 
2024-05-08 13:17:41.721317: Pseudo dice [0.8994] 
2024-05-08 13:17:41.721369: Epoch time: 40.88 s 
2024-05-08 13:17:41.721411: Yayy! New best EMA pseudo Dice: 0.8922 
2024-05-08 13:17:43.081141:  
2024-05-08 13:17:43.081342: Epoch 91 
2024-05-08 13:17:43.081437: Current learning rate: 0.00918 
2024-05-08 13:18:23.968819: train_loss -0.8117 
2024-05-08 13:18:23.968999: val_loss -0.8127 
2024-05-08 13:18:23.969047: Pseudo dice [0.8988] 
2024-05-08 13:18:23.969099: Epoch time: 40.89 s 
2024-05-08 13:18:23.969141: Yayy! New best EMA pseudo Dice: 0.8928 
2024-05-08 13:18:25.312350:  
2024-05-08 13:18:25.312490: Epoch 92 
2024-05-08 13:18:25.312581: Current learning rate: 0.00917 
2024-05-08 13:19:06.214283: train_loss -0.8035 
2024-05-08 13:19:06.214463: val_loss -0.8218 
2024-05-08 13:19:06.214512: Pseudo dice [0.8989] 
2024-05-08 13:19:06.214565: Epoch time: 40.9 s 
2024-05-08 13:19:06.214607: Yayy! New best EMA pseudo Dice: 0.8934 
2024-05-08 13:19:07.578965:  
2024-05-08 13:19:07.579115: Epoch 93 
2024-05-08 13:19:07.579206: Current learning rate: 0.00916 
2024-05-08 13:19:48.493770: train_loss -0.7895 
2024-05-08 13:19:48.493947: val_loss -0.8125 
2024-05-08 13:19:48.493996: Pseudo dice [0.8944] 
2024-05-08 13:19:48.494050: Epoch time: 40.92 s 
2024-05-08 13:19:48.494093: Yayy! New best EMA pseudo Dice: 0.8935 
2024-05-08 13:19:49.866172:  
2024-05-08 13:19:49.866557: Epoch 94 
2024-05-08 13:19:49.866653: Current learning rate: 0.00915 
2024-05-08 13:20:30.777189: train_loss -0.7947 
2024-05-08 13:20:30.777364: val_loss -0.8015 
2024-05-08 13:20:30.777414: Pseudo dice [0.8886] 
2024-05-08 13:20:30.777468: Epoch time: 40.91 s 
2024-05-08 13:20:31.976946:  
2024-05-08 13:20:31.977098: Epoch 95 
2024-05-08 13:20:31.977189: Current learning rate: 0.00914 
2024-05-08 13:21:13.359360: train_loss -0.7972 
2024-05-08 13:21:13.359546: val_loss -0.8218 
2024-05-08 13:21:13.359595: Pseudo dice [0.9099] 
2024-05-08 13:21:13.359648: Epoch time: 41.38 s 
2024-05-08 13:21:13.359690: Yayy! New best EMA pseudo Dice: 0.8947 
2024-05-08 13:21:14.753820:  
2024-05-08 13:21:14.753963: Epoch 96 
2024-05-08 13:21:14.754053: Current learning rate: 0.00913 
2024-05-08 13:21:55.744331: train_loss -0.8094 
2024-05-08 13:21:55.744506: val_loss -0.7826 
2024-05-08 13:21:55.744555: Pseudo dice [0.8844] 
2024-05-08 13:21:55.744608: Epoch time: 40.99 s 
2024-05-08 13:21:56.777128:  
2024-05-08 13:21:56.777312: Epoch 97 
2024-05-08 13:21:56.777406: Current learning rate: 0.00912 
2024-05-08 13:22:37.766233: train_loss -0.7848 
2024-05-08 13:22:37.766412: val_loss -0.784 
2024-05-08 13:22:37.766460: Pseudo dice [0.8853] 
2024-05-08 13:22:37.766512: Epoch time: 40.99 s 
2024-05-08 13:22:38.800544:  
2024-05-08 13:22:38.800764: Epoch 98 
2024-05-08 13:22:38.800866: Current learning rate: 0.00911 
2024-05-08 13:23:20.259234: train_loss -0.8065 
2024-05-08 13:23:20.259417: val_loss -0.807 
2024-05-08 13:23:20.259465: Pseudo dice [0.8929] 
2024-05-08 13:23:20.259582: Epoch time: 41.46 s 
2024-05-08 13:23:21.312085:  
2024-05-08 13:23:21.312223: Epoch 99 
2024-05-08 13:23:21.312318: Current learning rate: 0.0091 
2024-05-08 13:24:02.296431: train_loss -0.8045 
2024-05-08 13:24:02.296616: val_loss -0.8075 
2024-05-08 13:24:02.296684: Pseudo dice [0.8976] 
2024-05-08 13:24:02.296743: Epoch time: 40.99 s 
2024-05-08 13:24:03.663763:  
2024-05-08 13:24:03.663900: Epoch 100 
2024-05-08 13:24:03.664000: Current learning rate: 0.0091 
2024-05-08 13:24:44.622600: train_loss -0.7937 
2024-05-08 13:24:44.622783: val_loss -0.7957 
2024-05-08 13:24:44.622832: Pseudo dice [0.8937] 
2024-05-08 13:24:44.622888: Epoch time: 40.96 s 
2024-05-08 13:24:45.659296:  
2024-05-08 13:24:45.659421: Epoch 101 
2024-05-08 13:24:45.659519: Current learning rate: 0.00909 
2024-05-08 13:25:26.603720: train_loss -0.8028 
2024-05-08 13:25:26.603898: val_loss -0.8149 
2024-05-08 13:25:26.603947: Pseudo dice [0.9026] 
2024-05-08 13:25:26.604004: Epoch time: 40.95 s 
2024-05-08 13:25:27.827070:  
2024-05-08 13:25:27.827398: Epoch 102 
2024-05-08 13:25:27.827519: Current learning rate: 0.00908 
2024-05-08 13:26:08.767124: train_loss -0.8022 
2024-05-08 13:26:08.767300: val_loss -0.8005 
2024-05-08 13:26:08.767348: Pseudo dice [0.8921] 
2024-05-08 13:26:08.767401: Epoch time: 40.94 s 
2024-05-08 13:26:09.800829:  
2024-05-08 13:26:09.801052: Epoch 103 
2024-05-08 13:26:09.801145: Current learning rate: 0.00907 
2024-05-08 13:26:50.747542: train_loss -0.8051 
2024-05-08 13:26:50.747727: val_loss -0.818 
2024-05-08 13:26:50.747775: Pseudo dice [0.9002] 
2024-05-08 13:26:50.747830: Epoch time: 40.95 s 
2024-05-08 13:26:51.783159:  
2024-05-08 13:26:51.783298: Epoch 104 
2024-05-08 13:26:51.783387: Current learning rate: 0.00906 
2024-05-08 13:27:32.729041: train_loss -0.8087 
2024-05-08 13:27:32.729215: val_loss -0.801 
2024-05-08 13:27:32.729266: Pseudo dice [0.8907] 
2024-05-08 13:27:32.729321: Epoch time: 40.95 s 
2024-05-08 13:27:33.763359:  
2024-05-08 13:27:33.763536: Epoch 105 
2024-05-08 13:27:33.763631: Current learning rate: 0.00905 
2024-05-08 13:28:14.710419: train_loss -0.8128 
2024-05-08 13:28:14.710609: val_loss -0.8095 
2024-05-08 13:28:14.710658: Pseudo dice [0.9009] 
2024-05-08 13:28:14.710714: Epoch time: 40.95 s 
2024-05-08 13:28:14.710757: Yayy! New best EMA pseudo Dice: 0.8949 
2024-05-08 13:28:16.082041:  
2024-05-08 13:28:16.082186: Epoch 106 
2024-05-08 13:28:16.082279: Current learning rate: 0.00904 
2024-05-08 13:28:57.069429: train_loss -0.8016 
2024-05-08 13:28:57.069618: val_loss -0.8075 
2024-05-08 13:28:57.069667: Pseudo dice [0.8972] 
2024-05-08 13:28:57.069723: Epoch time: 40.99 s 
2024-05-08 13:28:57.069765: Yayy! New best EMA pseudo Dice: 0.8952 
2024-05-08 13:28:58.448457:  
2024-05-08 13:28:58.448597: Epoch 107 
2024-05-08 13:28:58.448698: Current learning rate: 0.00903 
2024-05-08 13:29:39.428026: train_loss -0.8086 
2024-05-08 13:29:39.428215: val_loss -0.8197 
2024-05-08 13:29:39.428269: Pseudo dice [0.9053] 
2024-05-08 13:29:39.428327: Epoch time: 40.98 s 
2024-05-08 13:29:39.428375: Yayy! New best EMA pseudo Dice: 0.8962 
2024-05-08 13:29:40.798971:  
2024-05-08 13:29:40.799103: Epoch 108 
2024-05-08 13:29:40.799198: Current learning rate: 0.00902 
2024-05-08 13:30:21.858895: train_loss -0.8002 
2024-05-08 13:30:21.859078: val_loss -0.8181 
2024-05-08 13:30:21.859127: Pseudo dice [0.8982] 
2024-05-08 13:30:21.859182: Epoch time: 41.06 s 
2024-05-08 13:30:21.859225: Yayy! New best EMA pseudo Dice: 0.8964 
2024-05-08 13:30:23.234741:  
2024-05-08 13:30:23.234890: Epoch 109 
2024-05-08 13:30:23.234982: Current learning rate: 0.00901 
2024-05-08 13:31:04.750660: train_loss -0.8082 
2024-05-08 13:31:04.750832: val_loss -0.8024 
2024-05-08 13:31:04.750880: Pseudo dice [0.8941] 
2024-05-08 13:31:04.750933: Epoch time: 41.52 s 
2024-05-08 13:31:05.796615:  
2024-05-08 13:31:05.796765: Epoch 110 
2024-05-08 13:31:05.796854: Current learning rate: 0.009 
2024-05-08 13:31:46.815599: train_loss -0.8082 
2024-05-08 13:31:46.815787: val_loss -0.8199 
2024-05-08 13:31:46.815880: Pseudo dice [0.9088] 
2024-05-08 13:31:46.815937: Epoch time: 41.02 s 
2024-05-08 13:31:46.815983: Yayy! New best EMA pseudo Dice: 0.8974 
2024-05-08 13:31:48.190696:  
2024-05-08 13:31:48.190845: Epoch 111 
2024-05-08 13:31:48.190947: Current learning rate: 0.009 
2024-05-08 13:32:48.531583: train_loss -0.7966 
2024-05-08 13:32:48.532404: val_loss -0.8243 
2024-05-08 13:32:48.532789: Pseudo dice [0.9047] 
2024-05-08 13:32:48.533165: Epoch time: 60.34 s 
2024-05-08 13:32:48.533529: Yayy! New best EMA pseudo Dice: 0.8982 
2024-05-08 13:32:54.060102:  
2024-05-08 13:32:54.069150: Epoch 112 
2024-05-08 13:32:54.069640: Current learning rate: 0.00899 
2024-05-08 13:34:26.384976: train_loss -0.7945 
2024-05-08 13:34:26.387287: val_loss -0.8112 
2024-05-08 13:34:26.387843: Pseudo dice [0.9001] 
2024-05-08 13:34:26.388376: Epoch time: 92.33 s 
2024-05-08 13:34:26.388900: Yayy! New best EMA pseudo Dice: 0.8983 
2024-05-08 13:34:31.679789:  
2024-05-08 13:34:31.680782: Epoch 113 
2024-05-08 13:34:31.681220: Current learning rate: 0.00898 
2024-05-08 13:36:08.245930: train_loss -0.8158 
2024-05-08 13:36:08.246169: val_loss -0.7997 
2024-05-08 13:36:08.246251: Pseudo dice [0.8915] 
2024-05-08 13:36:08.246338: Epoch time: 96.57 s 
2024-05-08 13:36:14.026328:  
2024-05-08 13:36:14.026935: Epoch 114 
2024-05-08 13:36:14.027351: Current learning rate: 0.00897 
2024-05-08 13:36:59.540960: train_loss -0.8073 
2024-05-08 13:36:59.541144: val_loss -0.8209 
2024-05-08 13:36:59.541192: Pseudo dice [0.9059] 
2024-05-08 13:36:59.541245: Epoch time: 45.52 s 
2024-05-08 13:36:59.541287: Yayy! New best EMA pseudo Dice: 0.8985 
2024-05-08 13:37:01.097808:  
2024-05-08 13:37:01.097965: Epoch 115 
2024-05-08 13:37:01.098056: Current learning rate: 0.00896 
2024-05-08 13:37:42.479611: train_loss -0.8072 
2024-05-08 13:37:42.480344: val_loss -0.8141 
2024-05-08 13:37:42.480394: Pseudo dice [0.9012] 
2024-05-08 13:37:42.480449: Epoch time: 41.38 s 
2024-05-08 13:37:42.480492: Yayy! New best EMA pseudo Dice: 0.8988 
2024-05-08 13:37:44.258987:  
2024-05-08 13:37:44.259258: Epoch 116 
2024-05-08 13:37:44.259438: Current learning rate: 0.00895 
2024-05-08 13:38:25.340957: train_loss -0.799 
2024-05-08 13:38:25.341148: val_loss -0.8196 
2024-05-08 13:38:25.341198: Pseudo dice [0.9039] 
2024-05-08 13:38:25.341253: Epoch time: 41.08 s 
2024-05-08 13:38:25.341295: Yayy! New best EMA pseudo Dice: 0.8993 
2024-05-08 13:38:26.757416:  
2024-05-08 13:38:26.757563: Epoch 117 
2024-05-08 13:38:26.757656: Current learning rate: 0.00894 
2024-05-08 13:39:07.763340: train_loss -0.8033 
2024-05-08 13:39:07.763517: val_loss -0.8043 
2024-05-08 13:39:07.763565: Pseudo dice [0.8994] 
2024-05-08 13:39:07.763618: Epoch time: 41.01 s 
2024-05-08 13:39:07.763661: Yayy! New best EMA pseudo Dice: 0.8993 
2024-05-08 13:39:09.166541:  
2024-05-08 13:39:09.166819: Epoch 118 
2024-05-08 13:39:09.166917: Current learning rate: 0.00893 
2024-05-08 13:39:50.491148: train_loss -0.8049 
2024-05-08 13:39:50.491331: val_loss -0.8329 
2024-05-08 13:39:50.491380: Pseudo dice [0.9101] 
2024-05-08 13:39:50.491433: Epoch time: 41.33 s 
2024-05-08 13:39:50.491475: Yayy! New best EMA pseudo Dice: 0.9004 
2024-05-08 13:39:51.917249:  
2024-05-08 13:39:51.917377: Epoch 119 
2024-05-08 13:39:51.917480: Current learning rate: 0.00892 
2024-05-08 13:40:33.081652: train_loss -0.8075 
2024-05-08 13:40:33.081840: val_loss -0.8203 
2024-05-08 13:40:33.081889: Pseudo dice [0.9003] 
2024-05-08 13:40:33.081943: Epoch time: 41.17 s 
2024-05-08 13:40:34.177794:  
2024-05-08 13:40:34.178010: Epoch 120 
2024-05-08 13:40:34.178105: Current learning rate: 0.00891 
2024-05-08 13:41:15.139857: train_loss -0.8144 
2024-05-08 13:41:15.140035: val_loss -0.8215 
2024-05-08 13:41:15.140082: Pseudo dice [0.9033] 
2024-05-08 13:41:15.140134: Epoch time: 40.96 s 
2024-05-08 13:41:15.140177: Yayy! New best EMA pseudo Dice: 0.9007 
2024-05-08 13:41:16.711156:  
2024-05-08 13:41:16.711367: Epoch 121 
2024-05-08 13:41:16.711482: Current learning rate: 0.0089 
2024-05-08 13:41:58.167572: train_loss -0.8086 
2024-05-08 13:41:58.167763: val_loss -0.8235 
2024-05-08 13:41:58.167810: Pseudo dice [0.9029] 
2024-05-08 13:41:58.167863: Epoch time: 41.46 s 
2024-05-08 13:41:58.167905: Yayy! New best EMA pseudo Dice: 0.9009 
2024-05-08 13:41:59.583375:  
2024-05-08 13:41:59.583519: Epoch 122 
2024-05-08 13:41:59.583614: Current learning rate: 0.00889 
2024-05-08 13:42:40.521441: train_loss -0.81 
2024-05-08 13:42:40.521622: val_loss -0.8176 
2024-05-08 13:42:40.521671: Pseudo dice [0.9026] 
2024-05-08 13:42:40.521723: Epoch time: 40.94 s 
2024-05-08 13:42:40.521765: Yayy! New best EMA pseudo Dice: 0.901 
2024-05-08 13:42:41.940696:  
2024-05-08 13:42:41.940916: Epoch 123 
2024-05-08 13:42:41.941018: Current learning rate: 0.00889 
2024-05-08 13:43:22.900883: train_loss -0.8196 
2024-05-08 13:43:22.901198: val_loss -0.8236 
2024-05-08 13:43:22.901282: Pseudo dice [0.9053] 
2024-05-08 13:43:22.901336: Epoch time: 40.96 s 
2024-05-08 13:43:22.901379: Yayy! New best EMA pseudo Dice: 0.9015 
2024-05-08 13:43:24.310331:  
2024-05-08 13:43:24.310460: Epoch 124 
2024-05-08 13:43:24.310549: Current learning rate: 0.00888 
2024-05-08 13:44:05.814035: train_loss -0.8244 
2024-05-08 13:44:05.814212: val_loss -0.8096 
2024-05-08 13:44:05.814261: Pseudo dice [0.8999] 
2024-05-08 13:44:05.814316: Epoch time: 41.5 s 
2024-05-08 13:44:06.885153:  
2024-05-08 13:44:06.885377: Epoch 125 
2024-05-08 13:44:06.885473: Current learning rate: 0.00887 
2024-05-08 13:44:47.890425: train_loss -0.8201 
2024-05-08 13:44:47.890601: val_loss -0.827 
2024-05-08 13:44:47.890650: Pseudo dice [0.9057] 
2024-05-08 13:44:47.890702: Epoch time: 41.01 s 
2024-05-08 13:44:47.890744: Yayy! New best EMA pseudo Dice: 0.9018 
2024-05-08 13:44:49.311699:  
2024-05-08 13:44:49.311873: Epoch 126 
2024-05-08 13:44:49.311963: Current learning rate: 0.00886 
2024-05-08 13:45:30.375460: train_loss -0.8209 
2024-05-08 13:45:30.375632: val_loss -0.8102 
2024-05-08 13:45:30.375680: Pseudo dice [0.8992] 
2024-05-08 13:45:30.375732: Epoch time: 41.06 s 
2024-05-08 13:45:31.615915:  
2024-05-08 13:45:31.616160: Epoch 127 
2024-05-08 13:45:31.616257: Current learning rate: 0.00885 
2024-05-08 13:46:12.695707: train_loss -0.8131 
2024-05-08 13:46:12.695891: val_loss -0.8145 
2024-05-08 13:46:12.695940: Pseudo dice [0.8984] 
2024-05-08 13:46:12.695993: Epoch time: 41.08 s 
2024-05-08 13:46:13.768010:  
2024-05-08 13:46:13.768137: Epoch 128 
2024-05-08 13:46:13.768228: Current learning rate: 0.00884 
2024-05-08 13:46:54.815728: train_loss -0.8168 
2024-05-08 13:46:54.815888: val_loss -0.8068 
2024-05-08 13:46:54.815936: Pseudo dice [0.8931] 
2024-05-08 13:46:54.815987: Epoch time: 41.05 s 
2024-05-08 13:46:55.888778:  
2024-05-08 13:46:55.888976: Epoch 129 
2024-05-08 13:46:55.889073: Current learning rate: 0.00883 
2024-05-08 13:47:36.933385: train_loss -0.8113 
2024-05-08 13:47:36.933555: val_loss -0.8179 
2024-05-08 13:47:36.933614: Pseudo dice [0.9034] 
2024-05-08 13:47:36.933667: Epoch time: 41.05 s 
2024-05-08 13:47:38.005086:  
2024-05-08 13:47:38.005221: Epoch 130 
2024-05-08 13:47:38.005312: Current learning rate: 0.00882 
2024-05-08 13:48:19.007992: train_loss -0.8165 
2024-05-08 13:48:19.008169: val_loss -0.8004 
2024-05-08 13:48:19.008219: Pseudo dice [0.8931] 
2024-05-08 13:48:19.008271: Epoch time: 41.0 s 
2024-05-08 13:48:20.081372:  
2024-05-08 13:48:20.081584: Epoch 131 
2024-05-08 13:48:20.081679: Current learning rate: 0.00881 
2024-05-08 13:49:01.575767: train_loss -0.8105 
2024-05-08 13:49:01.575948: val_loss -0.806 
2024-05-08 13:49:01.575998: Pseudo dice [0.8964] 
2024-05-08 13:49:01.576052: Epoch time: 41.5 s 
2024-05-08 13:49:02.659007:  
2024-05-08 13:49:02.659143: Epoch 132 
2024-05-08 13:49:02.659231: Current learning rate: 0.0088 
2024-05-08 13:49:43.617073: train_loss -0.8208 
2024-05-08 13:49:43.617250: val_loss -0.8254 
2024-05-08 13:49:43.617299: Pseudo dice [0.9036] 
2024-05-08 13:49:43.617350: Epoch time: 40.96 s 
2024-05-08 13:49:44.691382:  
2024-05-08 13:49:44.691515: Epoch 133 
2024-05-08 13:49:44.691605: Current learning rate: 0.00879 
2024-05-08 13:50:25.630183: train_loss -0.8134 
2024-05-08 13:50:25.630361: val_loss -0.8185 
2024-05-08 13:50:25.630409: Pseudo dice [0.9045] 
2024-05-08 13:50:25.630461: Epoch time: 40.94 s 
2024-05-08 13:50:26.906129:  
2024-05-08 13:50:26.906282: Epoch 134 
2024-05-08 13:50:26.906371: Current learning rate: 0.00879 
2024-05-08 13:51:08.154872: train_loss -0.8136 
2024-05-08 13:51:08.155051: val_loss -0.8177 
2024-05-08 13:51:08.155098: Pseudo dice [0.8989] 
2024-05-08 13:51:08.155151: Epoch time: 41.25 s 
2024-05-08 13:51:09.268117:  
2024-05-08 13:51:09.268262: Epoch 135 
2024-05-08 13:51:09.268355: Current learning rate: 0.00878 
2024-05-08 13:51:50.291141: train_loss -0.8192 
2024-05-08 13:51:50.291335: val_loss -0.8322 
2024-05-08 13:51:50.291383: Pseudo dice [0.9104] 
2024-05-08 13:51:50.291436: Epoch time: 41.02 s 
2024-05-08 13:51:51.382894:  
2024-05-08 13:51:51.383082: Epoch 136 
2024-05-08 13:51:51.383202: Current learning rate: 0.00877 
2024-05-08 13:52:32.784349: train_loss -0.8167 
2024-05-08 13:52:32.784525: val_loss -0.8284 
2024-05-08 13:52:32.784576: Pseudo dice [0.9086] 
2024-05-08 13:52:32.784628: Epoch time: 41.4 s 
2024-05-08 13:52:32.784679: Yayy! New best EMA pseudo Dice: 0.902 
2024-05-08 13:52:34.214578:  
2024-05-08 13:52:34.214710: Epoch 137 
2024-05-08 13:52:34.214797: Current learning rate: 0.00876 
2024-05-08 13:53:15.205374: train_loss -0.8179 
2024-05-08 13:53:15.205555: val_loss -0.8209 
2024-05-08 13:53:15.205606: Pseudo dice [0.9063] 
2024-05-08 13:53:15.205658: Epoch time: 40.99 s 
2024-05-08 13:53:15.205700: Yayy! New best EMA pseudo Dice: 0.9024 
2024-05-08 13:53:16.636610:  
2024-05-08 13:53:16.636749: Epoch 138 
2024-05-08 13:53:16.636841: Current learning rate: 0.00875 
2024-05-08 13:53:57.589505: train_loss -0.8121 
2024-05-08 13:53:57.589691: val_loss -0.8152 
2024-05-08 13:53:57.589740: Pseudo dice [0.9011] 
2024-05-08 13:53:57.589793: Epoch time: 40.95 s 
2024-05-08 13:53:58.681099:  
2024-05-08 13:53:58.681221: Epoch 139 
2024-05-08 13:53:58.681310: Current learning rate: 0.00874 
2024-05-08 13:54:39.623856: train_loss -0.7971 
2024-05-08 13:54:39.624035: val_loss -0.8095 
2024-05-08 13:54:39.624084: Pseudo dice [0.9016] 
2024-05-08 13:54:39.624138: Epoch time: 40.94 s 
2024-05-08 13:54:40.935613:  
2024-05-08 13:54:40.935747: Epoch 140 
2024-05-08 13:54:40.935846: Current learning rate: 0.00873 
2024-05-08 13:55:21.869554: train_loss -0.804 
2024-05-08 13:55:21.869735: val_loss -0.8316 
2024-05-08 13:55:21.869784: Pseudo dice [0.9081] 
2024-05-08 13:55:21.869842: Epoch time: 40.93 s 
2024-05-08 13:55:21.869887: Yayy! New best EMA pseudo Dice: 0.9028 
2024-05-08 13:55:23.306848:  
2024-05-08 13:55:23.307047: Epoch 141 
2024-05-08 13:55:23.307144: Current learning rate: 0.00872 
2024-05-08 13:56:04.255764: train_loss -0.8153 
2024-05-08 13:56:04.255948: val_loss -0.8319 
2024-05-08 13:56:04.255996: Pseudo dice [0.9068] 
2024-05-08 13:56:04.256049: Epoch time: 40.95 s 
2024-05-08 13:56:04.256091: Yayy! New best EMA pseudo Dice: 0.9032 
2024-05-08 13:56:05.684904:  
2024-05-08 13:56:05.685046: Epoch 142 
2024-05-08 13:56:05.685134: Current learning rate: 0.00871 
2024-05-08 13:56:46.627332: train_loss -0.8235 
2024-05-08 13:56:46.627502: val_loss -0.8208 
2024-05-08 13:56:46.627551: Pseudo dice [0.9012] 
2024-05-08 13:56:46.627604: Epoch time: 40.94 s 
2024-05-08 13:56:47.720117:  
2024-05-08 13:56:47.720254: Epoch 143 
2024-05-08 13:56:47.720348: Current learning rate: 0.0087 
2024-05-08 13:57:28.646083: train_loss -0.8177 
2024-05-08 13:57:28.646264: val_loss -0.8177 
2024-05-08 13:57:28.646313: Pseudo dice [0.9075] 
2024-05-08 13:57:28.646365: Epoch time: 40.93 s 
2024-05-08 13:57:28.646409: Yayy! New best EMA pseudo Dice: 0.9035 
2024-05-08 13:57:30.072502:  
2024-05-08 13:57:30.072640: Epoch 144 
2024-05-08 13:57:30.072752: Current learning rate: 0.00869 
2024-05-08 13:58:18.809652: train_loss -0.8187 
2024-05-08 13:58:18.812088: val_loss -0.8379 
2024-05-08 13:58:18.812611: Pseudo dice [0.9116] 
2024-05-08 13:58:18.813152: Epoch time: 48.74 s 
2024-05-08 13:58:18.813666: Yayy! New best EMA pseudo Dice: 0.9043 
2024-05-08 13:58:24.446388:  
2024-05-08 13:58:24.446912: Epoch 145 
2024-05-08 13:58:24.447266: Current learning rate: 0.00868 
2024-05-08 13:59:41.103482: train_loss -0.8182 
2024-05-08 13:59:41.103657: val_loss -0.8078 
2024-05-08 13:59:41.103706: Pseudo dice [0.899] 
2024-05-08 13:59:41.103770: Epoch time: 76.66 s 
2024-05-08 13:59:42.383441:  
2024-05-08 13:59:42.383616: Epoch 146 
2024-05-08 13:59:42.383731: Current learning rate: 0.00868 
2024-05-08 14:01:31.938919: train_loss -0.8188 
2024-05-08 14:01:31.961877: val_loss -0.8141 
2024-05-08 14:01:31.962867: Pseudo dice [0.9012] 
2024-05-08 14:01:31.963604: Epoch time: 109.56 s 
2024-05-08 14:01:38.497003:  
2024-05-08 14:01:38.497670: Epoch 147 
2024-05-08 14:01:38.498102: Current learning rate: 0.00867 
2024-05-08 14:03:43.791712: train_loss -0.8208 
2024-05-08 14:03:43.792021: val_loss -0.8327 
2024-05-08 14:03:43.792332: Pseudo dice [0.908] 
2024-05-08 14:03:43.792478: Epoch time: 125.3 s 
2024-05-08 14:03:45.324901:  
2024-05-08 14:03:45.325036: Epoch 148 
2024-05-08 14:03:45.325128: Current learning rate: 0.00866 
2024-05-08 14:04:26.456845: train_loss -0.8179 
2024-05-08 14:04:26.457020: val_loss -0.8328 
2024-05-08 14:04:26.457071: Pseudo dice [0.9098] 
2024-05-08 14:04:26.457124: Epoch time: 41.13 s 
2024-05-08 14:04:26.457166: Yayy! New best EMA pseudo Dice: 0.9045 
2024-05-08 14:04:27.911319:  
2024-05-08 14:04:27.911464: Epoch 149 
2024-05-08 14:04:27.911552: Current learning rate: 0.00865 
2024-05-08 14:05:08.964316: train_loss -0.8137 
2024-05-08 14:05:08.964473: val_loss -0.8225 
2024-05-08 14:05:08.964522: Pseudo dice [0.9069] 
2024-05-08 14:05:08.964575: Epoch time: 41.05 s 
2024-05-08 14:05:09.310411: Yayy! New best EMA pseudo Dice: 0.9048 
2024-05-08 14:05:10.775442:  
2024-05-08 14:05:10.775582: Epoch 150 
2024-05-08 14:05:10.775676: Current learning rate: 0.00864 
2024-05-08 14:05:51.844914: train_loss -0.8176 
2024-05-08 14:05:51.845078: val_loss -0.8365 
2024-05-08 14:05:51.845126: Pseudo dice [0.9091] 
2024-05-08 14:05:51.845180: Epoch time: 41.07 s 
2024-05-08 14:05:51.845223: Yayy! New best EMA pseudo Dice: 0.9052 
2024-05-08 14:05:53.297775:  
2024-05-08 14:05:53.297910: Epoch 151 
2024-05-08 14:05:53.297999: Current learning rate: 0.00863 
2024-05-08 14:06:34.331215: train_loss -0.8066 
2024-05-08 14:06:34.331390: val_loss -0.7968 
2024-05-08 14:06:34.331438: Pseudo dice [0.8861] 
2024-05-08 14:06:34.331491: Epoch time: 41.03 s 
2024-05-08 14:06:35.449928:  
2024-05-08 14:06:35.450054: Epoch 152 
2024-05-08 14:06:35.450143: Current learning rate: 0.00862 
2024-05-08 14:07:16.493294: train_loss -0.8066 
2024-05-08 14:07:16.493460: val_loss -0.8184 
2024-05-08 14:07:16.493508: Pseudo dice [0.9058] 
2024-05-08 14:07:16.493561: Epoch time: 41.04 s 
2024-05-08 14:07:17.814654:  
2024-05-08 14:07:17.814874: Epoch 153 
2024-05-08 14:07:17.814968: Current learning rate: 0.00861 
2024-05-08 14:07:58.849311: train_loss -0.8155 
2024-05-08 14:07:58.849489: val_loss -0.8205 
2024-05-08 14:07:58.849539: Pseudo dice [0.9045] 
2024-05-08 14:07:58.849591: Epoch time: 41.04 s 
2024-05-08 14:07:59.984316:  
2024-05-08 14:07:59.984504: Epoch 154 
2024-05-08 14:07:59.984604: Current learning rate: 0.0086 
2024-05-08 14:08:41.004255: train_loss -0.8121 
2024-05-08 14:08:41.004425: val_loss -0.825 
2024-05-08 14:08:41.004475: Pseudo dice [0.9022] 
2024-05-08 14:08:41.004528: Epoch time: 41.02 s 
2024-05-08 14:08:42.139483:  
2024-05-08 14:08:42.139606: Epoch 155 
2024-05-08 14:08:42.139692: Current learning rate: 0.00859 
2024-05-08 14:09:23.169157: train_loss -0.8167 
2024-05-08 14:09:23.169315: val_loss -0.826 
2024-05-08 14:09:23.169364: Pseudo dice [0.9082] 
2024-05-08 14:09:23.169420: Epoch time: 41.03 s 
2024-05-08 14:09:24.321163:  
2024-05-08 14:09:24.321311: Epoch 156 
2024-05-08 14:09:24.321411: Current learning rate: 0.00858 
2024-05-08 14:10:05.360354: train_loss -0.8222 
2024-05-08 14:10:05.360530: val_loss -0.8283 
2024-05-08 14:10:05.360579: Pseudo dice [0.9135] 
2024-05-08 14:10:05.360632: Epoch time: 41.04 s 
2024-05-08 14:10:06.490973:  
2024-05-08 14:10:06.491112: Epoch 157 
2024-05-08 14:10:06.491205: Current learning rate: 0.00858 
2024-05-08 14:10:47.526389: train_loss -0.8164 
2024-05-08 14:10:47.526558: val_loss -0.8277 
2024-05-08 14:10:47.526608: Pseudo dice [0.9072] 
2024-05-08 14:10:47.526672: Epoch time: 41.04 s 
2024-05-08 14:10:48.661837:  
2024-05-08 14:10:48.661964: Epoch 158 
2024-05-08 14:10:48.662051: Current learning rate: 0.00857 
2024-05-08 14:11:29.746826: train_loss -0.8232 
2024-05-08 14:11:29.747026: val_loss -0.8462 
2024-05-08 14:11:29.747074: Pseudo dice [0.9145] 
2024-05-08 14:11:29.747127: Epoch time: 41.09 s 
2024-05-08 14:11:29.747173: Yayy! New best EMA pseudo Dice: 0.9061 
2024-05-08 14:11:31.410894:  
2024-05-08 14:11:31.411055: Epoch 159 
2024-05-08 14:11:31.411151: Current learning rate: 0.00856 
2024-05-08 14:12:12.469111: train_loss -0.8269 
2024-05-08 14:12:12.469314: val_loss -0.8447 
2024-05-08 14:12:12.469371: Pseudo dice [0.9168] 
2024-05-08 14:12:12.469426: Epoch time: 41.06 s 
2024-05-08 14:12:12.469468: Yayy! New best EMA pseudo Dice: 0.9072 
2024-05-08 14:12:13.935986:  
2024-05-08 14:12:13.936119: Epoch 160 
2024-05-08 14:12:13.936209: Current learning rate: 0.00855 
2024-05-08 14:12:55.005194: train_loss -0.813 
2024-05-08 14:12:55.005387: val_loss -0.82 
2024-05-08 14:12:55.005436: Pseudo dice [0.9045] 
2024-05-08 14:12:55.005488: Epoch time: 41.07 s 
2024-05-08 14:12:56.132019:  
2024-05-08 14:12:56.132153: Epoch 161 
2024-05-08 14:12:56.132239: Current learning rate: 0.00854 
2024-05-08 14:13:37.187666: train_loss -0.8076 
2024-05-08 14:13:37.187833: val_loss -0.8126 
2024-05-08 14:13:37.187882: Pseudo dice [0.9028] 
2024-05-08 14:13:37.187935: Epoch time: 41.06 s 
2024-05-08 14:13:38.325420:  
2024-05-08 14:13:38.325616: Epoch 162 
2024-05-08 14:13:38.325709: Current learning rate: 0.00853 
2024-05-08 14:14:19.359695: train_loss -0.8238 
2024-05-08 14:14:19.359866: val_loss -0.8432 
2024-05-08 14:14:19.359914: Pseudo dice [0.9175] 
2024-05-08 14:14:19.359967: Epoch time: 41.04 s 
2024-05-08 14:14:19.360010: Yayy! New best EMA pseudo Dice: 0.9076 
2024-05-08 14:14:20.827786:  
2024-05-08 14:14:20.827993: Epoch 163 
2024-05-08 14:14:20.828084: Current learning rate: 0.00852 
2024-05-08 14:15:01.858349: train_loss -0.8277 
2024-05-08 14:15:01.858526: val_loss -0.8286 
2024-05-08 14:15:01.858575: Pseudo dice [0.9063] 
2024-05-08 14:15:01.858627: Epoch time: 41.03 s 
2024-05-08 14:15:03.159011:  
2024-05-08 14:15:03.159148: Epoch 164 
2024-05-08 14:15:03.159234: Current learning rate: 0.00851 
2024-05-08 14:15:44.204791: train_loss -0.8271 
2024-05-08 14:15:44.204960: val_loss -0.8334 
2024-05-08 14:15:44.205012: Pseudo dice [0.9088] 
2024-05-08 14:15:44.205065: Epoch time: 41.05 s 
2024-05-08 14:15:44.205108: Yayy! New best EMA pseudo Dice: 0.9076 
2024-05-08 14:15:45.644168:  
2024-05-08 14:15:45.644368: Epoch 165 
2024-05-08 14:15:45.644468: Current learning rate: 0.0085 
2024-05-08 14:16:26.680926: train_loss -0.83 
2024-05-08 14:16:26.681097: val_loss -0.835 
2024-05-08 14:16:26.681144: Pseudo dice [0.9075] 
2024-05-08 14:16:26.681196: Epoch time: 41.04 s 
2024-05-08 14:16:27.779357:  
2024-05-08 14:16:27.779625: Epoch 166 
2024-05-08 14:16:27.779718: Current learning rate: 0.00849 
2024-05-08 14:17:08.842047: train_loss -0.8271 
2024-05-08 14:17:08.842207: val_loss -0.8231 
2024-05-08 14:17:08.842259: Pseudo dice [0.9098] 
2024-05-08 14:17:08.842315: Epoch time: 41.06 s 
2024-05-08 14:17:08.842359: Yayy! New best EMA pseudo Dice: 0.9078 
2024-05-08 14:17:10.292537:  
2024-05-08 14:17:10.292755: Epoch 167 
2024-05-08 14:17:10.292852: Current learning rate: 0.00848 
2024-05-08 14:17:51.359492: train_loss -0.8328 
2024-05-08 14:17:51.359668: val_loss -0.8258 
2024-05-08 14:17:51.359717: Pseudo dice [0.9035] 
2024-05-08 14:17:51.359769: Epoch time: 41.07 s 
2024-05-08 14:17:52.477048:  
2024-05-08 14:17:52.477182: Epoch 168 
2024-05-08 14:17:52.477271: Current learning rate: 0.00847 
2024-05-08 14:18:33.593224: train_loss -0.8277 
2024-05-08 14:18:33.593401: val_loss -0.8399 
2024-05-08 14:18:33.593450: Pseudo dice [0.9126] 
2024-05-08 14:18:33.593502: Epoch time: 41.12 s 
2024-05-08 14:18:33.593544: Yayy! New best EMA pseudo Dice: 0.9079 
2024-05-08 14:18:35.077631:  
2024-05-08 14:18:35.077766: Epoch 169 
2024-05-08 14:18:35.077852: Current learning rate: 0.00847 
2024-05-08 14:19:16.160298: train_loss -0.8206 
2024-05-08 14:19:16.160468: val_loss -0.8379 
2024-05-08 14:19:16.160517: Pseudo dice [0.9126] 
2024-05-08 14:19:16.160570: Epoch time: 41.08 s 
2024-05-08 14:19:16.160611: Yayy! New best EMA pseudo Dice: 0.9084 
2024-05-08 14:19:17.802553:  
2024-05-08 14:19:17.802684: Epoch 170 
2024-05-08 14:19:17.802777: Current learning rate: 0.00846 
2024-05-08 14:19:58.894975: train_loss -0.8296 
2024-05-08 14:19:58.895149: val_loss -0.843 
2024-05-08 14:19:58.895196: Pseudo dice [0.916] 
2024-05-08 14:19:58.895248: Epoch time: 41.09 s 
2024-05-08 14:19:58.895290: Yayy! New best EMA pseudo Dice: 0.9091 
2024-05-08 14:20:00.406756:  
2024-05-08 14:20:00.406904: Epoch 171 
2024-05-08 14:20:00.406992: Current learning rate: 0.00845 
2024-05-08 14:20:41.499289: train_loss -0.8242 
2024-05-08 14:20:41.499614: val_loss -0.8343 
2024-05-08 14:20:41.499710: Pseudo dice [0.9099] 
2024-05-08 14:20:41.499767: Epoch time: 41.09 s 
2024-05-08 14:20:41.499809: Yayy! New best EMA pseudo Dice: 0.9092 
2024-05-08 14:20:42.963007:  
2024-05-08 14:20:42.963234: Epoch 172 
2024-05-08 14:20:42.963337: Current learning rate: 0.00844 
2024-05-08 14:21:24.022070: train_loss -0.8256 
2024-05-08 14:21:24.022241: val_loss -0.835 
2024-05-08 14:21:24.022292: Pseudo dice [0.9094] 
2024-05-08 14:21:24.022344: Epoch time: 41.06 s 
2024-05-08 14:21:24.022387: Yayy! New best EMA pseudo Dice: 0.9092 
2024-05-08 14:21:25.485881:  
2024-05-08 14:21:25.486087: Epoch 173 
2024-05-08 14:21:25.486177: Current learning rate: 0.00843 
2024-05-08 14:22:06.509485: train_loss -0.8218 
2024-05-08 14:22:06.509665: val_loss -0.8188 
2024-05-08 14:22:06.509715: Pseudo dice [0.9028] 
2024-05-08 14:22:06.509769: Epoch time: 41.02 s 
2024-05-08 14:22:07.651134:  
2024-05-08 14:22:07.651266: Epoch 174 
2024-05-08 14:22:07.651349: Current learning rate: 0.00842 
2024-05-08 14:22:48.676113: train_loss -0.833 
2024-05-08 14:22:48.676290: val_loss -0.8341 
2024-05-08 14:22:48.676338: Pseudo dice [0.9127] 
2024-05-08 14:22:48.676392: Epoch time: 41.03 s 
2024-05-08 14:22:49.801340:  
2024-05-08 14:22:49.801851: Epoch 175 
2024-05-08 14:22:49.801947: Current learning rate: 0.00841 
2024-05-08 14:23:30.831761: train_loss -0.8226 
2024-05-08 14:23:30.831939: val_loss -0.8344 
2024-05-08 14:23:30.831990: Pseudo dice [0.9061] 
2024-05-08 14:23:30.832042: Epoch time: 41.03 s 
2024-05-08 14:23:32.138162:  
2024-05-08 14:23:32.138361: Epoch 176 
2024-05-08 14:23:32.138451: Current learning rate: 0.0084 
2024-05-08 14:24:13.197519: train_loss -0.8211 
2024-05-08 14:24:13.197699: val_loss -0.8174 
2024-05-08 14:24:13.197748: Pseudo dice [0.9031] 
2024-05-08 14:24:13.197818: Epoch time: 41.06 s 
2024-05-08 14:24:14.319993:  
2024-05-08 14:24:14.320183: Epoch 177 
2024-05-08 14:24:14.320277: Current learning rate: 0.00839 
2024-05-08 14:24:55.396566: train_loss -0.8175 
2024-05-08 14:24:55.396787: val_loss -0.8226 
2024-05-08 14:24:55.396840: Pseudo dice [0.9031] 
2024-05-08 14:24:55.396894: Epoch time: 41.08 s 
2024-05-08 14:24:56.542174:  
2024-05-08 14:24:56.542499: Epoch 178 
2024-05-08 14:24:56.542595: Current learning rate: 0.00838 
2024-05-08 14:25:37.634180: train_loss -0.8067 
2024-05-08 14:25:37.634348: val_loss -0.8178 
2024-05-08 14:25:37.634398: Pseudo dice [0.9048] 
2024-05-08 14:25:37.634457: Epoch time: 41.09 s 
2024-05-08 14:25:38.749500:  
2024-05-08 14:25:38.749619: Epoch 179 
2024-05-08 14:25:38.749706: Current learning rate: 0.00837 
2024-05-08 14:26:19.803231: train_loss -0.8247 
2024-05-08 14:26:19.803408: val_loss -0.8265 
2024-05-08 14:26:19.803457: Pseudo dice [0.9064] 
2024-05-08 14:26:19.803511: Epoch time: 41.05 s 
2024-05-08 14:26:20.927791:  
2024-05-08 14:26:20.927919: Epoch 180 
2024-05-08 14:26:20.928011: Current learning rate: 0.00836 
2024-05-08 14:27:02.536271: train_loss -0.8184 
2024-05-08 14:27:02.536453: val_loss -0.8275 
2024-05-08 14:27:02.536502: Pseudo dice [0.9036] 
2024-05-08 14:27:02.536557: Epoch time: 41.61 s 
2024-05-08 14:27:03.675364:  
2024-05-08 14:27:03.675606: Epoch 181 
2024-05-08 14:27:03.675805: Current learning rate: 0.00836 
2024-05-08 14:27:44.748546: train_loss -0.827 
2024-05-08 14:27:44.748722: val_loss -0.823 
2024-05-08 14:27:44.748773: Pseudo dice [0.9095] 
2024-05-08 14:27:44.748826: Epoch time: 41.07 s 
2024-05-08 14:27:46.044029:  
2024-05-08 14:27:46.044166: Epoch 182 
2024-05-08 14:27:46.044254: Current learning rate: 0.00835 
2024-05-08 14:28:27.128961: train_loss -0.8277 
2024-05-08 14:28:27.129153: val_loss -0.8347 
2024-05-08 14:28:27.129208: Pseudo dice [0.9115] 
2024-05-08 14:28:27.129262: Epoch time: 41.09 s 
2024-05-08 14:28:28.246302:  
2024-05-08 14:28:28.246530: Epoch 183 
2024-05-08 14:28:28.246635: Current learning rate: 0.00834 
2024-05-08 14:29:09.305083: train_loss -0.8314 
2024-05-08 14:29:09.305256: val_loss -0.8352 
2024-05-08 14:29:09.305304: Pseudo dice [0.9144] 
2024-05-08 14:29:09.305357: Epoch time: 41.06 s 
2024-05-08 14:29:10.419040:  
2024-05-08 14:29:10.419181: Epoch 184 
2024-05-08 14:29:10.419272: Current learning rate: 0.00833 
2024-05-08 14:29:51.463858: train_loss -0.8301 
2024-05-08 14:29:51.464036: val_loss -0.8365 
2024-05-08 14:29:51.464091: Pseudo dice [0.9104] 
2024-05-08 14:29:51.464144: Epoch time: 41.05 s 
2024-05-08 14:29:52.584246:  
2024-05-08 14:29:52.584379: Epoch 185 
2024-05-08 14:29:52.584467: Current learning rate: 0.00832 
2024-05-08 14:30:33.639614: train_loss -0.825 
2024-05-08 14:30:33.639786: val_loss -0.841 
2024-05-08 14:30:33.639835: Pseudo dice [0.915] 
2024-05-08 14:30:33.639889: Epoch time: 41.06 s 
2024-05-08 14:30:34.758486:  
2024-05-08 14:30:34.758753: Epoch 186 
2024-05-08 14:30:34.758844: Current learning rate: 0.00831 
2024-05-08 14:31:15.804583: train_loss -0.8268 
2024-05-08 14:31:15.804786: val_loss -0.8371 
2024-05-08 14:31:15.804836: Pseudo dice [0.9129] 
2024-05-08 14:31:15.804890: Epoch time: 41.05 s 
2024-05-08 14:31:15.804934: Yayy! New best EMA pseudo Dice: 0.9095 
2024-05-08 14:31:17.279565:  
2024-05-08 14:31:17.279695: Epoch 187 
2024-05-08 14:31:17.279779: Current learning rate: 0.0083 
2024-05-08 14:31:58.324363: train_loss -0.8299 
2024-05-08 14:31:58.324537: val_loss -0.8398 
2024-05-08 14:31:58.324586: Pseudo dice [0.9138] 
2024-05-08 14:31:58.324639: Epoch time: 41.05 s 
2024-05-08 14:31:58.324689: Yayy! New best EMA pseudo Dice: 0.9099 
2024-05-08 14:31:59.781615:  
2024-05-08 14:31:59.781747: Epoch 188 
2024-05-08 14:31:59.781833: Current learning rate: 0.00829 
2024-05-08 14:32:40.867646: train_loss -0.8295 
2024-05-08 14:32:40.867827: val_loss -0.8313 
2024-05-08 14:32:40.867875: Pseudo dice [0.9084] 
2024-05-08 14:32:40.867928: Epoch time: 41.09 s 
2024-05-08 14:32:42.182503:  
2024-05-08 14:32:42.182750: Epoch 189 
2024-05-08 14:32:42.182846: Current learning rate: 0.00828 
2024-05-08 14:33:23.277162: train_loss -0.8204 
2024-05-08 14:33:23.277346: val_loss -0.82 
2024-05-08 14:33:23.277396: Pseudo dice [0.8994] 
2024-05-08 14:33:23.277449: Epoch time: 41.1 s 
2024-05-08 14:33:24.395239:  
2024-05-08 14:33:24.395447: Epoch 190 
2024-05-08 14:33:24.395536: Current learning rate: 0.00827 
2024-05-08 14:34:05.555136: train_loss -0.8215 
2024-05-08 14:34:05.555305: val_loss -0.8364 
2024-05-08 14:34:05.555362: Pseudo dice [0.9125] 
2024-05-08 14:34:05.555416: Epoch time: 41.16 s 
2024-05-08 14:34:06.677351:  
2024-05-08 14:34:06.677489: Epoch 191 
2024-05-08 14:34:06.677579: Current learning rate: 0.00826 
2024-05-08 14:34:48.300058: train_loss -0.8239 
2024-05-08 14:34:48.300242: val_loss -0.836 
2024-05-08 14:34:48.300294: Pseudo dice [0.9077] 
2024-05-08 14:34:48.300348: Epoch time: 41.62 s 
2024-05-08 14:34:49.441943:  
2024-05-08 14:34:49.442163: Epoch 192 
2024-05-08 14:34:49.442254: Current learning rate: 0.00825 
2024-05-08 14:35:30.487705: train_loss -0.8322 
2024-05-08 14:35:30.487883: val_loss -0.8472 
2024-05-08 14:35:30.487931: Pseudo dice [0.9199] 
2024-05-08 14:35:30.487985: Epoch time: 41.05 s 
2024-05-08 14:35:30.488029: Yayy! New best EMA pseudo Dice: 0.9101 
2024-05-08 14:35:31.962293:  
2024-05-08 14:35:31.962422: Epoch 193 
2024-05-08 14:35:31.962511: Current learning rate: 0.00824 
2024-05-08 14:36:13.042361: train_loss -0.8166 
2024-05-08 14:36:13.042538: val_loss -0.8193 
2024-05-08 14:36:13.042593: Pseudo dice [0.9032] 
2024-05-08 14:36:13.042646: Epoch time: 41.08 s 
2024-05-08 14:36:14.353522:  
2024-05-08 14:36:14.353716: Epoch 194 
2024-05-08 14:36:14.353812: Current learning rate: 0.00824 
2024-05-08 14:36:55.471964: train_loss -0.809 
2024-05-08 14:36:55.472149: val_loss -0.817 
2024-05-08 14:36:55.472202: Pseudo dice [0.9042] 
2024-05-08 14:36:55.472255: Epoch time: 41.12 s 
2024-05-08 14:36:56.606536:  
2024-05-08 14:36:56.606745: Epoch 195 
2024-05-08 14:36:56.606899: Current learning rate: 0.00823 
2024-05-08 14:37:37.716035: train_loss -0.8225 
2024-05-08 14:37:37.716203: val_loss -0.8301 
2024-05-08 14:37:37.716254: Pseudo dice [0.913] 
2024-05-08 14:37:37.716313: Epoch time: 41.11 s 
2024-05-08 14:37:38.849093:  
2024-05-08 14:37:38.849228: Epoch 196 
2024-05-08 14:37:38.849320: Current learning rate: 0.00822 
2024-05-08 14:38:19.958173: train_loss -0.8206 
2024-05-08 14:38:19.958351: val_loss -0.8313 
2024-05-08 14:38:19.958399: Pseudo dice [0.9095] 
2024-05-08 14:38:19.958452: Epoch time: 41.11 s 
2024-05-08 14:38:21.098572:  
2024-05-08 14:38:21.098702: Epoch 197 
2024-05-08 14:38:21.098788: Current learning rate: 0.00821 
2024-05-08 14:39:02.192102: train_loss -0.8328 
2024-05-08 14:39:02.192275: val_loss -0.8302 
2024-05-08 14:39:02.192326: Pseudo dice [0.9063] 
2024-05-08 14:39:02.192380: Epoch time: 41.09 s 
2024-05-08 14:39:03.322436:  
2024-05-08 14:39:03.322566: Epoch 198 
2024-05-08 14:39:03.322653: Current learning rate: 0.0082 
2024-05-08 14:39:44.445338: train_loss -0.8235 
2024-05-08 14:39:44.445518: val_loss -0.8373 
2024-05-08 14:39:44.445568: Pseudo dice [0.9127] 
2024-05-08 14:39:44.445622: Epoch time: 41.12 s 
2024-05-08 14:39:45.584273:  
2024-05-08 14:39:45.584397: Epoch 199 
2024-05-08 14:39:45.584483: Current learning rate: 0.00819 
2024-05-08 14:40:26.680668: train_loss -0.8165 
2024-05-08 14:40:26.680844: val_loss -0.8255 
2024-05-08 14:40:26.680894: Pseudo dice [0.9028] 
2024-05-08 14:40:26.680948: Epoch time: 41.1 s 
2024-05-08 14:40:28.183124:  
2024-05-08 14:40:28.183447: Epoch 200 
2024-05-08 14:40:28.183575: Current learning rate: 0.00818 
2024-05-08 14:41:09.818618: train_loss -0.8319 
2024-05-08 14:41:09.818756: val_loss -0.8375 
2024-05-08 14:41:09.818804: Pseudo dice [0.9106] 
2024-05-08 14:41:09.818855: Epoch time: 41.64 s 
2024-05-08 14:41:11.151865:  
2024-05-08 14:41:11.152002: Epoch 201 
2024-05-08 14:41:11.152087: Current learning rate: 0.00817 
2024-05-08 14:41:52.280347: train_loss -0.8209 
2024-05-08 14:41:52.280520: val_loss -0.8165 
2024-05-08 14:41:52.280567: Pseudo dice [0.9036] 
2024-05-08 14:41:52.280621: Epoch time: 41.13 s 
2024-05-08 14:41:53.412104:  
2024-05-08 14:41:53.412356: Epoch 202 
2024-05-08 14:41:53.412451: Current learning rate: 0.00816 
2024-05-08 14:42:34.489067: train_loss -0.8287 
2024-05-08 14:42:34.489237: val_loss -0.8443 
2024-05-08 14:42:34.489287: Pseudo dice [0.9145] 
2024-05-08 14:42:34.489340: Epoch time: 41.08 s 
2024-05-08 14:42:35.624302:  
2024-05-08 14:42:35.624512: Epoch 203 
2024-05-08 14:42:35.624802: Current learning rate: 0.00815 
2024-05-08 14:43:17.213180: train_loss -0.8271 
2024-05-08 14:43:17.213345: val_loss -0.8326 
2024-05-08 14:43:17.213394: Pseudo dice [0.9176] 
2024-05-08 14:43:17.213449: Epoch time: 41.59 s 
2024-05-08 14:43:18.347272:  
2024-05-08 14:43:18.347481: Epoch 204 
2024-05-08 14:43:18.347581: Current learning rate: 0.00814 
2024-05-08 14:43:59.368491: train_loss -0.8216 
2024-05-08 14:43:59.368671: val_loss -0.826 
2024-05-08 14:43:59.368724: Pseudo dice [0.9079] 
2024-05-08 14:43:59.368777: Epoch time: 41.02 s 
2024-05-08 14:44:00.514371:  
2024-05-08 14:44:00.514619: Epoch 205 
2024-05-08 14:44:00.514715: Current learning rate: 0.00813 
2024-05-08 14:44:41.531798: train_loss -0.8235 
2024-05-08 14:44:41.531979: val_loss -0.8375 
2024-05-08 14:44:41.532028: Pseudo dice [0.9133] 
2024-05-08 14:44:41.532081: Epoch time: 41.02 s 
2024-05-08 14:44:42.591655:  
2024-05-08 14:44:42.591850: Epoch 206 
2024-05-08 14:44:42.591939: Current learning rate: 0.00813 
2024-05-08 14:45:23.616265: train_loss -0.8269 
2024-05-08 14:45:23.616438: val_loss -0.8115 
2024-05-08 14:45:23.616486: Pseudo dice [0.8987] 
2024-05-08 14:45:23.616538: Epoch time: 41.03 s 
2024-05-08 14:45:24.867143:  
2024-05-08 14:45:24.867290: Epoch 207 
2024-05-08 14:45:24.867383: Current learning rate: 0.00812 
2024-05-08 14:46:05.893332: train_loss -0.8323 
2024-05-08 14:46:05.893506: val_loss -0.8425 
2024-05-08 14:46:05.893554: Pseudo dice [0.9138] 
2024-05-08 14:46:05.893607: Epoch time: 41.03 s 
2024-05-08 14:46:06.956639:  
2024-05-08 14:46:06.956783: Epoch 208 
2024-05-08 14:46:06.956873: Current learning rate: 0.00811 
2024-05-08 14:46:48.377060: train_loss -0.8278 
2024-05-08 14:46:48.377369: val_loss -0.8392 
2024-05-08 14:46:48.377452: Pseudo dice [0.9125] 
2024-05-08 14:46:48.377545: Epoch time: 41.42 s 
2024-05-08 14:46:49.533670:  
2024-05-08 14:46:49.533797: Epoch 209 
2024-05-08 14:46:49.533913: Current learning rate: 0.0081 
2024-05-08 14:47:30.562175: train_loss -0.8256 
2024-05-08 14:47:30.562359: val_loss -0.8275 
2024-05-08 14:47:30.562422: Pseudo dice [0.9054] 
2024-05-08 14:47:30.562489: Epoch time: 41.03 s 
2024-05-08 14:47:31.658947:  
2024-05-08 14:47:31.659072: Epoch 210 
2024-05-08 14:47:31.659179: Current learning rate: 0.00809 
2024-05-08 14:48:12.683568: train_loss -0.8277 
2024-05-08 14:48:12.683750: val_loss -0.8394 
2024-05-08 14:48:12.683814: Pseudo dice [0.9099] 
2024-05-08 14:48:12.683880: Epoch time: 41.03 s 
2024-05-08 14:48:13.752193:  
2024-05-08 14:48:13.752325: Epoch 211 
2024-05-08 14:48:13.752431: Current learning rate: 0.00808 
2024-05-08 14:48:54.794350: train_loss -0.8367 
2024-05-08 14:48:54.794547: val_loss -0.842 
2024-05-08 14:48:54.794609: Pseudo dice [0.9171] 
2024-05-08 14:48:54.794676: Epoch time: 41.04 s 
2024-05-08 14:48:54.794730: Yayy! New best EMA pseudo Dice: 0.9101 
2024-05-08 14:48:56.206164:  
2024-05-08 14:48:56.206326: Epoch 212 
2024-05-08 14:48:56.206454: Current learning rate: 0.00807 
2024-05-08 14:49:37.662203: train_loss -0.8328 
2024-05-08 14:49:37.662390: val_loss -0.8504 
2024-05-08 14:49:37.662455: Pseudo dice [0.9178] 
2024-05-08 14:49:37.662523: Epoch time: 41.46 s 
2024-05-08 14:49:37.662579: Yayy! New best EMA pseudo Dice: 0.9109 
2024-05-08 14:49:39.254180:  
2024-05-08 14:49:39.254371: Epoch 213 
2024-05-08 14:49:39.254491: Current learning rate: 0.00806 
2024-05-08 14:50:20.696643: train_loss -0.8391 
2024-05-08 14:50:20.696819: val_loss -0.8276 
2024-05-08 14:50:20.696869: Pseudo dice [0.9067] 
2024-05-08 14:50:20.696921: Epoch time: 41.44 s 
2024-05-08 14:50:21.770915:  
2024-05-08 14:50:21.771042: Epoch 214 
2024-05-08 14:50:21.771142: Current learning rate: 0.00805 
2024-05-08 14:51:02.860444: train_loss -0.8356 
2024-05-08 14:51:02.860663: val_loss -0.8393 
2024-05-08 14:51:02.860714: Pseudo dice [0.9161] 
2024-05-08 14:51:02.860767: Epoch time: 41.09 s 
2024-05-08 14:51:02.860810: Yayy! New best EMA pseudo Dice: 0.911 
2024-05-08 14:51:04.267733:  
2024-05-08 14:51:04.267930: Epoch 215 
2024-05-08 14:51:04.268085: Current learning rate: 0.00804 
2024-05-08 14:51:45.397855: train_loss -0.83 
2024-05-08 14:51:45.398038: val_loss -0.8401 
2024-05-08 14:51:45.398103: Pseudo dice [0.9129] 
2024-05-08 14:51:45.398171: Epoch time: 41.13 s 
2024-05-08 14:51:45.398229: Yayy! New best EMA pseudo Dice: 0.9112 
2024-05-08 14:51:46.809916:  
2024-05-08 14:51:46.810086: Epoch 216 
2024-05-08 14:51:46.810208: Current learning rate: 0.00803 
2024-05-08 14:52:27.906409: train_loss -0.8428 
2024-05-08 14:52:27.906591: val_loss -0.8411 
2024-05-08 14:52:27.906654: Pseudo dice [0.9152] 
2024-05-08 14:52:27.906722: Epoch time: 41.1 s 
2024-05-08 14:52:27.906778: Yayy! New best EMA pseudo Dice: 0.9116 
2024-05-08 14:52:29.359467:  
2024-05-08 14:52:29.359670: Epoch 217 
2024-05-08 14:52:29.359814: Current learning rate: 0.00802 
2024-05-08 14:53:10.450953: train_loss -0.8292 
2024-05-08 14:53:10.451138: val_loss -0.845 
2024-05-08 14:53:10.451200: Pseudo dice [0.9125] 
2024-05-08 14:53:10.451283: Epoch time: 41.09 s 
2024-05-08 14:53:10.451344: Yayy! New best EMA pseudo Dice: 0.9117 
2024-05-08 14:53:11.868388:  
2024-05-08 14:53:11.868509: Epoch 218 
2024-05-08 14:53:11.868595: Current learning rate: 0.00801 
2024-05-08 14:53:52.955609: train_loss -0.8265 
2024-05-08 14:53:52.955789: val_loss -0.84 
2024-05-08 14:53:52.955838: Pseudo dice [0.912] 
2024-05-08 14:53:52.955891: Epoch time: 41.09 s 
2024-05-08 14:53:52.955935: Yayy! New best EMA pseudo Dice: 0.9117 
2024-05-08 14:53:54.541057:  
2024-05-08 14:53:54.541202: Epoch 219 
2024-05-08 14:53:54.541309: Current learning rate: 0.00801 
2024-05-08 14:54:35.640600: train_loss -0.829 
2024-05-08 14:54:35.640778: val_loss -0.8323 
2024-05-08 14:54:35.640830: Pseudo dice [0.9084] 
2024-05-08 14:54:35.640883: Epoch time: 41.1 s 
2024-05-08 14:54:36.703858:  
2024-05-08 14:54:36.703994: Epoch 220 
2024-05-08 14:54:36.704080: Current learning rate: 0.008 
2024-05-08 14:55:17.809925: train_loss -0.8403 
2024-05-08 14:55:17.810099: val_loss -0.8435 
2024-05-08 14:55:17.810148: Pseudo dice [0.9149] 
2024-05-08 14:55:17.810202: Epoch time: 41.11 s 
2024-05-08 14:55:17.810246: Yayy! New best EMA pseudo Dice: 0.9117 
2024-05-08 14:55:19.216610:  
2024-05-08 14:55:19.216748: Epoch 221 
2024-05-08 14:55:19.216838: Current learning rate: 0.00799 
2024-05-08 14:56:00.317667: train_loss -0.8325 
2024-05-08 14:56:00.317837: val_loss -0.8425 
2024-05-08 14:56:00.317885: Pseudo dice [0.9134] 
2024-05-08 14:56:00.317936: Epoch time: 41.1 s 
2024-05-08 14:56:00.317979: Yayy! New best EMA pseudo Dice: 0.9119 
2024-05-08 14:56:01.721725:  
2024-05-08 14:56:01.721936: Epoch 222 
2024-05-08 14:56:01.722030: Current learning rate: 0.00798 
2024-05-08 14:56:42.819868: train_loss -0.8386 
2024-05-08 14:56:42.820048: val_loss -0.8496 
2024-05-08 14:56:42.820097: Pseudo dice [0.9137] 
2024-05-08 14:56:42.820151: Epoch time: 41.1 s 
2024-05-08 14:56:42.820193: Yayy! New best EMA pseudo Dice: 0.9121 
2024-05-08 14:56:44.209212:  
2024-05-08 14:56:44.209348: Epoch 223 
2024-05-08 14:56:44.209438: Current learning rate: 0.00797 
2024-05-08 14:57:25.283494: train_loss -0.8311 
2024-05-08 14:57:25.283674: val_loss -0.8452 
2024-05-08 14:57:25.283723: Pseudo dice [0.9151] 
2024-05-08 14:57:25.283777: Epoch time: 41.08 s 
2024-05-08 14:57:25.283819: Yayy! New best EMA pseudo Dice: 0.9124 
2024-05-08 14:57:26.690913:  
2024-05-08 14:57:26.691095: Epoch 224 
2024-05-08 14:57:26.691214: Current learning rate: 0.00796 
2024-05-08 14:58:07.744601: train_loss -0.8394 
2024-05-08 14:58:07.744771: val_loss -0.8371 
2024-05-08 14:58:07.744821: Pseudo dice [0.9148] 
2024-05-08 14:58:07.744875: Epoch time: 41.05 s 
2024-05-08 14:58:07.744918: Yayy! New best EMA pseudo Dice: 0.9126 
2024-05-08 14:58:09.147756:  
2024-05-08 14:58:09.147889: Epoch 225 
2024-05-08 14:58:09.147980: Current learning rate: 0.00795 
2024-05-08 14:58:50.640798: train_loss -0.8369 
2024-05-08 14:58:50.640976: val_loss -0.828 
2024-05-08 14:58:50.641025: Pseudo dice [0.9047] 
2024-05-08 14:58:50.641078: Epoch time: 41.49 s 
2024-05-08 14:58:51.887595:  
2024-05-08 14:58:51.887921: Epoch 226 
2024-05-08 14:58:51.888051: Current learning rate: 0.00794 
2024-05-08 14:59:33.032480: train_loss -0.8322 
2024-05-08 14:59:33.032662: val_loss -0.837 
2024-05-08 14:59:33.032715: Pseudo dice [0.9137] 
2024-05-08 14:59:33.032768: Epoch time: 41.15 s 
2024-05-08 14:59:34.112073:  
2024-05-08 14:59:34.112201: Epoch 227 
2024-05-08 14:59:34.112288: Current learning rate: 0.00793 
2024-05-08 15:00:15.204000: train_loss -0.8289 
2024-05-08 15:00:15.204181: val_loss -0.8354 
2024-05-08 15:00:15.204230: Pseudo dice [0.9123] 
2024-05-08 15:00:15.204283: Epoch time: 41.09 s 
2024-05-08 15:00:16.258742:  
2024-05-08 15:00:16.258876: Epoch 228 
2024-05-08 15:00:16.258976: Current learning rate: 0.00792 
2024-05-08 15:00:57.758417: train_loss -0.836 
2024-05-08 15:00:57.758582: val_loss -0.8408 
2024-05-08 15:00:57.758632: Pseudo dice [0.9159] 
2024-05-08 15:00:57.758686: Epoch time: 41.5 s 
2024-05-08 15:00:58.810041:  
2024-05-08 15:00:58.810231: Epoch 229 
2024-05-08 15:00:58.810329: Current learning rate: 0.00791 
2024-05-08 15:01:39.870664: train_loss -0.837 
2024-05-08 15:01:39.870836: val_loss -0.8456 
2024-05-08 15:01:39.870884: Pseudo dice [0.9172] 
2024-05-08 15:01:39.870940: Epoch time: 41.06 s 
2024-05-08 15:01:39.870983: Yayy! New best EMA pseudo Dice: 0.9129 
2024-05-08 15:01:41.267329:  
2024-05-08 15:01:41.267455: Epoch 230 
2024-05-08 15:01:41.267544: Current learning rate: 0.0079 
2024-05-08 15:02:22.359849: train_loss -0.834 
2024-05-08 15:02:22.360048: val_loss -0.8395 
2024-05-08 15:02:22.360099: Pseudo dice [0.9107] 
2024-05-08 15:02:22.360155: Epoch time: 41.09 s 
2024-05-08 15:02:23.410413:  
2024-05-08 15:02:23.410538: Epoch 231 
2024-05-08 15:02:23.410631: Current learning rate: 0.00789 
2024-05-08 15:03:04.482841: train_loss -0.8271 
2024-05-08 15:03:04.483005: val_loss -0.8324 
2024-05-08 15:03:04.483055: Pseudo dice [0.9068] 
2024-05-08 15:03:04.483108: Epoch time: 41.07 s 
2024-05-08 15:03:05.720274:  
2024-05-08 15:03:05.720599: Epoch 232 
2024-05-08 15:03:05.720708: Current learning rate: 0.00789 
2024-05-08 15:03:46.801729: train_loss -0.8292 
2024-05-08 15:03:46.801899: val_loss -0.8207 
2024-05-08 15:03:46.801949: Pseudo dice [0.9022] 
2024-05-08 15:03:46.802004: Epoch time: 41.08 s 
2024-05-08 15:03:47.857277:  
2024-05-08 15:03:47.857571: Epoch 233 
2024-05-08 15:03:47.857663: Current learning rate: 0.00788 
2024-05-08 15:04:28.938600: train_loss -0.8333 
2024-05-08 15:04:28.938774: val_loss -0.828 
2024-05-08 15:04:28.938824: Pseudo dice [0.9066] 
2024-05-08 15:04:28.938881: Epoch time: 41.08 s 
2024-05-08 15:04:29.995313:  
2024-05-08 15:04:29.996165: Epoch 234 
2024-05-08 15:04:29.996261: Current learning rate: 0.00787 
2024-05-08 15:05:11.072764: train_loss -0.8362 
2024-05-08 15:05:11.072970: val_loss -0.861 
2024-05-08 15:05:11.073022: Pseudo dice [0.923] 
2024-05-08 15:05:11.073081: Epoch time: 41.08 s 
2024-05-08 15:05:12.132627:  
2024-05-08 15:05:12.132919: Epoch 235 
2024-05-08 15:05:12.133021: Current learning rate: 0.00786 
2024-05-08 15:05:53.212775: train_loss -0.8382 
2024-05-08 15:05:53.212942: val_loss -0.8268 
2024-05-08 15:05:53.212990: Pseudo dice [0.906] 
2024-05-08 15:05:53.213046: Epoch time: 41.08 s 
2024-05-08 15:05:54.271868:  
2024-05-08 15:05:54.272119: Epoch 236 
2024-05-08 15:05:54.272217: Current learning rate: 0.00785 
2024-05-08 15:06:35.351005: train_loss -0.8394 
2024-05-08 15:06:35.351172: val_loss -0.8514 
2024-05-08 15:06:35.351221: Pseudo dice [0.9225] 
2024-05-08 15:06:35.351275: Epoch time: 41.08 s 
2024-05-08 15:06:36.413328:  
2024-05-08 15:06:36.413538: Epoch 237 
2024-05-08 15:06:36.413631: Current learning rate: 0.00784 
2024-05-08 15:07:17.506116: train_loss -0.8329 
2024-05-08 15:07:17.506309: val_loss -0.8363 
2024-05-08 15:07:17.506357: Pseudo dice [0.9142] 
2024-05-08 15:07:17.506415: Epoch time: 41.09 s 
2024-05-08 15:07:18.561265:  
2024-05-08 15:07:18.561434: Epoch 238 
2024-05-08 15:07:18.561609: Current learning rate: 0.00783 
2024-05-08 15:07:59.705092: train_loss -0.8311 
2024-05-08 15:07:59.705272: val_loss -0.8389 
2024-05-08 15:07:59.705322: Pseudo dice [0.9134] 
2024-05-08 15:07:59.705380: Epoch time: 41.14 s 
2024-05-08 15:08:00.960655:  
2024-05-08 15:08:00.960788: Epoch 239 
2024-05-08 15:08:00.960892: Current learning rate: 0.00782 
2024-05-08 15:08:42.678466: train_loss -0.8363 
2024-05-08 15:08:42.678658: val_loss -0.84 
2024-05-08 15:08:42.678706: Pseudo dice [0.9115] 
2024-05-08 15:08:42.678761: Epoch time: 41.72 s 
2024-05-08 15:08:43.748806:  
2024-05-08 15:08:43.748971: Epoch 240 
2024-05-08 15:08:43.749069: Current learning rate: 0.00781 
2024-05-08 15:09:24.824392: train_loss -0.8321 
2024-05-08 15:09:24.824570: val_loss -0.8341 
2024-05-08 15:09:24.824618: Pseudo dice [0.9117] 
2024-05-08 15:09:24.824677: Epoch time: 41.08 s 
2024-05-08 15:09:25.953652:  
2024-05-08 15:09:25.953796: Epoch 241 
2024-05-08 15:09:25.953886: Current learning rate: 0.0078 
2024-05-08 15:10:07.029931: train_loss -0.8356 
2024-05-08 15:10:07.030106: val_loss -0.8508 
2024-05-08 15:10:07.030168: Pseudo dice [0.9175] 
2024-05-08 15:10:07.030221: Epoch time: 41.08 s 
2024-05-08 15:10:07.030263: Yayy! New best EMA pseudo Dice: 0.913 
2024-05-08 15:10:08.446151:  
2024-05-08 15:10:08.446375: Epoch 242 
2024-05-08 15:10:08.446465: Current learning rate: 0.00779 
2024-05-08 15:10:49.512302: train_loss -0.8374 
2024-05-08 15:10:49.512477: val_loss -0.8555 
2024-05-08 15:10:49.512526: Pseudo dice [0.9195] 
2024-05-08 15:10:49.512578: Epoch time: 41.07 s 
2024-05-08 15:10:49.512622: Yayy! New best EMA pseudo Dice: 0.9136 
2024-05-08 15:10:50.930034:  
2024-05-08 15:10:50.930210: Epoch 243 
2024-05-08 15:10:50.930304: Current learning rate: 0.00778 
2024-05-08 15:11:32.007595: train_loss -0.8346 
2024-05-08 15:11:32.007769: val_loss -0.844 
2024-05-08 15:11:32.007819: Pseudo dice [0.9113] 
2024-05-08 15:11:32.007876: Epoch time: 41.08 s 
2024-05-08 15:11:33.078949:  
2024-05-08 15:11:33.079126: Epoch 244 
2024-05-08 15:11:33.079220: Current learning rate: 0.00777 
2024-05-08 15:12:14.149228: train_loss -0.8341 
2024-05-08 15:12:14.149395: val_loss -0.8485 
2024-05-08 15:12:14.149570: Pseudo dice [0.9162] 
2024-05-08 15:12:14.149621: Epoch time: 41.07 s 
2024-05-08 15:12:14.149664: Yayy! New best EMA pseudo Dice: 0.9137 
2024-05-08 15:12:15.793314:  
2024-05-08 15:12:15.793567: Epoch 245 
2024-05-08 15:12:15.793683: Current learning rate: 0.00777 
2024-05-08 15:12:57.422785: train_loss -0.8379 
2024-05-08 15:12:57.422993: val_loss -0.8464 
2024-05-08 15:12:57.423045: Pseudo dice [0.9137] 
2024-05-08 15:12:57.423100: Epoch time: 41.63 s 
2024-05-08 15:12:57.423143: Yayy! New best EMA pseudo Dice: 0.9137 
2024-05-08 15:12:58.848388:  
2024-05-08 15:12:58.848526: Epoch 246 
2024-05-08 15:12:58.848615: Current learning rate: 0.00776 
2024-05-08 15:13:40.333206: train_loss -0.8303 
2024-05-08 15:13:40.333415: val_loss -0.819 
2024-05-08 15:13:40.333466: Pseudo dice [0.9096] 
2024-05-08 15:13:40.333524: Epoch time: 41.49 s 
2024-05-08 15:13:41.555484:  
2024-05-08 15:13:41.555706: Epoch 247 
2024-05-08 15:13:41.555808: Current learning rate: 0.00775 
2024-05-08 15:14:22.825640: train_loss -0.8297 
2024-05-08 15:14:22.825823: val_loss -0.8504 
2024-05-08 15:14:22.825881: Pseudo dice [0.9147] 
2024-05-08 15:14:22.825933: Epoch time: 41.27 s 
2024-05-08 15:14:23.903504:  
2024-05-08 15:14:23.903751: Epoch 248 
2024-05-08 15:14:23.903844: Current learning rate: 0.00774 
2024-05-08 15:15:05.068923: train_loss -0.8311 
2024-05-08 15:15:05.069113: val_loss -0.8351 
2024-05-08 15:15:05.069177: Pseudo dice [0.913] 
2024-05-08 15:15:05.069244: Epoch time: 41.17 s 
2024-05-08 15:15:06.142928:  
2024-05-08 15:15:06.143058: Epoch 249 
2024-05-08 15:15:06.143165: Current learning rate: 0.00773 
2024-05-08 15:15:47.402962: train_loss -0.83 
2024-05-08 15:15:47.403138: val_loss -0.8514 
2024-05-08 15:15:47.403190: Pseudo dice [0.9169] 
2024-05-08 15:15:47.403244: Epoch time: 41.26 s 
2024-05-08 15:15:47.751470: Yayy! New best EMA pseudo Dice: 0.9137 
2024-05-08 15:15:49.183984:  
2024-05-08 15:15:49.184128: Epoch 250 
2024-05-08 15:15:49.184219: Current learning rate: 0.00772 
2024-05-08 15:16:30.541571: train_loss -0.8285 
2024-05-08 15:16:30.541748: val_loss -0.8415 
2024-05-08 15:16:30.541798: Pseudo dice [0.9156] 
2024-05-08 15:16:30.541850: Epoch time: 41.36 s 
2024-05-08 15:16:30.541893: Yayy! New best EMA pseudo Dice: 0.9139 
2024-05-08 15:16:31.961139:  
2024-05-08 15:16:31.961317: Epoch 251 
2024-05-08 15:16:31.961415: Current learning rate: 0.00771 
2024-05-08 15:17:13.447858: train_loss -0.8337 
2024-05-08 15:17:13.448385: val_loss -0.8411 
2024-05-08 15:17:13.448642: Pseudo dice [0.9176] 
2024-05-08 15:17:13.448916: Epoch time: 41.49 s 
2024-05-08 15:17:13.449161: Yayy! New best EMA pseudo Dice: 0.9143 
2024-05-08 15:17:15.384879:  
2024-05-08 15:17:15.385026: Epoch 252 
2024-05-08 15:17:15.385123: Current learning rate: 0.0077 
2024-05-08 15:17:56.589736: train_loss -0.8365 
2024-05-08 15:17:56.589927: val_loss -0.8381 
2024-05-08 15:17:56.589976: Pseudo dice [0.9149] 
2024-05-08 15:17:56.590041: Epoch time: 41.21 s 
2024-05-08 15:17:56.590084: Yayy! New best EMA pseudo Dice: 0.9143 
2024-05-08 15:17:58.007775:  
2024-05-08 15:17:58.007949: Epoch 253 
2024-05-08 15:17:58.008160: Current learning rate: 0.00769 
2024-05-08 15:18:39.783290: train_loss -0.8343 
2024-05-08 15:18:39.783470: val_loss -0.8485 
2024-05-08 15:18:39.783519: Pseudo dice [0.9189] 
2024-05-08 15:18:39.783572: Epoch time: 41.78 s 
2024-05-08 15:18:39.783614: Yayy! New best EMA pseudo Dice: 0.9148 
2024-05-08 15:18:41.222192:  
2024-05-08 15:18:41.222507: Epoch 254 
2024-05-08 15:18:41.222600: Current learning rate: 0.00768 
2024-05-08 15:19:22.341317: train_loss -0.8333 
2024-05-08 15:19:22.341498: val_loss -0.842 
2024-05-08 15:19:22.341549: Pseudo dice [0.9118] 
2024-05-08 15:19:22.341619: Epoch time: 41.12 s 
2024-05-08 15:19:23.422050:  
2024-05-08 15:19:23.422181: Epoch 255 
2024-05-08 15:19:23.422270: Current learning rate: 0.00767 
2024-05-08 15:20:04.485089: train_loss -0.8295 
2024-05-08 15:20:04.485271: val_loss -0.8405 
2024-05-08 15:20:04.485320: Pseudo dice [0.9123] 
2024-05-08 15:20:04.485374: Epoch time: 41.06 s 
2024-05-08 15:20:05.562504:  
2024-05-08 15:20:05.562634: Epoch 256 
2024-05-08 15:20:05.562729: Current learning rate: 0.00766 
2024-05-08 15:20:46.598001: train_loss -0.8257 
2024-05-08 15:20:46.598184: val_loss -0.8291 
2024-05-08 15:20:46.598233: Pseudo dice [0.9108] 
2024-05-08 15:20:46.598287: Epoch time: 41.04 s 
2024-05-08 15:20:47.671024:  
2024-05-08 15:20:47.671203: Epoch 257 
2024-05-08 15:20:47.671384: Current learning rate: 0.00765 
2024-05-08 15:21:28.719821: train_loss -0.8207 
2024-05-08 15:21:28.720057: val_loss -0.8389 
2024-05-08 15:21:28.720109: Pseudo dice [0.9093] 
2024-05-08 15:21:28.720163: Epoch time: 41.05 s 
2024-05-08 15:21:29.978209:  
2024-05-08 15:21:29.978410: Epoch 258 
2024-05-08 15:21:29.978507: Current learning rate: 0.00764 
2024-05-08 15:22:11.036077: train_loss -0.823 
2024-05-08 15:22:11.036248: val_loss -0.8314 
2024-05-08 15:22:11.036297: Pseudo dice [0.9079] 
2024-05-08 15:22:11.036349: Epoch time: 41.06 s 
2024-05-08 15:22:12.106249:  
2024-05-08 15:22:12.106481: Epoch 259 
2024-05-08 15:22:12.106610: Current learning rate: 0.00764 
2024-05-08 15:22:53.210344: train_loss -0.8309 
2024-05-08 15:22:53.210523: val_loss -0.8326 
2024-05-08 15:22:53.210572: Pseudo dice [0.9108] 
2024-05-08 15:22:53.210625: Epoch time: 41.11 s 
2024-05-08 15:22:54.325297:  
2024-05-08 15:22:54.325437: Epoch 260 
2024-05-08 15:22:54.325530: Current learning rate: 0.00763 
2024-05-08 15:23:35.385751: train_loss -0.8387 
2024-05-08 15:23:35.385953: val_loss -0.8396 
2024-05-08 15:23:35.386003: Pseudo dice [0.9139] 
2024-05-08 15:23:35.386057: Epoch time: 41.06 s 
2024-05-08 15:23:36.458713:  
2024-05-08 15:23:36.458845: Epoch 261 
2024-05-08 15:23:36.458933: Current learning rate: 0.00762 
2024-05-08 15:24:17.515030: train_loss -0.845 
2024-05-08 15:24:17.515208: val_loss -0.8409 
2024-05-08 15:24:17.515256: Pseudo dice [0.9129] 
2024-05-08 15:24:17.515308: Epoch time: 41.06 s 
2024-05-08 15:24:18.595544:  
2024-05-08 15:24:18.595912: Epoch 262 
2024-05-08 15:24:18.596009: Current learning rate: 0.00761 
2024-05-08 15:24:59.655533: train_loss -0.8423 
2024-05-08 15:24:59.655715: val_loss -0.8455 
2024-05-08 15:24:59.655763: Pseudo dice [0.9172] 
2024-05-08 15:24:59.655815: Epoch time: 41.06 s 
2024-05-08 15:25:00.738267:  
2024-05-08 15:25:00.738450: Epoch 263 
2024-05-08 15:25:00.738568: Current learning rate: 0.0076 
2024-05-08 15:25:41.815496: train_loss -0.8398 
2024-05-08 15:25:41.815663: val_loss -0.8472 
2024-05-08 15:25:41.815711: Pseudo dice [0.9165] 
2024-05-08 15:25:41.815763: Epoch time: 41.08 s 
2024-05-08 15:25:43.077337:  
2024-05-08 15:25:43.077547: Epoch 264 
2024-05-08 15:25:43.077641: Current learning rate: 0.00759 
2024-05-08 15:26:24.148572: train_loss -0.8414 
2024-05-08 15:26:24.148756: val_loss -0.8423 
2024-05-08 15:26:24.148808: Pseudo dice [0.9102] 
2024-05-08 15:26:24.148861: Epoch time: 41.07 s 
2024-05-08 15:26:25.228522:  
2024-05-08 15:26:25.228655: Epoch 265 
2024-05-08 15:26:25.228755: Current learning rate: 0.00758 
2024-05-08 15:27:06.299073: train_loss -0.8374 
2024-05-08 15:27:06.299250: val_loss -0.8495 
2024-05-08 15:27:06.299299: Pseudo dice [0.9169] 
2024-05-08 15:27:06.299352: Epoch time: 41.07 s 
2024-05-08 15:27:07.380551:  
2024-05-08 15:27:07.380693: Epoch 266 
2024-05-08 15:27:07.380786: Current learning rate: 0.00757 
2024-05-08 15:27:48.820955: train_loss -0.8354 
2024-05-08 15:27:48.821130: val_loss -0.8497 
2024-05-08 15:27:48.821181: Pseudo dice [0.9188] 
2024-05-08 15:27:48.821234: Epoch time: 41.44 s 
2024-05-08 15:27:49.911560:  
2024-05-08 15:27:49.911792: Epoch 267 
2024-05-08 15:27:49.911925: Current learning rate: 0.00756 
2024-05-08 15:28:31.294146: train_loss -0.8334 
2024-05-08 15:28:31.294327: val_loss -0.8454 
2024-05-08 15:28:31.294376: Pseudo dice [0.9152] 
2024-05-08 15:28:31.294428: Epoch time: 41.38 s 
2024-05-08 15:28:32.374806:  
2024-05-08 15:28:32.374938: Epoch 268 
2024-05-08 15:28:32.375032: Current learning rate: 0.00755 
2024-05-08 15:29:13.505218: train_loss -0.8391 
2024-05-08 15:29:13.505378: val_loss -0.8401 
2024-05-08 15:29:13.505426: Pseudo dice [0.9128] 
2024-05-08 15:29:13.505480: Epoch time: 41.13 s 
2024-05-08 15:29:14.593352:  
2024-05-08 15:29:14.593573: Epoch 269 
2024-05-08 15:29:14.593671: Current learning rate: 0.00754 
2024-05-08 15:29:56.294429: train_loss -0.8434 
2024-05-08 15:29:56.294618: val_loss -0.837 
2024-05-08 15:29:56.294667: Pseudo dice [0.9125] 
2024-05-08 15:29:56.294724: Epoch time: 41.7 s 
2024-05-08 15:29:57.374743:  
2024-05-08 15:29:57.375064: Epoch 270 
2024-05-08 15:29:57.375242: Current learning rate: 0.00753 
2024-05-08 15:30:38.500712: train_loss -0.8354 
2024-05-08 15:30:38.500874: val_loss -0.8427 
2024-05-08 15:30:38.500922: Pseudo dice [0.9162] 
2024-05-08 15:30:38.500976: Epoch time: 41.13 s 
2024-05-08 15:30:39.768413:  
2024-05-08 15:30:39.768620: Epoch 271 
2024-05-08 15:30:39.768731: Current learning rate: 0.00752 
2024-05-08 15:31:21.532679: train_loss -0.8439 
2024-05-08 15:31:21.532903: val_loss -0.8357 
2024-05-08 15:31:21.532954: Pseudo dice [0.9069] 
2024-05-08 15:31:21.533008: Epoch time: 41.77 s 
2024-05-08 15:31:22.609093:  
2024-05-08 15:31:22.609237: Epoch 272 
2024-05-08 15:31:22.609324: Current learning rate: 0.00751 
2024-05-08 15:32:03.708662: train_loss -0.8357 
2024-05-08 15:32:03.708843: val_loss -0.8552 
2024-05-08 15:32:03.708893: Pseudo dice [0.9213] 
2024-05-08 15:32:03.708946: Epoch time: 41.1 s 
2024-05-08 15:32:04.811862:  
2024-05-08 15:32:04.812002: Epoch 273 
2024-05-08 15:32:04.812091: Current learning rate: 0.00751 
2024-05-08 15:32:46.474328: train_loss -0.8365 
2024-05-08 15:32:46.474509: val_loss -0.832 
2024-05-08 15:32:46.474563: Pseudo dice [0.9071] 
2024-05-08 15:32:46.474622: Epoch time: 41.66 s 
2024-05-08 15:32:47.550098:  
2024-05-08 15:32:47.550232: Epoch 274 
2024-05-08 15:32:47.550321: Current learning rate: 0.0075 
2024-05-08 15:33:29.314698: train_loss -0.8406 
2024-05-08 15:33:29.314902: val_loss -0.8465 
2024-05-08 15:33:29.314951: Pseudo dice [0.9168] 
2024-05-08 15:33:29.315005: Epoch time: 41.77 s 
2024-05-08 15:33:30.399854:  
2024-05-08 15:33:30.399976: Epoch 275 
2024-05-08 15:33:30.400065: Current learning rate: 0.00749 
2024-05-08 15:34:11.497517: train_loss -0.8471 
2024-05-08 15:34:11.497769: val_loss -0.8483 
2024-05-08 15:34:11.497819: Pseudo dice [0.916] 
2024-05-08 15:34:11.497873: Epoch time: 41.1 s 
2024-05-08 15:34:12.569495:  
2024-05-08 15:34:12.569624: Epoch 276 
2024-05-08 15:34:12.569716: Current learning rate: 0.00748 
2024-05-08 15:34:53.677073: train_loss -0.8469 
2024-05-08 15:34:53.677226: val_loss -0.8475 
2024-05-08 15:34:53.677280: Pseudo dice [0.9156] 
2024-05-08 15:34:53.677332: Epoch time: 41.11 s 
2024-05-08 15:34:54.934747:  
2024-05-08 15:34:54.934902: Epoch 277 
2024-05-08 15:34:54.934995: Current learning rate: 0.00747 
2024-05-08 15:35:36.013119: train_loss -0.8427 
2024-05-08 15:35:36.013305: val_loss -0.8583 
2024-05-08 15:35:36.013354: Pseudo dice [0.9197] 
2024-05-08 15:35:36.013407: Epoch time: 41.08 s 
2024-05-08 15:35:37.080876:  
2024-05-08 15:35:37.081009: Epoch 278 
2024-05-08 15:35:37.081098: Current learning rate: 0.00746 
2024-05-08 15:36:18.893450: train_loss -0.8417 
2024-05-08 15:36:18.893613: val_loss -0.8394 
2024-05-08 15:36:18.893661: Pseudo dice [0.9149] 
2024-05-08 15:36:18.893713: Epoch time: 41.81 s 
2024-05-08 15:36:19.975103:  
2024-05-08 15:36:19.975240: Epoch 279 
2024-05-08 15:36:19.975335: Current learning rate: 0.00745 
2024-05-08 15:37:01.105063: train_loss -0.8392 
2024-05-08 15:37:01.105240: val_loss -0.8531 
2024-05-08 15:37:01.105289: Pseudo dice [0.9192] 
2024-05-08 15:37:01.105342: Epoch time: 41.13 s 
2024-05-08 15:37:01.105385: Yayy! New best EMA pseudo Dice: 0.9152 
2024-05-08 15:37:02.529123:  
2024-05-08 15:37:02.529265: Epoch 280 
2024-05-08 15:37:02.529351: Current learning rate: 0.00744 
2024-05-08 15:37:43.627348: train_loss -0.8193 
2024-05-08 15:37:43.627528: val_loss -0.8243 
2024-05-08 15:37:43.627580: Pseudo dice [0.8974] 
2024-05-08 15:37:43.627632: Epoch time: 41.1 s 
2024-05-08 15:37:44.698446:  
2024-05-08 15:37:44.698643: Epoch 281 
2024-05-08 15:37:44.698739: Current learning rate: 0.00743 
2024-05-08 15:38:25.802461: train_loss -0.831 
2024-05-08 15:38:25.802634: val_loss -0.842 
2024-05-08 15:38:25.802683: Pseudo dice [0.9126] 
2024-05-08 15:38:25.802734: Epoch time: 41.11 s 
2024-05-08 15:38:26.876210:  
2024-05-08 15:38:26.876403: Epoch 282 
2024-05-08 15:38:26.876500: Current learning rate: 0.00742 
2024-05-08 15:39:07.980955: train_loss -0.8375 
2024-05-08 15:39:07.981121: val_loss -0.8497 
2024-05-08 15:39:07.981168: Pseudo dice [0.9169] 
2024-05-08 15:39:07.981220: Epoch time: 41.11 s 
2024-05-08 15:39:09.076340:  
2024-05-08 15:39:09.076462: Epoch 283 
2024-05-08 15:39:09.076555: Current learning rate: 0.00741 
2024-05-08 15:39:50.155136: train_loss -0.8347 
2024-05-08 15:39:50.155314: val_loss -0.8454 
2024-05-08 15:39:50.155363: Pseudo dice [0.9114] 
2024-05-08 15:39:50.155415: Epoch time: 41.08 s 
2024-05-08 15:39:51.411999:  
2024-05-08 15:39:51.412152: Epoch 284 
2024-05-08 15:39:51.412241: Current learning rate: 0.0074 
2024-05-08 15:40:32.467486: train_loss -0.8457 
2024-05-08 15:40:32.467667: val_loss -0.858 
2024-05-08 15:40:32.467716: Pseudo dice [0.9201] 
2024-05-08 15:40:32.467806: Epoch time: 41.06 s 
2024-05-08 15:40:33.542986:  
2024-05-08 15:40:33.543234: Epoch 285 
2024-05-08 15:40:33.543332: Current learning rate: 0.00739 
2024-05-08 15:41:14.618063: train_loss -0.8449 
2024-05-08 15:41:14.618251: val_loss -0.8481 
2024-05-08 15:41:14.618303: Pseudo dice [0.9206] 
2024-05-08 15:41:14.618357: Epoch time: 41.08 s 
2024-05-08 15:41:15.696268:  
2024-05-08 15:41:15.696501: Epoch 286 
2024-05-08 15:41:15.696641: Current learning rate: 0.00738 
2024-05-08 15:41:56.713308: train_loss -0.8432 
2024-05-08 15:41:56.713488: val_loss -0.8479 
2024-05-08 15:41:56.713536: Pseudo dice [0.9188] 
2024-05-08 15:41:56.713588: Epoch time: 41.02 s 
2024-05-08 15:41:57.799423:  
2024-05-08 15:41:57.799654: Epoch 287 
2024-05-08 15:41:57.799782: Current learning rate: 0.00738 
2024-05-08 15:42:38.821283: train_loss -0.8437 
2024-05-08 15:42:38.821474: val_loss -0.8521 
2024-05-08 15:42:38.821523: Pseudo dice [0.9215] 
2024-05-08 15:42:38.821576: Epoch time: 41.02 s 
2024-05-08 15:42:38.821618: Yayy! New best EMA pseudo Dice: 0.9158 
2024-05-08 15:42:40.250925:  
2024-05-08 15:42:40.251064: Epoch 288 
2024-05-08 15:42:40.251151: Current learning rate: 0.00737 
2024-05-08 15:43:21.249982: train_loss -0.8346 
2024-05-08 15:43:21.250162: val_loss -0.8435 
2024-05-08 15:43:21.250210: Pseudo dice [0.9167] 
2024-05-08 15:43:21.250263: Epoch time: 41.0 s 
2024-05-08 15:43:21.250306: Yayy! New best EMA pseudo Dice: 0.9159 
2024-05-08 15:43:22.665594:  
2024-05-08 15:43:22.665725: Epoch 289 
2024-05-08 15:43:22.665816: Current learning rate: 0.00736 
2024-05-08 15:44:03.668502: train_loss -0.8441 
2024-05-08 15:44:03.668702: val_loss -0.8499 
2024-05-08 15:44:03.668756: Pseudo dice [0.9184] 
2024-05-08 15:44:03.668808: Epoch time: 41.0 s 
2024-05-08 15:44:03.668860: Yayy! New best EMA pseudo Dice: 0.9162 
2024-05-08 15:44:05.272935:  
2024-05-08 15:44:05.273087: Epoch 290 
2024-05-08 15:44:05.273176: Current learning rate: 0.00735 
2024-05-08 15:44:46.351079: train_loss -0.8351 
2024-05-08 15:44:46.351279: val_loss -0.8357 
2024-05-08 15:44:46.351329: Pseudo dice [0.9135] 
2024-05-08 15:44:46.351383: Epoch time: 41.08 s 
2024-05-08 15:44:47.432724:  
2024-05-08 15:44:47.432915: Epoch 291 
2024-05-08 15:44:47.433007: Current learning rate: 0.00734 
2024-05-08 15:45:28.499544: train_loss -0.8333 
2024-05-08 15:45:28.499730: val_loss -0.842 
2024-05-08 15:45:28.499779: Pseudo dice [0.9143] 
2024-05-08 15:45:28.499835: Epoch time: 41.07 s 
2024-05-08 15:45:29.604560:  
2024-05-08 15:45:29.604715: Epoch 292 
2024-05-08 15:45:29.604817: Current learning rate: 0.00733 
2024-05-08 15:46:10.691387: train_loss -0.8404 
2024-05-08 15:46:10.691558: val_loss -0.8566 
2024-05-08 15:46:10.691605: Pseudo dice [0.9227] 
2024-05-08 15:46:10.691657: Epoch time: 41.09 s 
2024-05-08 15:46:10.691698: Yayy! New best EMA pseudo Dice: 0.9164 
2024-05-08 15:46:12.106094:  
2024-05-08 15:46:12.106382: Epoch 293 
2024-05-08 15:46:12.106477: Current learning rate: 0.00732 
2024-05-08 15:46:53.180032: train_loss -0.8436 
2024-05-08 15:46:53.180211: val_loss -0.8456 
2024-05-08 15:46:53.180262: Pseudo dice [0.916] 
2024-05-08 15:46:53.180314: Epoch time: 41.07 s 
2024-05-08 15:46:54.251695:  
2024-05-08 15:46:54.251830: Epoch 294 
2024-05-08 15:46:54.251920: Current learning rate: 0.00731 
2024-05-08 15:47:35.311514: train_loss -0.8478 
2024-05-08 15:47:35.311696: val_loss -0.8278 
2024-05-08 15:47:35.311744: Pseudo dice [0.9123] 
2024-05-08 15:47:35.311800: Epoch time: 41.06 s 
2024-05-08 15:47:36.391084:  
2024-05-08 15:47:36.391215: Epoch 295 
2024-05-08 15:47:36.391305: Current learning rate: 0.0073 
2024-05-08 15:48:17.457920: train_loss -0.8479 
2024-05-08 15:48:17.458090: val_loss -0.8501 
2024-05-08 15:48:17.458138: Pseudo dice [0.9205] 
2024-05-08 15:48:17.458190: Epoch time: 41.07 s 
2024-05-08 15:48:17.458232: Yayy! New best EMA pseudo Dice: 0.9164 
2024-05-08 15:48:19.061978:  
2024-05-08 15:48:19.062122: Epoch 296 
2024-05-08 15:48:19.062209: Current learning rate: 0.00729 
2024-05-08 15:49:00.134388: train_loss -0.8498 
2024-05-08 15:49:00.134562: val_loss -0.8504 
2024-05-08 15:49:00.134609: Pseudo dice [0.9208] 
2024-05-08 15:49:00.134661: Epoch time: 41.07 s 
2024-05-08 15:49:00.134704: Yayy! New best EMA pseudo Dice: 0.9169 
2024-05-08 15:49:01.548627:  
2024-05-08 15:49:01.548982: Epoch 297 
2024-05-08 15:49:01.549112: Current learning rate: 0.00728 
2024-05-08 15:49:42.625214: train_loss -0.839 
2024-05-08 15:49:42.625390: val_loss -0.8635 
2024-05-08 15:49:42.625440: Pseudo dice [0.9201] 
2024-05-08 15:49:42.625493: Epoch time: 41.08 s 
2024-05-08 15:49:42.625534: Yayy! New best EMA pseudo Dice: 0.9172 
2024-05-08 15:49:44.042656:  
2024-05-08 15:49:44.042819: Epoch 298 
2024-05-08 15:49:44.042910: Current learning rate: 0.00727 
2024-05-08 15:50:25.073147: train_loss -0.8367 
2024-05-08 15:50:25.073324: val_loss -0.845 
2024-05-08 15:50:25.073372: Pseudo dice [0.9172] 
2024-05-08 15:50:25.073426: Epoch time: 41.03 s 
2024-05-08 15:50:25.073468: Yayy! New best EMA pseudo Dice: 0.9172 
2024-05-08 15:50:26.494216:  
2024-05-08 15:50:26.494359: Epoch 299 
2024-05-08 15:50:26.494453: Current learning rate: 0.00726 
2024-05-08 15:51:07.569299: train_loss -0.8424 
2024-05-08 15:51:07.569468: val_loss -0.8474 
2024-05-08 15:51:07.569517: Pseudo dice [0.9209] 
2024-05-08 15:51:07.569570: Epoch time: 41.08 s 
2024-05-08 15:51:07.920882: Yayy! New best EMA pseudo Dice: 0.9176 
2024-05-08 15:51:09.327136:  
2024-05-08 15:51:09.327265: Epoch 300 
2024-05-08 15:51:09.327359: Current learning rate: 0.00725 
2024-05-08 15:51:50.338925: train_loss -0.8388 
2024-05-08 15:51:50.339100: val_loss -0.8438 
2024-05-08 15:51:50.339148: Pseudo dice [0.9145] 
2024-05-08 15:51:50.339199: Epoch time: 41.01 s 
2024-05-08 15:51:51.581998:  
2024-05-08 15:51:51.582157: Epoch 301 
2024-05-08 15:51:51.582248: Current learning rate: 0.00724 
2024-05-08 15:52:32.608477: train_loss -0.8454 
2024-05-08 15:52:32.608640: val_loss -0.8417 
2024-05-08 15:52:32.608698: Pseudo dice [0.9111] 
2024-05-08 15:52:32.608751: Epoch time: 41.03 s 
2024-05-08 15:52:33.699901:  
2024-05-08 15:52:33.700039: Epoch 302 
2024-05-08 15:52:33.700129: Current learning rate: 0.00724 
2024-05-08 15:53:14.743155: train_loss -0.8469 
2024-05-08 15:53:14.743335: val_loss -0.8489 
2024-05-08 15:53:14.743384: Pseudo dice [0.9179] 
2024-05-08 15:53:14.743436: Epoch time: 41.04 s 
2024-05-08 15:53:15.827555:  
2024-05-08 15:53:15.827698: Epoch 303 
2024-05-08 15:53:15.827787: Current learning rate: 0.00723 
2024-05-08 15:53:56.872459: train_loss -0.8383 
2024-05-08 15:53:56.872638: val_loss -0.8516 
2024-05-08 15:53:56.872718: Pseudo dice [0.9225] 
2024-05-08 15:53:56.872770: Epoch time: 41.05 s 
2024-05-08 15:53:57.960816:  
2024-05-08 15:53:57.961137: Epoch 304 
2024-05-08 15:53:57.961228: Current learning rate: 0.00722 
2024-05-08 15:54:38.988154: train_loss -0.8384 
2024-05-08 15:54:38.988322: val_loss -0.8491 
2024-05-08 15:54:38.988371: Pseudo dice [0.9188] 
2024-05-08 15:54:38.988424: Epoch time: 41.03 s 
2024-05-08 15:54:40.081224:  
2024-05-08 15:54:40.081350: Epoch 305 
2024-05-08 15:54:40.081437: Current learning rate: 0.00721 
2024-05-08 15:55:21.103313: train_loss -0.8392 
2024-05-08 15:55:21.103593: val_loss -0.8371 
2024-05-08 15:55:21.103644: Pseudo dice [0.9128] 
2024-05-08 15:55:21.103697: Epoch time: 41.02 s 
2024-05-08 15:55:22.191743:  
2024-05-08 15:55:22.191869: Epoch 306 
2024-05-08 15:55:22.191957: Current learning rate: 0.0072 
2024-05-08 15:56:03.238394: train_loss -0.8325 
2024-05-08 15:56:03.238581: val_loss -0.844 
2024-05-08 15:56:03.238631: Pseudo dice [0.9158] 
2024-05-08 15:56:03.238687: Epoch time: 41.05 s 
2024-05-08 15:56:04.346726:  
2024-05-08 15:56:04.346862: Epoch 307 
2024-05-08 15:56:04.346952: Current learning rate: 0.00719 
2024-05-08 15:56:45.393462: train_loss -0.8265 
2024-05-08 15:56:45.393643: val_loss -0.83 
2024-05-08 15:56:45.393691: Pseudo dice [0.9095] 
2024-05-08 15:56:45.393744: Epoch time: 41.05 s 
2024-05-08 15:56:46.686620:  
2024-05-08 15:56:46.686769: Epoch 308 
2024-05-08 15:56:46.686858: Current learning rate: 0.00718 
2024-05-08 15:57:27.732002: train_loss -0.8339 
2024-05-08 15:57:27.732178: val_loss -0.8519 
2024-05-08 15:57:27.732228: Pseudo dice [0.9233] 
2024-05-08 15:57:27.732280: Epoch time: 41.05 s 
2024-05-08 15:57:28.828230:  
2024-05-08 15:57:28.828511: Epoch 309 
2024-05-08 15:57:28.828729: Current learning rate: 0.00717 
2024-05-08 15:58:09.851188: train_loss -0.8364 
2024-05-08 15:58:09.851369: val_loss -0.8393 
2024-05-08 15:58:09.851417: Pseudo dice [0.9113] 
2024-05-08 15:58:09.851469: Epoch time: 41.02 s 
2024-05-08 15:58:10.938218:  
2024-05-08 15:58:10.938368: Epoch 310 
2024-05-08 15:58:10.938468: Current learning rate: 0.00716 
2024-05-08 15:58:51.955217: train_loss -0.8422 
2024-05-08 15:58:51.955396: val_loss -0.8616 
2024-05-08 15:58:51.955443: Pseudo dice [0.924] 
2024-05-08 15:58:51.955495: Epoch time: 41.02 s 
2024-05-08 15:58:53.035810:  
2024-05-08 15:58:53.036014: Epoch 311 
2024-05-08 15:58:53.036103: Current learning rate: 0.00715 
2024-05-08 15:59:34.057062: train_loss -0.8395 
2024-05-08 15:59:34.057237: val_loss -0.852 
2024-05-08 15:59:34.057285: Pseudo dice [0.9208] 
2024-05-08 15:59:34.057337: Epoch time: 41.02 s 
2024-05-08 15:59:35.138527:  
2024-05-08 15:59:35.138746: Epoch 312 
2024-05-08 15:59:35.138836: Current learning rate: 0.00714 
2024-05-08 16:00:16.116007: train_loss -0.8388 
2024-05-08 16:00:16.116177: val_loss -0.8416 
2024-05-08 16:00:16.116226: Pseudo dice [0.9154] 
2024-05-08 16:00:16.116277: Epoch time: 40.98 s 
2024-05-08 16:00:17.199486:  
2024-05-08 16:00:17.199710: Epoch 313 
2024-05-08 16:00:17.199806: Current learning rate: 0.00713 
2024-05-08 16:00:58.171981: train_loss -0.846 
2024-05-08 16:00:58.172157: val_loss -0.8435 
2024-05-08 16:00:58.172205: Pseudo dice [0.9158] 
2024-05-08 16:00:58.172258: Epoch time: 40.97 s 
2024-05-08 16:00:59.463372:  
2024-05-08 16:00:59.463652: Epoch 314 
2024-05-08 16:00:59.463768: Current learning rate: 0.00712 
2024-05-08 16:01:40.248003: train_loss -0.8327 
2024-05-08 16:01:40.248180: val_loss -0.8361 
2024-05-08 16:01:40.248229: Pseudo dice [0.9085] 
2024-05-08 16:01:40.248283: Epoch time: 40.79 s 
2024-05-08 16:01:41.322789:  
2024-05-08 16:01:41.322932: Epoch 315 
2024-05-08 16:01:41.323034: Current learning rate: 0.00711 
2024-05-08 16:02:22.087041: train_loss -0.847 
2024-05-08 16:02:22.087215: val_loss -0.8478 
2024-05-08 16:02:22.087263: Pseudo dice [0.9172] 
2024-05-08 16:02:22.087314: Epoch time: 40.77 s 
2024-05-08 16:02:23.161204:  
2024-05-08 16:02:23.161345: Epoch 316 
2024-05-08 16:02:23.161436: Current learning rate: 0.0071 
2024-05-08 16:03:04.085151: train_loss -0.8492 
2024-05-08 16:03:04.085330: val_loss -0.849 
2024-05-08 16:03:04.085380: Pseudo dice [0.9164] 
2024-05-08 16:03:04.085431: Epoch time: 40.92 s 
2024-05-08 16:03:05.173217:  
2024-05-08 16:03:05.173341: Epoch 317 
2024-05-08 16:03:05.173435: Current learning rate: 0.0071 
2024-05-08 16:03:46.093846: train_loss -0.8438 
2024-05-08 16:03:46.094023: val_loss -0.8501 
2024-05-08 16:03:46.094071: Pseudo dice [0.9177] 
2024-05-08 16:03:46.094123: Epoch time: 40.92 s 
2024-05-08 16:03:47.174073:  
2024-05-08 16:03:47.174213: Epoch 318 
2024-05-08 16:03:47.174309: Current learning rate: 0.00709 
2024-05-08 16:04:28.061002: train_loss -0.8446 
2024-05-08 16:04:28.061179: val_loss -0.8559 
2024-05-08 16:04:28.061228: Pseudo dice [0.9195] 
2024-05-08 16:04:28.061280: Epoch time: 40.89 s 
2024-05-08 16:04:29.152407:  
2024-05-08 16:04:29.152783: Epoch 319 
2024-05-08 16:04:29.152893: Current learning rate: 0.00708 
2024-05-08 16:05:10.041644: train_loss -0.8399 
2024-05-08 16:05:10.041817: val_loss -0.8571 
2024-05-08 16:05:10.041866: Pseudo dice [0.9218] 
2024-05-08 16:05:10.041918: Epoch time: 40.89 s 
2024-05-08 16:05:11.162365:  
2024-05-08 16:05:11.162642: Epoch 320 
2024-05-08 16:05:11.162786: Current learning rate: 0.00707 
2024-05-08 16:05:52.090251: train_loss -0.8518 
2024-05-08 16:05:52.090421: val_loss -0.8658 
2024-05-08 16:05:52.090469: Pseudo dice [0.9248] 
2024-05-08 16:05:52.090522: Epoch time: 40.93 s 
2024-05-08 16:05:52.090564: Yayy! New best EMA pseudo Dice: 0.918 
2024-05-08 16:05:53.710581:  
2024-05-08 16:05:53.710775: Epoch 321 
2024-05-08 16:05:53.710868: Current learning rate: 0.00706 
2024-05-08 16:06:34.691786: train_loss -0.8517 
2024-05-08 16:06:34.691960: val_loss -0.852 
2024-05-08 16:06:34.692008: Pseudo dice [0.9199] 
2024-05-08 16:06:34.692060: Epoch time: 40.98 s 
2024-05-08 16:06:34.692101: Yayy! New best EMA pseudo Dice: 0.9182 
2024-05-08 16:06:36.117134:  
2024-05-08 16:06:36.117281: Epoch 322 
2024-05-08 16:06:36.117377: Current learning rate: 0.00705 
2024-05-08 16:07:17.084293: train_loss -0.8401 
2024-05-08 16:07:17.084476: val_loss -0.8394 
2024-05-08 16:07:17.084525: Pseudo dice [0.9159] 
2024-05-08 16:07:17.084577: Epoch time: 40.97 s 
2024-05-08 16:07:18.170476:  
2024-05-08 16:07:18.170619: Epoch 323 
2024-05-08 16:07:18.170716: Current learning rate: 0.00704 
2024-05-08 16:07:59.114558: train_loss -0.8432 
2024-05-08 16:07:59.114740: val_loss -0.839 
2024-05-08 16:07:59.114788: Pseudo dice [0.9162] 
2024-05-08 16:07:59.114841: Epoch time: 40.95 s 
2024-05-08 16:08:00.201235:  
2024-05-08 16:08:00.201382: Epoch 324 
2024-05-08 16:08:00.201473: Current learning rate: 0.00703 
2024-05-08 16:08:41.143467: train_loss -0.8326 
2024-05-08 16:08:41.143687: val_loss -0.8511 
2024-05-08 16:08:41.143737: Pseudo dice [0.9168] 
2024-05-08 16:08:41.143790: Epoch time: 40.94 s 
2024-05-08 16:08:42.223369:  
2024-05-08 16:08:42.223621: Epoch 325 
2024-05-08 16:08:42.223719: Current learning rate: 0.00702 
2024-05-08 16:09:23.245821: train_loss -0.8202 
2024-05-08 16:09:23.246002: val_loss -0.8371 
2024-05-08 16:09:23.246061: Pseudo dice [0.9138] 
2024-05-08 16:09:23.246123: Epoch time: 41.02 s 
2024-05-08 16:09:24.329594:  
2024-05-08 16:09:24.329724: Epoch 326 
2024-05-08 16:09:24.329813: Current learning rate: 0.00701 
2024-05-08 16:10:05.246069: train_loss -0.8282 
2024-05-08 16:10:05.246249: val_loss -0.8403 
2024-05-08 16:10:05.246298: Pseudo dice [0.9124] 
2024-05-08 16:10:05.246350: Epoch time: 40.92 s 
2024-05-08 16:10:06.521893:  
2024-05-08 16:10:06.522045: Epoch 327 
2024-05-08 16:10:06.522184: Current learning rate: 0.007 
2024-05-08 16:10:47.474650: train_loss -0.8265 
2024-05-08 16:10:47.474835: val_loss -0.839 
2024-05-08 16:10:47.474883: Pseudo dice [0.9123] 
2024-05-08 16:10:47.474936: Epoch time: 40.95 s 
2024-05-08 16:10:48.564831:  
2024-05-08 16:10:48.565087: Epoch 328 
2024-05-08 16:10:48.565237: Current learning rate: 0.00699 
2024-05-08 16:11:29.473642: train_loss -0.8404 
2024-05-08 16:11:29.473815: val_loss -0.8577 
2024-05-08 16:11:29.473863: Pseudo dice [0.921] 
2024-05-08 16:11:29.473915: Epoch time: 40.91 s 
2024-05-08 16:11:30.583750:  
2024-05-08 16:11:30.583897: Epoch 329 
2024-05-08 16:11:30.583997: Current learning rate: 0.00698 
2024-05-08 16:12:11.521047: train_loss -0.8429 
2024-05-08 16:12:11.521236: val_loss -0.8601 
2024-05-08 16:12:11.521286: Pseudo dice [0.9242] 
2024-05-08 16:12:11.521342: Epoch time: 40.94 s 
2024-05-08 16:12:12.604775:  
2024-05-08 16:12:12.604977: Epoch 330 
2024-05-08 16:12:12.605121: Current learning rate: 0.00697 
2024-05-08 16:12:53.515398: train_loss -0.8494 
2024-05-08 16:12:53.515579: val_loss -0.8557 
2024-05-08 16:12:53.515628: Pseudo dice [0.9211] 
2024-05-08 16:12:53.515680: Epoch time: 40.91 s 
2024-05-08 16:12:54.594880:  
2024-05-08 16:12:54.595018: Epoch 331 
2024-05-08 16:12:54.595114: Current learning rate: 0.00696 
2024-05-08 16:13:35.507771: train_loss -0.8506 
2024-05-08 16:13:35.507949: val_loss -0.8644 
2024-05-08 16:13:35.507998: Pseudo dice [0.9273] 
2024-05-08 16:13:35.508050: Epoch time: 40.91 s 
2024-05-08 16:13:35.508093: Yayy! New best EMA pseudo Dice: 0.9189 
2024-05-08 16:13:36.931918:  
2024-05-08 16:13:36.932040: Epoch 332 
2024-05-08 16:13:36.932133: Current learning rate: 0.00696 
2024-05-08 16:14:17.862989: train_loss -0.8441 
2024-05-08 16:14:17.863167: val_loss -0.8492 
2024-05-08 16:14:17.863216: Pseudo dice [0.921] 
2024-05-08 16:14:17.863267: Epoch time: 40.93 s 
2024-05-08 16:14:17.863309: Yayy! New best EMA pseudo Dice: 0.9191 
2024-05-08 16:14:19.456175:  
2024-05-08 16:14:19.456368: Epoch 333 
2024-05-08 16:14:19.456461: Current learning rate: 0.00695 
2024-05-08 16:15:00.425081: train_loss -0.8434 
2024-05-08 16:15:00.425261: val_loss -0.8391 
2024-05-08 16:15:00.425310: Pseudo dice [0.91] 
2024-05-08 16:15:00.425364: Epoch time: 40.97 s 
2024-05-08 16:15:01.513685:  
2024-05-08 16:15:01.513827: Epoch 334 
2024-05-08 16:15:01.513918: Current learning rate: 0.00694 
2024-05-08 16:15:42.475991: train_loss -0.8384 
2024-05-08 16:15:42.476167: val_loss -0.8473 
2024-05-08 16:15:42.476214: Pseudo dice [0.9197] 
2024-05-08 16:15:42.476268: Epoch time: 40.96 s 
2024-05-08 16:15:43.575533:  
2024-05-08 16:15:43.575729: Epoch 335 
2024-05-08 16:15:43.575821: Current learning rate: 0.00693 
2024-05-08 16:16:24.585602: train_loss -0.8446 
2024-05-08 16:16:24.585792: val_loss -0.859 
2024-05-08 16:16:24.585839: Pseudo dice [0.9229] 
2024-05-08 16:16:24.585891: Epoch time: 41.01 s 
2024-05-08 16:16:25.691823:  
2024-05-08 16:16:25.691965: Epoch 336 
2024-05-08 16:16:25.692062: Current learning rate: 0.00692 
2024-05-08 16:17:06.688764: train_loss -0.8365 
2024-05-08 16:17:06.688931: val_loss -0.8435 
2024-05-08 16:17:06.688978: Pseudo dice [0.9221] 
2024-05-08 16:17:06.689029: Epoch time: 41.0 s 
2024-05-08 16:17:06.689072: Yayy! New best EMA pseudo Dice: 0.9191 
2024-05-08 16:17:08.129095:  
2024-05-08 16:17:08.129279: Epoch 337 
2024-05-08 16:17:08.129405: Current learning rate: 0.00691 
2024-05-08 16:17:49.060392: train_loss -0.8388 
2024-05-08 16:17:49.060549: val_loss -0.8579 
2024-05-08 16:17:49.060599: Pseudo dice [0.923] 
2024-05-08 16:17:49.060658: Epoch time: 40.93 s 
2024-05-08 16:17:49.060703: Yayy! New best EMA pseudo Dice: 0.9195 
2024-05-08 16:17:50.525444:  
2024-05-08 16:17:50.525615: Epoch 338 
2024-05-08 16:17:50.525711: Current learning rate: 0.0069 
2024-05-08 16:18:31.436293: train_loss -0.8477 
2024-05-08 16:18:31.436466: val_loss -0.8588 
2024-05-08 16:18:31.436515: Pseudo dice [0.9215] 
2024-05-08 16:18:31.436570: Epoch time: 40.91 s 
2024-05-08 16:18:31.436618: Yayy! New best EMA pseudo Dice: 0.9197 
2024-05-08 16:18:33.050258:  
2024-05-08 16:18:33.050481: Epoch 339 
2024-05-08 16:18:33.050595: Current learning rate: 0.00689 
2024-05-08 16:19:14.022966: train_loss -0.8418 
2024-05-08 16:19:14.023136: val_loss -0.8519 
2024-05-08 16:19:14.023184: Pseudo dice [0.9193] 
2024-05-08 16:19:14.023236: Epoch time: 40.97 s 
2024-05-08 16:19:15.129060:  
2024-05-08 16:19:15.129200: Epoch 340 
2024-05-08 16:19:15.129288: Current learning rate: 0.00688 
2024-05-08 16:19:56.161463: train_loss -0.8439 
2024-05-08 16:19:56.161628: val_loss -0.8609 
2024-05-08 16:19:56.161677: Pseudo dice [0.9201] 
2024-05-08 16:19:56.161729: Epoch time: 41.03 s 
2024-05-08 16:19:56.161771: Yayy! New best EMA pseudo Dice: 0.9197 
2024-05-08 16:19:57.610393:  
2024-05-08 16:19:57.610541: Epoch 341 
2024-05-08 16:19:57.610631: Current learning rate: 0.00687 
2024-05-08 16:20:38.616281: train_loss -0.8443 
2024-05-08 16:20:38.616444: val_loss -0.8379 
2024-05-08 16:20:38.616492: Pseudo dice [0.9103] 
2024-05-08 16:20:38.616544: Epoch time: 41.01 s 
2024-05-08 16:20:39.714184:  
2024-05-08 16:20:39.714344: Epoch 342 
2024-05-08 16:20:39.714446: Current learning rate: 0.00686 
2024-05-08 16:21:20.676262: train_loss -0.8371 
2024-05-08 16:21:20.676419: val_loss -0.8586 
2024-05-08 16:21:20.676468: Pseudo dice [0.9243] 
2024-05-08 16:21:20.676521: Epoch time: 40.96 s 
2024-05-08 16:21:21.794338:  
2024-05-08 16:21:21.794472: Epoch 343 
2024-05-08 16:21:21.794559: Current learning rate: 0.00685 
2024-05-08 16:22:02.747118: train_loss -0.8457 
2024-05-08 16:22:02.747299: val_loss -0.8503 
2024-05-08 16:22:02.747347: Pseudo dice [0.9162] 
2024-05-08 16:22:02.747400: Epoch time: 40.95 s 
2024-05-08 16:22:03.852296:  
2024-05-08 16:22:03.852431: Epoch 344 
2024-05-08 16:22:03.852525: Current learning rate: 0.00684 
2024-05-08 16:22:44.764229: train_loss -0.8462 
2024-05-08 16:22:44.764392: val_loss -0.8556 
2024-05-08 16:22:44.764441: Pseudo dice [0.9223] 
2024-05-08 16:22:44.764495: Epoch time: 40.91 s 
2024-05-08 16:22:45.864070:  
2024-05-08 16:22:45.864190: Epoch 345 
2024-05-08 16:22:45.864280: Current learning rate: 0.00683 
2024-05-08 16:23:26.748853: train_loss -0.8474 
2024-05-08 16:23:26.749032: val_loss -0.8518 
2024-05-08 16:23:26.749081: Pseudo dice [0.921] 
2024-05-08 16:23:26.749134: Epoch time: 40.89 s 
2024-05-08 16:23:28.059073:  
2024-05-08 16:23:28.059226: Epoch 346 
2024-05-08 16:23:28.059326: Current learning rate: 0.00682 
2024-05-08 16:24:08.963137: train_loss -0.829 
2024-05-08 16:24:08.963325: val_loss -0.8444 
2024-05-08 16:24:08.963375: Pseudo dice [0.9155] 
2024-05-08 16:24:08.963429: Epoch time: 40.91 s 
2024-05-08 16:24:10.054184:  
2024-05-08 16:24:10.054371: Epoch 347 
2024-05-08 16:24:10.054468: Current learning rate: 0.00681 
2024-05-08 16:24:50.936321: train_loss -0.8409 
2024-05-08 16:24:50.936503: val_loss -0.8593 
2024-05-08 16:24:50.936552: Pseudo dice [0.9213] 
2024-05-08 16:24:50.936605: Epoch time: 40.88 s 
2024-05-08 16:24:52.076282:  
2024-05-08 16:24:52.076418: Epoch 348 
2024-05-08 16:24:52.076509: Current learning rate: 0.0068 
2024-05-08 16:25:32.940117: train_loss -0.8427 
2024-05-08 16:25:32.940292: val_loss -0.8592 
2024-05-08 16:25:32.940341: Pseudo dice [0.9198] 
2024-05-08 16:25:32.940394: Epoch time: 40.86 s 
2024-05-08 16:25:34.104846:  
2024-05-08 16:25:34.104990: Epoch 349 
2024-05-08 16:25:34.105079: Current learning rate: 0.0068 
2024-05-08 16:26:14.953251: train_loss -0.8426 
2024-05-08 16:26:14.953421: val_loss -0.8592 
2024-05-08 16:26:14.953470: Pseudo dice [0.9213] 
2024-05-08 16:26:14.953522: Epoch time: 40.85 s 
2024-05-08 16:26:16.404972:  
2024-05-08 16:26:16.405112: Epoch 350 
2024-05-08 16:26:16.405205: Current learning rate: 0.00679 
2024-05-08 16:26:57.287126: train_loss -0.8469 
2024-05-08 16:26:57.287316: val_loss -0.8539 
2024-05-08 16:26:57.287389: Pseudo dice [0.9236] 
2024-05-08 16:26:57.287442: Epoch time: 40.88 s 
2024-05-08 16:26:57.287483: Yayy! New best EMA pseudo Dice: 0.92 
2024-05-08 16:26:58.736031:  
2024-05-08 16:26:58.736156: Epoch 351 
2024-05-08 16:26:58.736244: Current learning rate: 0.00678 
2024-05-08 16:27:39.687970: train_loss -0.8296 
2024-05-08 16:27:39.688156: val_loss -0.8387 
2024-05-08 16:27:39.688215: Pseudo dice [0.9157] 
2024-05-08 16:27:39.688269: Epoch time: 40.95 s 
2024-05-08 16:27:40.980954:  
2024-05-08 16:27:40.981091: Epoch 352 
2024-05-08 16:27:40.981180: Current learning rate: 0.00677 
2024-05-08 16:28:21.938246: train_loss -0.8387 
2024-05-08 16:28:21.938430: val_loss -0.8525 
2024-05-08 16:28:21.938479: Pseudo dice [0.9203] 
2024-05-08 16:28:21.938531: Epoch time: 40.96 s 
2024-05-08 16:28:23.050678:  
2024-05-08 16:28:23.050887: Epoch 353 
2024-05-08 16:28:23.050980: Current learning rate: 0.00676 
2024-05-08 16:29:04.008783: train_loss -0.838 
2024-05-08 16:29:04.008964: val_loss -0.8512 
2024-05-08 16:29:04.009012: Pseudo dice [0.9204] 
2024-05-08 16:29:04.009065: Epoch time: 40.96 s 
2024-05-08 16:29:05.137917:  
2024-05-08 16:29:05.138104: Epoch 354 
2024-05-08 16:29:05.138251: Current learning rate: 0.00675 
2024-05-08 16:29:46.095912: train_loss -0.8379 
2024-05-08 16:29:46.096089: val_loss -0.8605 
2024-05-08 16:29:46.096137: Pseudo dice [0.9231] 
2024-05-08 16:29:46.096191: Epoch time: 40.96 s 
2024-05-08 16:29:46.096233: Yayy! New best EMA pseudo Dice: 0.92 
2024-05-08 16:29:47.556396:  
2024-05-08 16:29:47.556618: Epoch 355 
2024-05-08 16:29:47.556716: Current learning rate: 0.00674 
2024-05-08 16:30:28.457710: train_loss -0.8332 
2024-05-08 16:30:28.457886: val_loss -0.8518 
2024-05-08 16:30:28.457936: Pseudo dice [0.9196] 
2024-05-08 16:30:28.458099: Epoch time: 40.9 s 
2024-05-08 16:30:29.563926:  
2024-05-08 16:30:29.564053: Epoch 356 
2024-05-08 16:30:29.564144: Current learning rate: 0.00673 
2024-05-08 16:31:10.450893: train_loss -0.8457 
2024-05-08 16:31:10.451068: val_loss -0.8563 
2024-05-08 16:31:10.451116: Pseudo dice [0.9198] 
2024-05-08 16:31:10.451169: Epoch time: 40.89 s 
2024-05-08 16:31:11.546463:  
2024-05-08 16:31:11.546610: Epoch 357 
2024-05-08 16:31:11.546710: Current learning rate: 0.00672 
2024-05-08 16:31:52.412908: train_loss -0.8479 
2024-05-08 16:31:52.413088: val_loss -0.8587 
2024-05-08 16:31:52.413135: Pseudo dice [0.9267] 
2024-05-08 16:31:52.413188: Epoch time: 40.87 s 
2024-05-08 16:31:52.413230: Yayy! New best EMA pseudo Dice: 0.9206 
2024-05-08 16:31:54.051376:  
2024-05-08 16:31:54.051579: Epoch 358 
2024-05-08 16:31:54.051696: Current learning rate: 0.00671 
2024-05-08 16:32:34.888406: train_loss -0.8494 
2024-05-08 16:32:34.888578: val_loss -0.8638 
2024-05-08 16:32:34.888627: Pseudo dice [0.9265] 
2024-05-08 16:32:34.888688: Epoch time: 40.84 s 
2024-05-08 16:32:34.888733: Yayy! New best EMA pseudo Dice: 0.9212 
2024-05-08 16:32:36.323294:  
2024-05-08 16:32:36.323533: Epoch 359 
2024-05-08 16:32:36.323628: Current learning rate: 0.0067 
2024-05-08 16:33:17.217775: train_loss -0.8516 
2024-05-08 16:33:17.217973: val_loss -0.8376 
2024-05-08 16:33:17.218037: Pseudo dice [0.9141] 
2024-05-08 16:33:17.218104: Epoch time: 40.9 s 
2024-05-08 16:33:18.308759:  
2024-05-08 16:33:18.308905: Epoch 360 
2024-05-08 16:33:18.309012: Current learning rate: 0.00669 
2024-05-08 16:33:59.137568: train_loss -0.8467 
2024-05-08 16:33:59.137749: val_loss -0.8553 
2024-05-08 16:33:59.137798: Pseudo dice [0.9175] 
2024-05-08 16:33:59.137850: Epoch time: 40.83 s 
2024-05-08 16:34:00.243093:  
2024-05-08 16:34:00.243227: Epoch 361 
2024-05-08 16:34:00.243318: Current learning rate: 0.00668 
2024-05-08 16:34:41.132308: train_loss -0.851 
2024-05-08 16:34:41.132489: val_loss -0.849 
2024-05-08 16:34:41.132537: Pseudo dice [0.916] 
2024-05-08 16:34:41.132590: Epoch time: 40.89 s 
2024-05-08 16:34:42.267725:  
2024-05-08 16:34:42.267861: Epoch 362 
2024-05-08 16:34:42.267948: Current learning rate: 0.00667 
2024-05-08 16:35:23.145592: train_loss -0.8497 
2024-05-08 16:35:23.145783: val_loss -0.8606 
2024-05-08 16:35:23.145837: Pseudo dice [0.9176] 
2024-05-08 16:35:23.145890: Epoch time: 40.88 s 
2024-05-08 16:35:24.247667:  
2024-05-08 16:35:24.247798: Epoch 363 
2024-05-08 16:35:24.247890: Current learning rate: 0.00666 
2024-05-08 16:36:05.162593: train_loss -0.8511 
2024-05-08 16:36:05.162807: val_loss -0.8505 
2024-05-08 16:36:05.162879: Pseudo dice [0.9181] 
2024-05-08 16:36:05.162980: Epoch time: 40.92 s 
2024-05-08 16:36:06.465377:  
2024-05-08 16:36:06.465523: Epoch 364 
2024-05-08 16:36:06.465622: Current learning rate: 0.00665 
2024-05-08 16:36:47.365577: train_loss -0.8501 
2024-05-08 16:36:47.365762: val_loss -0.8529 
2024-05-08 16:36:47.365822: Pseudo dice [0.9225] 
2024-05-08 16:36:47.365876: Epoch time: 40.9 s 
2024-05-08 16:36:48.470337:  
2024-05-08 16:36:48.470478: Epoch 365 
2024-05-08 16:36:48.470569: Current learning rate: 0.00665 
2024-05-08 16:37:29.375045: train_loss -0.8609 
2024-05-08 16:37:29.375226: val_loss -0.8595 
2024-05-08 16:37:29.375274: Pseudo dice [0.9204] 
2024-05-08 16:37:29.375328: Epoch time: 40.91 s 
2024-05-08 16:37:30.483532:  
2024-05-08 16:37:30.483666: Epoch 366 
2024-05-08 16:37:30.483755: Current learning rate: 0.00664 
2024-05-08 16:38:11.320160: train_loss -0.8502 
2024-05-08 16:38:11.320341: val_loss -0.86 
2024-05-08 16:38:11.320389: Pseudo dice [0.9217] 
2024-05-08 16:38:11.320447: Epoch time: 40.84 s 
2024-05-08 16:38:12.434788:  
2024-05-08 16:38:12.435090: Epoch 367 
2024-05-08 16:38:12.435183: Current learning rate: 0.00663 
2024-05-08 16:38:53.268210: train_loss -0.8454 
2024-05-08 16:38:53.268390: val_loss -0.8496 
2024-05-08 16:38:53.268438: Pseudo dice [0.9168] 
2024-05-08 16:38:53.268491: Epoch time: 40.83 s 
2024-05-08 16:38:54.576187:  
2024-05-08 16:38:54.576318: Epoch 368 
2024-05-08 16:38:54.576410: Current learning rate: 0.00662 
2024-05-08 16:39:35.427777: train_loss -0.8496 
2024-05-08 16:39:35.427951: val_loss -0.8619 
2024-05-08 16:39:35.428002: Pseudo dice [0.925] 
2024-05-08 16:39:35.428054: Epoch time: 40.85 s 
2024-05-08 16:39:36.576510:  
2024-05-08 16:39:36.576684: Epoch 369 
2024-05-08 16:39:36.576777: Current learning rate: 0.00661 
2024-05-08 16:40:17.406267: train_loss -0.8527 
2024-05-08 16:40:17.406440: val_loss -0.8501 
2024-05-08 16:40:17.406490: Pseudo dice [0.9209] 
2024-05-08 16:40:17.406542: Epoch time: 40.83 s 
2024-05-08 16:40:18.722867:  
2024-05-08 16:40:18.723062: Epoch 370 
2024-05-08 16:40:18.723156: Current learning rate: 0.0066 
2024-05-08 16:40:59.652523: train_loss -0.8507 
2024-05-08 16:40:59.652710: val_loss -0.8515 
2024-05-08 16:40:59.652763: Pseudo dice [0.9175] 
2024-05-08 16:40:59.652816: Epoch time: 40.93 s 
2024-05-08 16:41:00.764812:  
2024-05-08 16:41:00.765034: Epoch 371 
2024-05-08 16:41:00.765266: Current learning rate: 0.00659 
2024-05-08 16:41:41.660475: train_loss -0.8481 
2024-05-08 16:41:41.660668: val_loss -0.858 
2024-05-08 16:41:41.660721: Pseudo dice [0.9198] 
2024-05-08 16:41:41.660776: Epoch time: 40.9 s 
2024-05-08 16:41:42.765554:  
2024-05-08 16:41:42.765708: Epoch 372 
2024-05-08 16:41:42.765801: Current learning rate: 0.00658 
2024-05-08 16:42:23.621845: train_loss -0.845 
2024-05-08 16:42:23.622029: val_loss -0.8522 
2024-05-08 16:42:23.622203: Pseudo dice [0.9177] 
2024-05-08 16:42:23.622256: Epoch time: 40.86 s 
2024-05-08 16:42:24.722467:  
2024-05-08 16:42:24.722607: Epoch 373 
2024-05-08 16:42:24.722700: Current learning rate: 0.00657 
2024-05-08 16:43:05.589761: train_loss -0.8459 
2024-05-08 16:43:05.589942: val_loss -0.8539 
2024-05-08 16:43:05.589991: Pseudo dice [0.9199] 
2024-05-08 16:43:05.590044: Epoch time: 40.87 s 
2024-05-08 16:43:06.697258:  
2024-05-08 16:43:06.697400: Epoch 374 
2024-05-08 16:43:06.697495: Current learning rate: 0.00656 
2024-05-08 16:43:47.534360: train_loss -0.8503 
2024-05-08 16:43:47.534544: val_loss -0.8626 
2024-05-08 16:43:47.534593: Pseudo dice [0.9272] 
2024-05-08 16:43:47.534646: Epoch time: 40.84 s 
2024-05-08 16:43:48.686552:  
2024-05-08 16:43:48.686686: Epoch 375 
2024-05-08 16:43:48.686785: Current learning rate: 0.00655 
2024-05-08 16:44:29.568390: train_loss -0.844 
2024-05-08 16:44:29.568570: val_loss -0.8713 
2024-05-08 16:44:29.568617: Pseudo dice [0.9278] 
2024-05-08 16:44:29.568678: Epoch time: 40.88 s 
2024-05-08 16:44:29.568731: Yayy! New best EMA pseudo Dice: 0.9212 
2024-05-08 16:44:31.184621:  
2024-05-08 16:44:31.184817: Epoch 376 
2024-05-08 16:44:31.184919: Current learning rate: 0.00654 
2024-05-08 16:45:12.035717: train_loss -0.8627 
2024-05-08 16:45:12.035901: val_loss -0.8762 
2024-05-08 16:45:12.035952: Pseudo dice [0.9318] 
2024-05-08 16:45:12.036005: Epoch time: 40.85 s 
2024-05-08 16:45:12.036047: Yayy! New best EMA pseudo Dice: 0.9223 
2024-05-08 16:45:13.486876:  
2024-05-08 16:45:13.487022: Epoch 377 
2024-05-08 16:45:13.487111: Current learning rate: 0.00653 
2024-05-08 16:45:54.371088: train_loss -0.8615 
2024-05-08 16:45:54.371273: val_loss -0.8673 
2024-05-08 16:45:54.371324: Pseudo dice [0.9259] 
2024-05-08 16:45:54.371378: Epoch time: 40.89 s 
2024-05-08 16:45:54.371421: Yayy! New best EMA pseudo Dice: 0.9226 
2024-05-08 16:45:55.830378:  
2024-05-08 16:45:55.830519: Epoch 378 
2024-05-08 16:45:55.830612: Current learning rate: 0.00652 
2024-05-08 16:46:36.736511: train_loss -0.8539 
2024-05-08 16:46:36.736696: val_loss -0.8396 
2024-05-08 16:46:36.736748: Pseudo dice [0.914] 
2024-05-08 16:46:36.736801: Epoch time: 40.91 s 
2024-05-08 16:46:37.850295:  
2024-05-08 16:46:37.850431: Epoch 379 
2024-05-08 16:46:37.850522: Current learning rate: 0.00651 
2024-05-08 16:47:18.771603: train_loss -0.8553 
2024-05-08 16:47:18.771906: val_loss -0.8504 
2024-05-08 16:47:18.771960: Pseudo dice [0.9168] 
2024-05-08 16:47:18.772013: Epoch time: 40.92 s 
2024-05-08 16:47:19.882554:  
2024-05-08 16:47:19.882808: Epoch 380 
2024-05-08 16:47:19.882904: Current learning rate: 0.0065 
2024-05-08 16:48:00.712660: train_loss -0.854 
2024-05-08 16:48:00.712840: val_loss -0.8599 
2024-05-08 16:48:00.712888: Pseudo dice [0.9247] 
2024-05-08 16:48:00.712940: Epoch time: 40.83 s 
2024-05-08 16:48:01.831229:  
2024-05-08 16:48:01.831406: Epoch 381 
2024-05-08 16:48:01.831503: Current learning rate: 0.00649 
2024-05-08 16:48:42.658757: train_loss -0.8542 
2024-05-08 16:48:42.658939: val_loss -0.857 
2024-05-08 16:48:42.658988: Pseudo dice [0.9235] 
2024-05-08 16:48:42.659040: Epoch time: 40.83 s 
2024-05-08 16:48:43.961415:  
2024-05-08 16:48:43.961567: Epoch 382 
2024-05-08 16:48:43.961663: Current learning rate: 0.00648 
2024-05-08 16:49:24.803673: train_loss -0.852 
2024-05-08 16:49:24.803860: val_loss -0.8538 
2024-05-08 16:49:24.803912: Pseudo dice [0.9195] 
2024-05-08 16:49:24.803980: Epoch time: 40.84 s 
2024-05-08 16:49:25.926872:  
2024-05-08 16:49:25.927014: Epoch 383 
2024-05-08 16:49:25.927105: Current learning rate: 0.00648 
2024-05-08 16:50:06.764986: train_loss -0.8536 
2024-05-08 16:50:06.765164: val_loss -0.8676 
2024-05-08 16:50:06.765213: Pseudo dice [0.9251] 
2024-05-08 16:50:06.765267: Epoch time: 40.84 s 
2024-05-08 16:50:07.873586:  
2024-05-08 16:50:07.873724: Epoch 384 
2024-05-08 16:50:07.873815: Current learning rate: 0.00647 
2024-05-08 16:50:48.736709: train_loss -0.8511 
2024-05-08 16:50:48.736888: val_loss -0.8593 
2024-05-08 16:50:48.736939: Pseudo dice [0.9257] 
2024-05-08 16:50:48.737045: Epoch time: 40.86 s 
2024-05-08 16:50:49.860840:  
2024-05-08 16:50:49.860977: Epoch 385 
2024-05-08 16:50:49.861069: Current learning rate: 0.00646 
2024-05-08 16:51:30.703559: train_loss -0.8213 
2024-05-08 16:51:30.703740: val_loss -0.8086 
2024-05-08 16:51:30.703793: Pseudo dice [0.8949] 
2024-05-08 16:51:30.703879: Epoch time: 40.84 s 
2024-05-08 16:51:31.826251:  
2024-05-08 16:51:31.826387: Epoch 386 
2024-05-08 16:51:31.826480: Current learning rate: 0.00645 
2024-05-08 16:52:12.685959: train_loss -0.8315 
2024-05-08 16:52:12.686138: val_loss -0.8436 
2024-05-08 16:52:12.686187: Pseudo dice [0.9132] 
2024-05-08 16:52:12.686239: Epoch time: 40.86 s 
2024-05-08 16:52:13.806419:  
2024-05-08 16:52:13.806697: Epoch 387 
2024-05-08 16:52:13.806791: Current learning rate: 0.00644 
2024-05-08 16:52:54.663872: train_loss -0.8345 
2024-05-08 16:52:54.664116: val_loss -0.8467 
2024-05-08 16:52:54.664169: Pseudo dice [0.9176] 
2024-05-08 16:52:54.664227: Epoch time: 40.86 s 
2024-05-08 16:52:55.974225:  
2024-05-08 16:52:55.974365: Epoch 388 
2024-05-08 16:52:55.974463: Current learning rate: 0.00643 
2024-05-08 16:53:36.889485: train_loss -0.8452 
2024-05-08 16:53:36.889680: val_loss -0.8455 
2024-05-08 16:53:36.889739: Pseudo dice [0.9179] 
2024-05-08 16:53:36.889797: Epoch time: 40.92 s 
2024-05-08 16:53:38.031441:  
2024-05-08 16:53:38.031580: Epoch 389 
2024-05-08 16:53:38.031671: Current learning rate: 0.00642 
2024-05-08 16:54:19.017302: train_loss -0.8327 
2024-05-08 16:54:19.017468: val_loss -0.8382 
2024-05-08 16:54:19.017517: Pseudo dice [0.9149] 
2024-05-08 16:54:19.017569: Epoch time: 40.99 s 
2024-05-08 16:54:20.164424:  
2024-05-08 16:54:20.164559: Epoch 390 
2024-05-08 16:54:20.164652: Current learning rate: 0.00641 
2024-05-08 16:55:01.079072: train_loss -0.8353 
2024-05-08 16:55:01.079311: val_loss -0.8583 
2024-05-08 16:55:01.079361: Pseudo dice [0.9216] 
2024-05-08 16:55:01.079414: Epoch time: 40.92 s 
2024-05-08 16:55:02.200526:  
2024-05-08 16:55:02.200663: Epoch 391 
2024-05-08 16:55:02.200760: Current learning rate: 0.0064 
2024-05-08 16:55:43.137779: train_loss -0.8363 
2024-05-08 16:55:43.137957: val_loss -0.848 
2024-05-08 16:55:43.138008: Pseudo dice [0.9131] 
2024-05-08 16:55:43.138197: Epoch time: 40.94 s 
2024-05-08 16:55:44.253942:  
2024-05-08 16:55:44.254166: Epoch 392 
2024-05-08 16:55:44.254261: Current learning rate: 0.00639 
2024-05-08 16:56:25.158571: train_loss -0.8451 
2024-05-08 16:56:25.158750: val_loss -0.8605 
2024-05-08 16:56:25.158804: Pseudo dice [0.9199] 
2024-05-08 16:56:25.158857: Epoch time: 40.91 s 
2024-05-08 16:56:26.284864:  
2024-05-08 16:56:26.284991: Epoch 393 
2024-05-08 16:56:26.285081: Current learning rate: 0.00638 
2024-05-08 16:57:07.243299: train_loss -0.852 
2024-05-08 16:57:07.243474: val_loss -0.8696 
2024-05-08 16:57:07.243532: Pseudo dice [0.928] 
2024-05-08 16:57:07.243648: Epoch time: 40.96 s 
2024-05-08 16:57:08.560883:  
2024-05-08 16:57:08.561078: Epoch 394 
2024-05-08 16:57:08.561207: Current learning rate: 0.00637 
2024-05-08 16:57:49.492549: train_loss -0.8452 
2024-05-08 16:57:49.492740: val_loss -0.8585 
2024-05-08 16:57:49.492792: Pseudo dice [0.9215] 
2024-05-08 16:57:49.492844: Epoch time: 40.93 s 
2024-05-08 16:57:50.632367:  
2024-05-08 16:57:50.632568: Epoch 395 
2024-05-08 16:57:50.632688: Current learning rate: 0.00636 
2024-05-08 16:58:31.515433: train_loss -0.8442 
2024-05-08 16:58:31.515614: val_loss -0.846 
2024-05-08 16:58:31.515662: Pseudo dice [0.9138] 
2024-05-08 16:58:31.515716: Epoch time: 40.88 s 
2024-05-08 16:58:32.649370:  
2024-05-08 16:58:32.649553: Epoch 396 
2024-05-08 16:58:32.649692: Current learning rate: 0.00635 
2024-05-08 16:59:13.538748: train_loss -0.8489 
2024-05-08 16:59:13.538928: val_loss -0.8576 
2024-05-08 16:59:13.538980: Pseudo dice [0.9201] 
2024-05-08 16:59:13.539034: Epoch time: 40.89 s 
2024-05-08 16:59:14.662807:  
2024-05-08 16:59:14.662946: Epoch 397 
2024-05-08 16:59:14.663036: Current learning rate: 0.00634 
2024-05-08 16:59:55.516246: train_loss -0.8464 
2024-05-08 16:59:55.516426: val_loss -0.8614 
2024-05-08 16:59:55.516474: Pseudo dice [0.9225] 
2024-05-08 16:59:55.516526: Epoch time: 40.85 s 
2024-05-08 16:59:56.641767:  
2024-05-08 16:59:56.641893: Epoch 398 
2024-05-08 16:59:56.641988: Current learning rate: 0.00633 
2024-05-08 17:00:37.955569: train_loss -0.8484 
2024-05-08 17:00:37.955747: val_loss -0.8694 
2024-05-08 17:00:37.955795: Pseudo dice [0.9267] 
2024-05-08 17:00:37.955849: Epoch time: 41.31 s 
2024-05-08 17:00:39.095627:  
2024-05-08 17:00:39.095760: Epoch 399 
2024-05-08 17:00:39.095851: Current learning rate: 0.00632 
2024-05-08 17:01:20.064909: train_loss -0.8451 
2024-05-08 17:01:20.065087: val_loss -0.8545 
2024-05-08 17:01:20.065138: Pseudo dice [0.9214] 
2024-05-08 17:01:20.065192: Epoch time: 40.97 s 
2024-05-08 17:01:21.693942:  
2024-05-08 17:01:21.694158: Epoch 400 
2024-05-08 17:01:21.694289: Current learning rate: 0.00631 
2024-05-08 17:02:03.037112: train_loss -0.846 
2024-05-08 17:02:03.037284: val_loss -0.8519 
2024-05-08 17:02:03.037333: Pseudo dice [0.9243] 
2024-05-08 17:02:03.037389: Epoch time: 41.34 s 
2024-05-08 17:02:04.174282:  
2024-05-08 17:02:04.174524: Epoch 401 
2024-05-08 17:02:04.174614: Current learning rate: 0.0063 
2024-05-08 17:02:45.087536: train_loss -0.8434 
2024-05-08 17:02:45.087713: val_loss -0.8526 
2024-05-08 17:02:45.087763: Pseudo dice [0.9189] 
2024-05-08 17:02:45.087816: Epoch time: 40.91 s 
2024-05-08 17:02:46.224244:  
2024-05-08 17:02:46.224439: Epoch 402 
2024-05-08 17:02:46.224548: Current learning rate: 0.0063 
2024-05-08 17:03:27.034366: train_loss -0.8425 
2024-05-08 17:03:27.034539: val_loss -0.837 
2024-05-08 17:03:27.034588: Pseudo dice [0.9131] 
2024-05-08 17:03:27.034643: Epoch time: 40.81 s 
2024-05-08 17:03:28.159985:  
2024-05-08 17:03:28.160130: Epoch 403 
2024-05-08 17:03:28.160233: Current learning rate: 0.00629 
2024-05-08 17:04:08.983071: train_loss -0.8349 
2024-05-08 17:04:08.983260: val_loss -0.8498 
2024-05-08 17:04:08.983308: Pseudo dice [0.9192] 
2024-05-08 17:04:08.983360: Epoch time: 40.82 s 
2024-05-08 17:04:10.108668:  
2024-05-08 17:04:10.108805: Epoch 404 
2024-05-08 17:04:10.108894: Current learning rate: 0.00628 
2024-05-08 17:04:50.905078: train_loss -0.8461 
2024-05-08 17:04:50.905249: val_loss -0.8496 
2024-05-08 17:04:50.905298: Pseudo dice [0.9164] 
2024-05-08 17:04:50.905350: Epoch time: 40.8 s 
2024-05-08 17:04:52.020421:  
2024-05-08 17:04:52.020550: Epoch 405 
2024-05-08 17:04:52.020639: Current learning rate: 0.00627 
2024-05-08 17:05:32.937480: train_loss -0.8508 
2024-05-08 17:05:32.937883: val_loss -0.8661 
2024-05-08 17:05:32.937934: Pseudo dice [0.9268] 
2024-05-08 17:05:32.937988: Epoch time: 40.92 s 
2024-05-08 17:05:34.251799:  
2024-05-08 17:05:34.252005: Epoch 406 
2024-05-08 17:05:34.252101: Current learning rate: 0.00626 
2024-05-08 17:06:15.424493: train_loss -0.8483 
2024-05-08 17:06:15.425052: val_loss -0.8547 
2024-05-08 17:06:15.425107: Pseudo dice [0.9223] 
2024-05-08 17:06:15.425386: Epoch time: 41.17 s 
2024-05-08 17:06:16.585000:  
2024-05-08 17:06:16.585147: Epoch 407 
2024-05-08 17:06:16.585239: Current learning rate: 0.00625 
2024-05-08 17:06:57.770828: train_loss -0.8441 
2024-05-08 17:06:57.771024: val_loss -0.8557 
2024-05-08 17:06:57.771073: Pseudo dice [0.9222] 
2024-05-08 17:06:57.771127: Epoch time: 41.19 s 
2024-05-08 17:06:58.904674:  
2024-05-08 17:06:58.904893: Epoch 408 
2024-05-08 17:06:58.904991: Current learning rate: 0.00624 
2024-05-08 17:07:39.882918: train_loss -0.8521 
2024-05-08 17:07:39.883099: val_loss -0.8508 
2024-05-08 17:07:39.883150: Pseudo dice [0.9181] 
2024-05-08 17:07:39.883204: Epoch time: 40.98 s 
2024-05-08 17:07:41.007255:  
2024-05-08 17:07:41.007392: Epoch 409 
2024-05-08 17:07:41.007483: Current learning rate: 0.00623 
2024-05-08 17:08:21.896127: train_loss -0.8547 
2024-05-08 17:08:21.896305: val_loss -0.8656 
2024-05-08 17:08:21.896353: Pseudo dice [0.9234] 
2024-05-08 17:08:21.896406: Epoch time: 40.89 s 
2024-05-08 17:08:23.024269:  
2024-05-08 17:08:23.024400: Epoch 410 
2024-05-08 17:08:23.024493: Current learning rate: 0.00622 
2024-05-08 17:09:03.806201: train_loss -0.8492 
2024-05-08 17:09:03.806381: val_loss -0.8651 
2024-05-08 17:09:03.806431: Pseudo dice [0.9273] 
2024-05-08 17:09:03.806484: Epoch time: 40.78 s 
2024-05-08 17:09:04.863083:  
2024-05-08 17:09:04.863272: Epoch 411 
2024-05-08 17:09:04.863361: Current learning rate: 0.00621 
2024-05-08 17:09:46.273626: train_loss -0.8528 
2024-05-08 17:09:46.273814: val_loss -0.8608 
2024-05-08 17:09:46.273864: Pseudo dice [0.9203] 
2024-05-08 17:09:46.273924: Epoch time: 41.41 s 
2024-05-08 17:09:47.523358:  
2024-05-08 17:09:47.523600: Epoch 412 
2024-05-08 17:09:47.523693: Current learning rate: 0.0062 
2024-05-08 17:10:28.936228: train_loss -0.8502 
2024-05-08 17:10:28.936415: val_loss -0.8463 
2024-05-08 17:10:28.936463: Pseudo dice [0.9158] 
2024-05-08 17:10:28.936532: Epoch time: 41.41 s 
2024-05-08 17:10:30.006043:  
2024-05-08 17:10:30.006238: Epoch 413 
2024-05-08 17:10:30.006334: Current learning rate: 0.00619 
2024-05-08 17:11:10.793407: train_loss -0.839 
2024-05-08 17:11:10.793595: val_loss -0.8491 
2024-05-08 17:11:10.793644: Pseudo dice [0.923] 
2024-05-08 17:11:10.793698: Epoch time: 40.79 s 
2024-05-08 17:11:11.880993:  
2024-05-08 17:11:11.881270: Epoch 414 
2024-05-08 17:11:11.882263: Current learning rate: 0.00618 
2024-05-08 17:11:53.304532: train_loss -0.8289 
2024-05-08 17:11:53.304738: val_loss -0.8253 
2024-05-08 17:11:53.304790: Pseudo dice [0.9034] 
2024-05-08 17:11:53.304844: Epoch time: 41.42 s 
2024-05-08 17:11:54.374488:  
2024-05-08 17:11:54.374792: Epoch 415 
2024-05-08 17:11:54.374992: Current learning rate: 0.00617 
2024-05-08 17:12:35.375322: train_loss -0.8134 
2024-05-08 17:12:35.375502: val_loss -0.8433 
2024-05-08 17:12:35.375552: Pseudo dice [0.9152] 
2024-05-08 17:12:35.375604: Epoch time: 41.0 s 
2024-05-08 17:12:36.468640:  
2024-05-08 17:12:36.468786: Epoch 416 
2024-05-08 17:12:36.468883: Current learning rate: 0.00616 
2024-05-08 17:13:17.366672: train_loss -0.8239 
2024-05-08 17:13:17.366853: val_loss -0.8286 
2024-05-08 17:13:17.366901: Pseudo dice [0.9103] 
2024-05-08 17:13:17.366958: Epoch time: 40.9 s 
2024-05-08 17:13:18.435953:  
2024-05-08 17:13:18.436096: Epoch 417 
2024-05-08 17:13:18.436188: Current learning rate: 0.00615 
2024-05-08 17:13:59.292216: train_loss -0.8315 
2024-05-08 17:13:59.292404: val_loss -0.8481 
2024-05-08 17:13:59.292454: Pseudo dice [0.911] 
2024-05-08 17:13:59.292506: Epoch time: 40.86 s 
2024-05-08 17:14:00.349610:  
2024-05-08 17:14:00.349826: Epoch 418 
2024-05-08 17:14:00.349920: Current learning rate: 0.00614 
2024-05-08 17:14:41.992470: train_loss -0.8418 
2024-05-08 17:14:41.992668: val_loss -0.8466 
2024-05-08 17:14:41.992729: Pseudo dice [0.919] 
2024-05-08 17:14:41.992790: Epoch time: 41.64 s 
2024-05-08 17:14:43.330821:  
2024-05-08 17:14:43.330966: Epoch 419 
2024-05-08 17:14:43.331060: Current learning rate: 0.00613 
2024-05-08 17:15:24.437497: train_loss -0.8411 
2024-05-08 17:15:24.437679: val_loss -0.8455 
2024-05-08 17:15:24.437730: Pseudo dice [0.9194] 
2024-05-08 17:15:24.437782: Epoch time: 41.11 s 
2024-05-08 17:15:25.502314:  
2024-05-08 17:15:25.502462: Epoch 420 
2024-05-08 17:15:25.502561: Current learning rate: 0.00612 
2024-05-08 17:16:07.052525: train_loss -0.8477 
2024-05-08 17:16:07.052704: val_loss -0.8425 
2024-05-08 17:16:07.052758: Pseudo dice [0.9157] 
2024-05-08 17:16:07.052815: Epoch time: 41.55 s 
2024-05-08 17:16:08.139417:  
2024-05-08 17:16:08.139644: Epoch 421 
2024-05-08 17:16:08.139745: Current learning rate: 0.00612 
2024-05-08 17:16:49.179057: train_loss -0.8353 
2024-05-08 17:16:49.179227: val_loss -0.8508 
2024-05-08 17:16:49.179276: Pseudo dice [0.9215] 
2024-05-08 17:16:49.179333: Epoch time: 41.04 s 
2024-05-08 17:16:50.241140:  
2024-05-08 17:16:50.241292: Epoch 422 
2024-05-08 17:16:50.241389: Current learning rate: 0.00611 
2024-05-08 17:17:31.143495: train_loss -0.8434 
2024-05-08 17:17:31.143679: val_loss -0.8509 
2024-05-08 17:17:31.143734: Pseudo dice [0.9216] 
2024-05-08 17:17:31.143790: Epoch time: 40.9 s 
2024-05-08 17:17:32.196077:  
2024-05-08 17:17:32.196298: Epoch 423 
2024-05-08 17:17:32.196400: Current learning rate: 0.0061 
2024-05-08 17:18:13.016799: train_loss -0.8374 
2024-05-08 17:18:13.016982: val_loss -0.8549 
2024-05-08 17:18:13.017031: Pseudo dice [0.921] 
2024-05-08 17:18:13.017086: Epoch time: 40.82 s 
2024-05-08 17:18:14.075941:  
2024-05-08 17:18:14.076161: Epoch 424 
2024-05-08 17:18:14.076258: Current learning rate: 0.00609 
2024-05-08 17:18:54.889829: train_loss -0.8364 
2024-05-08 17:18:54.890043: val_loss -0.8228 
2024-05-08 17:18:54.890094: Pseudo dice [0.9077] 
2024-05-08 17:18:54.890147: Epoch time: 40.81 s 
2024-05-08 17:18:56.138878:  
2024-05-08 17:18:56.139023: Epoch 425 
2024-05-08 17:18:56.139113: Current learning rate: 0.00608 
2024-05-08 17:19:36.933835: train_loss -0.8273 
2024-05-08 17:19:36.934030: val_loss -0.8419 
2024-05-08 17:19:36.934080: Pseudo dice [0.9139] 
2024-05-08 17:19:36.934132: Epoch time: 40.8 s 
2024-05-08 17:19:37.992655:  
2024-05-08 17:19:37.992837: Epoch 426 
2024-05-08 17:19:37.992929: Current learning rate: 0.00607 
2024-05-08 17:20:18.774091: train_loss -0.8317 
2024-05-08 17:20:18.775614: val_loss -0.8422 
2024-05-08 17:20:18.775687: Pseudo dice [0.9128] 
2024-05-08 17:20:18.777062: Epoch time: 40.78 s 
2024-05-08 17:20:19.837941:  
2024-05-08 17:20:19.838077: Epoch 427 
2024-05-08 17:20:19.838181: Current learning rate: 0.00606 
2024-05-08 17:21:00.618209: train_loss -0.8297 
2024-05-08 17:21:00.618400: val_loss -0.865 
2024-05-08 17:21:00.618451: Pseudo dice [0.925] 
2024-05-08 17:21:00.618505: Epoch time: 40.78 s 
2024-05-08 17:21:01.680557:  
2024-05-08 17:21:01.680688: Epoch 428 
2024-05-08 17:21:01.680777: Current learning rate: 0.00605 
2024-05-08 17:21:42.497996: train_loss -0.8412 
2024-05-08 17:21:42.498177: val_loss -0.8507 
2024-05-08 17:21:42.498226: Pseudo dice [0.9192] 
2024-05-08 17:21:42.498279: Epoch time: 40.82 s 
2024-05-08 17:21:43.554622:  
2024-05-08 17:21:43.554738: Epoch 429 
2024-05-08 17:21:43.554831: Current learning rate: 0.00604 
2024-05-08 17:22:24.327976: train_loss -0.8483 
2024-05-08 17:22:24.328153: val_loss -0.8571 
2024-05-08 17:22:24.328205: Pseudo dice [0.9224] 
2024-05-08 17:22:24.328259: Epoch time: 40.77 s 
2024-05-08 17:22:25.387450:  
2024-05-08 17:22:25.387573: Epoch 430 
2024-05-08 17:22:25.387661: Current learning rate: 0.00603 
2024-05-08 17:23:06.575626: train_loss -0.8497 
2024-05-08 17:23:06.575813: val_loss -0.861 
2024-05-08 17:23:06.575863: Pseudo dice [0.9221] 
2024-05-08 17:23:06.575917: Epoch time: 41.19 s 
2024-05-08 17:23:07.909912:  
2024-05-08 17:23:07.910203: Epoch 431 
2024-05-08 17:23:07.910301: Current learning rate: 0.00602 
2024-05-08 17:23:49.189832: train_loss -0.85 
2024-05-08 17:23:49.190011: val_loss -0.852 
2024-05-08 17:23:49.190060: Pseudo dice [0.922] 
2024-05-08 17:23:49.190113: Epoch time: 41.28 s 
2024-05-08 17:23:50.252263:  
2024-05-08 17:23:50.252399: Epoch 432 
2024-05-08 17:23:50.252490: Current learning rate: 0.00601 
2024-05-08 17:24:31.318047: train_loss -0.8473 
2024-05-08 17:24:31.318237: val_loss -0.8524 
2024-05-08 17:24:31.318287: Pseudo dice [0.9159] 
2024-05-08 17:24:31.318340: Epoch time: 41.07 s 
2024-05-08 17:24:32.380262:  
2024-05-08 17:24:32.380400: Epoch 433 
2024-05-08 17:24:32.380489: Current learning rate: 0.006 
2024-05-08 17:25:13.937515: train_loss -0.8443 
2024-05-08 17:25:13.937699: val_loss -0.8671 
2024-05-08 17:25:13.937747: Pseudo dice [0.9263] 
2024-05-08 17:25:13.937801: Epoch time: 41.56 s 
2024-05-08 17:25:15.003314:  
2024-05-08 17:25:15.003447: Epoch 434 
2024-05-08 17:25:15.003540: Current learning rate: 0.00599 
2024-05-08 17:25:55.967127: train_loss -0.8586 
2024-05-08 17:25:55.967308: val_loss -0.8639 
2024-05-08 17:25:55.967356: Pseudo dice [0.9233] 
2024-05-08 17:25:55.967410: Epoch time: 40.96 s 
2024-05-08 17:25:57.031123:  
2024-05-08 17:25:57.031279: Epoch 435 
2024-05-08 17:25:57.031379: Current learning rate: 0.00598 
2024-05-08 17:26:37.901965: train_loss -0.8515 
2024-05-08 17:26:37.902146: val_loss -0.85 
2024-05-08 17:26:37.902195: Pseudo dice [0.9162] 
2024-05-08 17:26:37.902252: Epoch time: 40.87 s 
2024-05-08 17:26:38.960005:  
2024-05-08 17:26:38.960126: Epoch 436 
2024-05-08 17:26:38.960222: Current learning rate: 0.00597 
2024-05-08 17:27:19.829256: train_loss -0.8579 
2024-05-08 17:27:19.829435: val_loss -0.8603 
2024-05-08 17:27:19.829484: Pseudo dice [0.9224] 
2024-05-08 17:27:19.829543: Epoch time: 40.87 s 
2024-05-08 17:27:20.887981:  
2024-05-08 17:27:20.888117: Epoch 437 
2024-05-08 17:27:20.888220: Current learning rate: 0.00596 
2024-05-08 17:28:01.797341: train_loss -0.8481 
2024-05-08 17:28:01.797525: val_loss -0.8589 
2024-05-08 17:28:01.797574: Pseudo dice [0.9233] 
2024-05-08 17:28:01.797627: Epoch time: 40.91 s 
2024-05-08 17:28:03.056598:  
2024-05-08 17:28:03.056754: Epoch 438 
2024-05-08 17:28:03.056855: Current learning rate: 0.00595 
2024-05-08 17:28:43.974333: train_loss -0.8522 
2024-05-08 17:28:43.974513: val_loss -0.8493 
2024-05-08 17:28:43.974563: Pseudo dice [0.9169] 
2024-05-08 17:28:43.974619: Epoch time: 40.92 s 
2024-05-08 17:28:45.032122:  
2024-05-08 17:28:45.032383: Epoch 439 
2024-05-08 17:28:45.032481: Current learning rate: 0.00594 
2024-05-08 17:29:26.615042: train_loss -0.8431 
2024-05-08 17:29:26.615215: val_loss -0.8533 
2024-05-08 17:29:26.615264: Pseudo dice [0.9167] 
2024-05-08 17:29:26.615319: Epoch time: 41.58 s 
2024-05-08 17:29:27.691692:  
2024-05-08 17:29:27.691880: Epoch 440 
2024-05-08 17:29:27.692009: Current learning rate: 0.00593 
2024-05-08 17:30:08.784409: train_loss -0.842 
2024-05-08 17:30:08.784579: val_loss -0.8597 
2024-05-08 17:30:08.784634: Pseudo dice [0.9215] 
2024-05-08 17:30:08.784696: Epoch time: 41.09 s 
2024-05-08 17:30:09.852283:  
2024-05-08 17:30:09.852496: Epoch 441 
2024-05-08 17:30:09.852593: Current learning rate: 0.00592 
2024-05-08 17:30:50.936972: train_loss -0.847 
2024-05-08 17:30:50.937199: val_loss -0.8637 
2024-05-08 17:30:50.937250: Pseudo dice [0.9261] 
2024-05-08 17:30:50.937304: Epoch time: 41.09 s 
2024-05-08 17:30:52.009471:  
2024-05-08 17:30:52.009718: Epoch 442 
2024-05-08 17:30:52.009811: Current learning rate: 0.00592 
2024-05-08 17:31:33.538347: train_loss -0.8514 
2024-05-08 17:31:33.538528: val_loss -0.8625 
2024-05-08 17:31:33.538578: Pseudo dice [0.9253] 
2024-05-08 17:31:33.538635: Epoch time: 41.53 s 
2024-05-08 17:31:34.637632:  
2024-05-08 17:31:34.637764: Epoch 443 
2024-05-08 17:31:34.637858: Current learning rate: 0.00591 
2024-05-08 17:32:15.600852: train_loss -0.8578 
2024-05-08 17:32:15.601040: val_loss -0.8654 
2024-05-08 17:32:15.601089: Pseudo dice [0.9271] 
2024-05-08 17:32:15.601144: Epoch time: 40.96 s 
2024-05-08 17:32:16.851013:  
2024-05-08 17:32:16.851166: Epoch 444 
2024-05-08 17:32:16.851272: Current learning rate: 0.0059 
2024-05-08 17:32:57.831450: train_loss -0.8489 
2024-05-08 17:32:57.831635: val_loss -0.8501 
2024-05-08 17:32:57.831686: Pseudo dice [0.9164] 
2024-05-08 17:32:57.831741: Epoch time: 40.98 s 
2024-05-08 17:32:58.899017:  
2024-05-08 17:32:58.899401: Epoch 445 
2024-05-08 17:32:58.899498: Current learning rate: 0.00589 
2024-05-08 17:33:39.878977: train_loss -0.852 
2024-05-08 17:33:39.879293: val_loss -0.869 
2024-05-08 17:33:39.879339: Pseudo dice [0.9296] 
2024-05-08 17:33:39.879394: Epoch time: 40.98 s 
2024-05-08 17:33:40.972873:  
2024-05-08 17:33:40.973016: Epoch 446 
2024-05-08 17:33:40.973117: Current learning rate: 0.00588 
2024-05-08 17:34:21.962817: train_loss -0.8482 
2024-05-08 17:34:21.962997: val_loss -0.8613 
2024-05-08 17:34:21.963047: Pseudo dice [0.9232] 
2024-05-08 17:34:21.963099: Epoch time: 40.99 s 
2024-05-08 17:34:23.034606:  
2024-05-08 17:34:23.034747: Epoch 447 
2024-05-08 17:34:23.034839: Current learning rate: 0.00587 
2024-05-08 17:35:04.394785: train_loss -0.8548 
2024-05-08 17:35:04.394958: val_loss -0.8558 
2024-05-08 17:35:04.395009: Pseudo dice [0.9226] 
2024-05-08 17:35:04.395062: Epoch time: 41.36 s 
2024-05-08 17:35:05.546029:  
2024-05-08 17:35:05.546209: Epoch 448 
2024-05-08 17:35:05.546305: Current learning rate: 0.00586 
2024-05-08 17:35:46.667352: train_loss -0.8565 
2024-05-08 17:35:46.667541: val_loss -0.846 
2024-05-08 17:35:46.667590: Pseudo dice [0.9194] 
2024-05-08 17:35:46.667643: Epoch time: 41.12 s 
2024-05-08 17:35:47.750895:  
2024-05-08 17:35:47.751037: Epoch 449 
2024-05-08 17:35:47.751129: Current learning rate: 0.00585 
2024-05-08 17:36:29.279782: train_loss -0.854 
2024-05-08 17:36:29.279966: val_loss -0.8538 
2024-05-08 17:36:29.280015: Pseudo dice [0.9238] 
2024-05-08 17:36:29.280069: Epoch time: 41.53 s 
2024-05-08 17:36:30.671381:  
2024-05-08 17:36:30.671566: Epoch 450 
2024-05-08 17:36:30.671661: Current learning rate: 0.00584 
2024-05-08 17:37:11.734661: train_loss -0.8479 
2024-05-08 17:37:11.734848: val_loss -0.8704 
2024-05-08 17:37:11.734898: Pseudo dice [0.9263] 
2024-05-08 17:37:11.734953: Epoch time: 41.06 s 
2024-05-08 17:37:12.992983:  
2024-05-08 17:37:12.993134: Epoch 451 
2024-05-08 17:37:12.993234: Current learning rate: 0.00583 
2024-05-08 17:37:54.061569: train_loss -0.854 
2024-05-08 17:37:54.061755: val_loss -0.8596 
2024-05-08 17:37:54.061803: Pseudo dice [0.9221] 
2024-05-08 17:37:54.061856: Epoch time: 41.07 s 
2024-05-08 17:37:55.119532:  
2024-05-08 17:37:55.119676: Epoch 452 
2024-05-08 17:37:55.119769: Current learning rate: 0.00582 
2024-05-08 17:38:36.193355: train_loss -0.8428 
2024-05-08 17:38:36.193540: val_loss -0.8502 
2024-05-08 17:38:36.193588: Pseudo dice [0.9165] 
2024-05-08 17:38:36.193642: Epoch time: 41.07 s 
2024-05-08 17:38:37.251368:  
2024-05-08 17:38:37.251505: Epoch 453 
2024-05-08 17:38:37.251596: Current learning rate: 0.00581 
2024-05-08 17:39:18.308364: train_loss -0.8465 
2024-05-08 17:39:18.308547: val_loss -0.8484 
2024-05-08 17:39:18.308595: Pseudo dice [0.9222] 
2024-05-08 17:39:18.308654: Epoch time: 41.06 s 
2024-05-08 17:39:19.365908:  
2024-05-08 17:39:19.366049: Epoch 454 
2024-05-08 17:39:19.366141: Current learning rate: 0.0058 
2024-05-08 17:40:00.405068: train_loss -0.8522 
2024-05-08 17:40:00.405251: val_loss -0.8613 
2024-05-08 17:40:00.405299: Pseudo dice [0.9249] 
2024-05-08 17:40:00.405353: Epoch time: 41.04 s 
2024-05-08 17:40:01.459062:  
2024-05-08 17:40:01.459199: Epoch 455 
2024-05-08 17:40:01.459290: Current learning rate: 0.00579 
2024-05-08 17:40:42.517620: train_loss -0.8526 
2024-05-08 17:40:42.517833: val_loss -0.8632 
2024-05-08 17:40:42.517913: Pseudo dice [0.9246] 
2024-05-08 17:40:42.517969: Epoch time: 41.06 s 
2024-05-08 17:40:43.567971:  
2024-05-08 17:40:43.568106: Epoch 456 
2024-05-08 17:40:43.568199: Current learning rate: 0.00578 
2024-05-08 17:41:25.070749: train_loss -0.8514 
2024-05-08 17:41:25.070987: val_loss -0.8591 
2024-05-08 17:41:25.071060: Pseudo dice [0.9239] 
2024-05-08 17:41:25.071143: Epoch time: 41.5 s 
2024-05-08 17:41:26.488419:  
2024-05-08 17:41:26.488613: Epoch 457 
2024-05-08 17:41:26.488717: Current learning rate: 0.00577 
2024-05-08 17:42:07.563387: train_loss -0.8599 
2024-05-08 17:42:07.563564: val_loss -0.8649 
2024-05-08 17:42:07.563613: Pseudo dice [0.9222] 
2024-05-08 17:42:07.563664: Epoch time: 41.08 s 
2024-05-08 17:42:08.816314:  
2024-05-08 17:42:08.816499: Epoch 458 
2024-05-08 17:42:08.816641: Current learning rate: 0.00576 
2024-05-08 17:42:49.852626: train_loss -0.8559 
2024-05-08 17:42:49.852823: val_loss -0.8578 
2024-05-08 17:42:49.852873: Pseudo dice [0.9259] 
2024-05-08 17:42:49.852927: Epoch time: 41.04 s 
2024-05-08 17:42:49.852971: Yayy! New best EMA pseudo Dice: 0.9228 
2024-05-08 17:42:51.226220:  
2024-05-08 17:42:51.226383: Epoch 459 
2024-05-08 17:42:51.226477: Current learning rate: 0.00575 
2024-05-08 17:43:32.254982: train_loss -0.8572 
2024-05-08 17:43:32.255163: val_loss -0.8755 
2024-05-08 17:43:32.255211: Pseudo dice [0.9317] 
2024-05-08 17:43:32.255265: Epoch time: 41.03 s 
2024-05-08 17:43:32.255308: Yayy! New best EMA pseudo Dice: 0.9237 
2024-05-08 17:43:33.653002:  
2024-05-08 17:43:33.653222: Epoch 460 
2024-05-08 17:43:33.653333: Current learning rate: 0.00574 
2024-05-08 17:44:15.478417: train_loss -0.8554 
2024-05-08 17:44:15.478605: val_loss -0.8634 
2024-05-08 17:44:15.478654: Pseudo dice [0.9229] 
2024-05-08 17:44:15.478708: Epoch time: 41.83 s 
2024-05-08 17:44:16.574074:  
2024-05-08 17:44:16.574227: Epoch 461 
2024-05-08 17:44:16.574318: Current learning rate: 0.00573 
2024-05-08 17:44:57.556946: train_loss -0.8574 
2024-05-08 17:44:57.557140: val_loss -0.8704 
2024-05-08 17:44:57.557191: Pseudo dice [0.9288] 
2024-05-08 17:44:57.557245: Epoch time: 40.98 s 
2024-05-08 17:44:57.557288: Yayy! New best EMA pseudo Dice: 0.9241 
2024-05-08 17:44:58.992058:  
2024-05-08 17:44:58.992248: Epoch 462 
2024-05-08 17:44:58.992352: Current learning rate: 0.00572 
2024-05-08 17:45:40.057871: train_loss -0.8517 
2024-05-08 17:45:40.058050: val_loss -0.8487 
2024-05-08 17:45:40.058100: Pseudo dice [0.923] 
2024-05-08 17:45:40.058152: Epoch time: 41.07 s 
2024-05-08 17:45:41.117044:  
2024-05-08 17:45:41.117179: Epoch 463 
2024-05-08 17:45:41.117275: Current learning rate: 0.00571 
2024-05-08 17:46:22.188682: train_loss -0.8586 
2024-05-08 17:46:22.188874: val_loss -0.8619 
2024-05-08 17:46:22.188926: Pseudo dice [0.924] 
2024-05-08 17:46:22.188981: Epoch time: 41.07 s 
2024-05-08 17:46:23.435669:  
2024-05-08 17:46:23.435817: Epoch 464 
2024-05-08 17:46:23.435910: Current learning rate: 0.0057 
2024-05-08 17:47:04.513215: train_loss -0.8462 
2024-05-08 17:47:04.513400: val_loss -0.8542 
2024-05-08 17:47:04.513448: Pseudo dice [0.9244] 
2024-05-08 17:47:04.513500: Epoch time: 41.08 s 
2024-05-08 17:47:05.573551:  
2024-05-08 17:47:05.573748: Epoch 465 
2024-05-08 17:47:05.573844: Current learning rate: 0.0057 
2024-05-08 17:47:46.607803: train_loss -0.851 
2024-05-08 17:47:46.607976: val_loss -0.8745 
2024-05-08 17:47:46.608025: Pseudo dice [0.9321] 
2024-05-08 17:47:46.608078: Epoch time: 41.04 s 
2024-05-08 17:47:46.608122: Yayy! New best EMA pseudo Dice: 0.9249 
2024-05-08 17:47:48.016653:  
2024-05-08 17:47:48.016805: Epoch 466 
2024-05-08 17:47:48.016906: Current learning rate: 0.00569 
2024-05-08 17:48:29.043207: train_loss -0.8589 
2024-05-08 17:48:29.043398: val_loss -0.8377 
2024-05-08 17:48:29.043448: Pseudo dice [0.9134] 
2024-05-08 17:48:29.043501: Epoch time: 41.03 s 
2024-05-08 17:48:30.104990:  
2024-05-08 17:48:30.105118: Epoch 467 
2024-05-08 17:48:30.105212: Current learning rate: 0.00568 
2024-05-08 17:49:11.142091: train_loss -0.8516 
2024-05-08 17:49:11.142282: val_loss -0.864 
2024-05-08 17:49:11.142333: Pseudo dice [0.9271] 
2024-05-08 17:49:11.142386: Epoch time: 41.04 s 
2024-05-08 17:49:12.201061:  
2024-05-08 17:49:12.201257: Epoch 468 
2024-05-08 17:49:12.201358: Current learning rate: 0.00567 
2024-05-08 17:49:53.241926: train_loss -0.8525 
2024-05-08 17:49:53.242101: val_loss -0.8708 
2024-05-08 17:49:53.242150: Pseudo dice [0.929] 
2024-05-08 17:49:53.242203: Epoch time: 41.04 s 
2024-05-08 17:49:54.299850:  
2024-05-08 17:49:54.299975: Epoch 469 
2024-05-08 17:49:54.300070: Current learning rate: 0.00566 
2024-05-08 17:50:35.901657: train_loss -0.8575 
2024-05-08 17:50:35.901870: val_loss -0.8725 
2024-05-08 17:50:35.901920: Pseudo dice [0.9286] 
2024-05-08 17:50:35.901973: Epoch time: 41.6 s 
2024-05-08 17:50:35.902016: Yayy! New best EMA pseudo Dice: 0.9249 
2024-05-08 17:50:37.481559:  
2024-05-08 17:50:37.481843: Epoch 470 
2024-05-08 17:50:37.481966: Current learning rate: 0.00565 
2024-05-08 17:51:18.475544: train_loss -0.8509 
2024-05-08 17:51:18.475767: val_loss -0.8623 
2024-05-08 17:51:18.475817: Pseudo dice [0.9254] 
2024-05-08 17:51:18.475869: Epoch time: 41.0 s 
2024-05-08 17:51:18.475913: Yayy! New best EMA pseudo Dice: 0.925 
2024-05-08 17:51:19.882655:  
2024-05-08 17:51:19.882794: Epoch 471 
2024-05-08 17:51:19.882887: Current learning rate: 0.00564 
2024-05-08 17:52:00.917908: train_loss -0.8413 
2024-05-08 17:52:00.918096: val_loss -0.8525 
2024-05-08 17:52:00.918164: Pseudo dice [0.9152] 
2024-05-08 17:52:00.918227: Epoch time: 41.04 s 
2024-05-08 17:52:01.984470:  
2024-05-08 17:52:01.984637: Epoch 472 
2024-05-08 17:52:01.984739: Current learning rate: 0.00563 
2024-05-08 17:52:43.056398: train_loss -0.8557 
2024-05-08 17:52:43.056567: val_loss -0.8639 
2024-05-08 17:52:43.056616: Pseudo dice [0.9258] 
2024-05-08 17:52:43.056676: Epoch time: 41.07 s 
2024-05-08 17:52:44.118552:  
2024-05-08 17:52:44.118673: Epoch 473 
2024-05-08 17:52:44.118773: Current learning rate: 0.00562 
2024-05-08 17:53:25.850449: train_loss -0.8421 
2024-05-08 17:53:25.850793: val_loss -0.8477 
2024-05-08 17:53:25.850945: Pseudo dice [0.9199] 
2024-05-08 17:53:25.851138: Epoch time: 41.73 s 
2024-05-08 17:53:26.928379:  
2024-05-08 17:53:26.928587: Epoch 474 
2024-05-08 17:53:26.928686: Current learning rate: 0.00561 
2024-05-08 17:54:08.092427: train_loss -0.8482 
2024-05-08 17:54:08.092608: val_loss -0.8665 
2024-05-08 17:54:08.092662: Pseudo dice [0.927] 
2024-05-08 17:54:08.092717: Epoch time: 41.17 s 
2024-05-08 17:54:09.148661:  
2024-05-08 17:54:09.148800: Epoch 475 
2024-05-08 17:54:09.148902: Current learning rate: 0.0056 
2024-05-08 17:54:50.349829: train_loss -0.8507 
2024-05-08 17:54:50.350039: val_loss -0.8646 
2024-05-08 17:54:50.350090: Pseudo dice [0.9276] 
2024-05-08 17:54:50.350146: Epoch time: 41.2 s 
2024-05-08 17:54:51.416295:  
2024-05-08 17:54:51.416428: Epoch 476 
2024-05-08 17:54:51.416518: Current learning rate: 0.00559 
2024-05-08 17:55:32.490112: train_loss -0.8611 
2024-05-08 17:55:32.490309: val_loss -0.8603 
2024-05-08 17:55:32.490359: Pseudo dice [0.9249] 
2024-05-08 17:55:32.490412: Epoch time: 41.07 s 
2024-05-08 17:55:33.747575:  
2024-05-08 17:55:33.747729: Epoch 477 
2024-05-08 17:55:33.747824: Current learning rate: 0.00558 
2024-05-08 17:56:14.851568: train_loss -0.8536 
2024-05-08 17:56:14.851775: val_loss -0.8595 
2024-05-08 17:56:14.851825: Pseudo dice [0.9249] 
2024-05-08 17:56:14.851882: Epoch time: 41.11 s 
2024-05-08 17:56:15.924882:  
2024-05-08 17:56:15.925034: Epoch 478 
2024-05-08 17:56:15.925128: Current learning rate: 0.00557 
2024-05-08 17:56:56.994147: train_loss -0.8603 
2024-05-08 17:56:56.994327: val_loss -0.8628 
2024-05-08 17:56:56.994377: Pseudo dice [0.9275] 
2024-05-08 17:56:56.994432: Epoch time: 41.07 s 
2024-05-08 17:56:58.065125:  
2024-05-08 17:56:58.065305: Epoch 479 
2024-05-08 17:56:58.065400: Current learning rate: 0.00556 
2024-05-08 17:57:39.135791: train_loss -0.8572 
2024-05-08 17:57:39.135983: val_loss -0.8611 
2024-05-08 17:57:39.136033: Pseudo dice [0.9243] 
2024-05-08 17:57:39.136087: Epoch time: 41.07 s 
2024-05-08 17:57:40.212825:  
2024-05-08 17:57:40.212972: Epoch 480 
2024-05-08 17:57:40.213064: Current learning rate: 0.00555 
2024-05-08 17:58:21.281568: train_loss -0.8562 
2024-05-08 17:58:21.281752: val_loss -0.8655 
2024-05-08 17:58:21.281800: Pseudo dice [0.9263] 
2024-05-08 17:58:21.281854: Epoch time: 41.07 s 
2024-05-08 17:58:22.352481:  
2024-05-08 17:58:22.352737: Epoch 481 
2024-05-08 17:58:22.352827: Current learning rate: 0.00554 
2024-05-08 17:59:03.433802: train_loss -0.853 
2024-05-08 17:59:03.433988: val_loss -0.8586 
2024-05-08 17:59:03.434037: Pseudo dice [0.9247] 
2024-05-08 17:59:03.434091: Epoch time: 41.08 s 
2024-05-08 17:59:04.503451:  
2024-05-08 17:59:04.503640: Epoch 482 
2024-05-08 17:59:04.503739: Current learning rate: 0.00553 
2024-05-08 17:59:45.520797: train_loss -0.8559 
2024-05-08 17:59:45.520976: val_loss -0.863 
2024-05-08 17:59:45.521025: Pseudo dice [0.9271] 
2024-05-08 17:59:45.521079: Epoch time: 41.02 s 
2024-05-08 17:59:45.521122: Yayy! New best EMA pseudo Dice: 0.9251 
2024-05-08 17:59:46.937083:  
2024-05-08 17:59:46.937350: Epoch 483 
2024-05-08 17:59:46.937449: Current learning rate: 0.00552 
2024-05-08 18:00:27.929274: train_loss -0.8515 
2024-05-08 18:00:27.929460: val_loss -0.8507 
2024-05-08 18:00:27.929509: Pseudo dice [0.9205] 
2024-05-08 18:00:27.929562: Epoch time: 40.99 s 
2024-05-08 18:00:29.197768:  
2024-05-08 18:00:29.197922: Epoch 484 
2024-05-08 18:00:29.198025: Current learning rate: 0.00551 
2024-05-08 18:01:10.172283: train_loss -0.852 
2024-05-08 18:01:10.172471: val_loss -0.8651 
2024-05-08 18:01:10.172519: Pseudo dice [0.9278] 
2024-05-08 18:01:10.172573: Epoch time: 40.98 s 
2024-05-08 18:01:11.243685:  
2024-05-08 18:01:11.243874: Epoch 485 
2024-05-08 18:01:11.243967: Current learning rate: 0.0055 
2024-05-08 18:01:52.587781: train_loss -0.851 
2024-05-08 18:01:52.587986: val_loss -0.8517 
2024-05-08 18:01:52.588035: Pseudo dice [0.9214] 
2024-05-08 18:01:52.588089: Epoch time: 41.35 s 
2024-05-08 18:01:53.691919:  
2024-05-08 18:01:53.692302: Epoch 486 
2024-05-08 18:01:53.692402: Current learning rate: 0.00549 
2024-05-08 18:02:34.841361: train_loss -0.8471 
2024-05-08 18:02:34.841545: val_loss -0.858 
2024-05-08 18:02:34.841594: Pseudo dice [0.9192] 
2024-05-08 18:02:34.841646: Epoch time: 41.15 s 
2024-05-08 18:02:35.915127:  
2024-05-08 18:02:35.915259: Epoch 487 
2024-05-08 18:02:35.915357: Current learning rate: 0.00548 
2024-05-08 18:03:16.729681: train_loss -0.8517 
2024-05-08 18:03:16.729867: val_loss -0.853 
2024-05-08 18:03:16.729916: Pseudo dice [0.9231] 
2024-05-08 18:03:16.729980: Epoch time: 40.82 s 
2024-05-08 18:03:17.796210:  
2024-05-08 18:03:17.796385: Epoch 488 
2024-05-08 18:03:17.796479: Current learning rate: 0.00547 
2024-05-08 18:03:58.611654: train_loss -0.8499 
2024-05-08 18:03:58.611851: val_loss -0.8554 
2024-05-08 18:03:58.611902: Pseudo dice [0.9235] 
2024-05-08 18:03:58.611955: Epoch time: 40.82 s 
2024-05-08 18:03:59.682972:  
2024-05-08 18:03:59.683109: Epoch 489 
2024-05-08 18:03:59.683202: Current learning rate: 0.00546 
2024-05-08 18:04:40.473063: train_loss -0.8499 
2024-05-08 18:04:40.473310: val_loss -0.8658 
2024-05-08 18:04:40.473361: Pseudo dice [0.9251] 
2024-05-08 18:04:40.473413: Epoch time: 40.79 s 
2024-05-08 18:04:41.754488:  
2024-05-08 18:04:41.754614: Epoch 490 
2024-05-08 18:04:41.754716: Current learning rate: 0.00546 
2024-05-08 18:05:22.625764: train_loss -0.8545 
2024-05-08 18:05:22.625950: val_loss -0.8566 
2024-05-08 18:05:22.625999: Pseudo dice [0.9227] 
2024-05-08 18:05:22.626050: Epoch time: 40.87 s 
2024-05-08 18:05:23.690303:  
2024-05-08 18:05:23.690454: Epoch 491 
2024-05-08 18:05:23.690556: Current learning rate: 0.00545 
2024-05-08 18:06:04.557193: train_loss -0.8537 
2024-05-08 18:06:04.557375: val_loss -0.8749 
2024-05-08 18:06:04.557423: Pseudo dice [0.9317] 
2024-05-08 18:06:04.557475: Epoch time: 40.87 s 
2024-05-08 18:06:05.627007:  
2024-05-08 18:06:05.627144: Epoch 492 
2024-05-08 18:06:05.627239: Current learning rate: 0.00544 
2024-05-08 18:06:46.470647: train_loss -0.8588 
2024-05-08 18:06:46.470829: val_loss -0.8589 
2024-05-08 18:06:46.470879: Pseudo dice [0.9243] 
2024-05-08 18:06:46.470932: Epoch time: 40.84 s 
2024-05-08 18:06:47.534861:  
2024-05-08 18:06:47.535043: Epoch 493 
2024-05-08 18:06:47.535159: Current learning rate: 0.00543 
2024-05-08 18:07:28.363489: train_loss -0.8512 
2024-05-08 18:07:28.363684: val_loss -0.8671 
2024-05-08 18:07:28.363734: Pseudo dice [0.929] 
2024-05-08 18:07:28.363786: Epoch time: 40.83 s 
2024-05-08 18:07:29.425745:  
2024-05-08 18:07:29.425870: Epoch 494 
2024-05-08 18:07:29.425965: Current learning rate: 0.00542 
2024-05-08 18:08:10.281646: train_loss -0.8456 
2024-05-08 18:08:10.281833: val_loss -0.8465 
2024-05-08 18:08:10.281883: Pseudo dice [0.919] 
2024-05-08 18:08:10.281937: Epoch time: 40.86 s 
2024-05-08 18:08:11.367203:  
2024-05-08 18:08:11.367415: Epoch 495 
2024-05-08 18:08:11.367512: Current learning rate: 0.00541 
2024-05-08 18:08:52.259508: train_loss -0.8365 
2024-05-08 18:08:52.259833: val_loss -0.8483 
2024-05-08 18:08:52.259891: Pseudo dice [0.9152] 
2024-05-08 18:08:52.259944: Epoch time: 40.89 s 
2024-05-08 18:08:53.321818:  
2024-05-08 18:08:53.321940: Epoch 496 
2024-05-08 18:08:53.322032: Current learning rate: 0.0054 
2024-05-08 18:09:34.213870: train_loss -0.845 
2024-05-08 18:09:34.214054: val_loss -0.8428 
2024-05-08 18:09:34.214109: Pseudo dice [0.92] 
2024-05-08 18:09:34.214161: Epoch time: 40.89 s 
2024-05-08 18:09:35.477531:  
2024-05-08 18:09:35.477671: Epoch 497 
2024-05-08 18:09:35.477764: Current learning rate: 0.00539 
2024-05-08 18:10:16.378055: train_loss -0.8399 
2024-05-08 18:10:16.378238: val_loss -0.8483 
2024-05-08 18:10:16.378287: Pseudo dice [0.9182] 
2024-05-08 18:10:16.378338: Epoch time: 40.9 s 
2024-05-08 18:10:17.450913:  
2024-05-08 18:10:17.451055: Epoch 498 
2024-05-08 18:10:17.451151: Current learning rate: 0.00538 
2024-05-08 18:10:58.364498: train_loss -0.847 
2024-05-08 18:10:58.364694: val_loss -0.8542 
2024-05-08 18:10:58.364743: Pseudo dice [0.9216] 
2024-05-08 18:10:58.364797: Epoch time: 40.91 s 
2024-05-08 18:10:59.433194:  
2024-05-08 18:10:59.433377: Epoch 499 
2024-05-08 18:10:59.433470: Current learning rate: 0.00537 
2024-05-08 18:11:40.356936: train_loss -0.844 
2024-05-08 18:11:40.357130: val_loss -0.8552 
2024-05-08 18:11:40.357179: Pseudo dice [0.9222] 
2024-05-08 18:11:40.357234: Epoch time: 40.92 s 
2024-05-08 18:11:41.786829:  
2024-05-08 18:11:41.787012: Epoch 500 
2024-05-08 18:11:41.787111: Current learning rate: 0.00536 
2024-05-08 18:12:22.693216: train_loss -0.856 
2024-05-08 18:12:22.693400: val_loss -0.8669 
2024-05-08 18:12:22.693450: Pseudo dice [0.9263] 
2024-05-08 18:12:22.693503: Epoch time: 40.91 s 
2024-05-08 18:12:23.762342:  
2024-05-08 18:12:23.762465: Epoch 501 
2024-05-08 18:12:23.762556: Current learning rate: 0.00535 
2024-05-08 18:13:04.656180: train_loss -0.8518 
2024-05-08 18:13:04.656355: val_loss -0.8613 
2024-05-08 18:13:04.656405: Pseudo dice [0.9257] 
2024-05-08 18:13:04.656457: Epoch time: 40.89 s 
2024-05-08 18:13:05.727370:  
2024-05-08 18:13:05.727500: Epoch 502 
2024-05-08 18:13:05.727593: Current learning rate: 0.00534 
2024-05-08 18:13:46.617272: train_loss -0.8645 
2024-05-08 18:13:46.617458: val_loss -0.8689 
2024-05-08 18:13:46.617507: Pseudo dice [0.93] 
2024-05-08 18:13:46.617560: Epoch time: 40.89 s 
2024-05-08 18:13:47.886192:  
2024-05-08 18:13:47.886338: Epoch 503 
2024-05-08 18:13:47.886445: Current learning rate: 0.00533 
2024-05-08 18:14:28.798586: train_loss -0.8543 
2024-05-08 18:14:28.798765: val_loss -0.8628 
2024-05-08 18:14:28.798817: Pseudo dice [0.926] 
2024-05-08 18:14:28.798872: Epoch time: 40.91 s 
2024-05-08 18:14:29.855564:  
2024-05-08 18:14:29.855708: Epoch 504 
2024-05-08 18:14:29.855798: Current learning rate: 0.00532 
2024-05-08 18:15:11.315995: train_loss -0.8614 
2024-05-08 18:15:11.316183: val_loss -0.8636 
2024-05-08 18:15:11.316234: Pseudo dice [0.9239] 
2024-05-08 18:15:11.316287: Epoch time: 41.46 s 
2024-05-08 18:15:12.384967:  
2024-05-08 18:15:12.385164: Epoch 505 
2024-05-08 18:15:12.385278: Current learning rate: 0.00531 
2024-05-08 18:15:53.445408: train_loss -0.8549 
2024-05-08 18:15:53.445593: val_loss -0.8641 
2024-05-08 18:15:53.445642: Pseudo dice [0.924] 
2024-05-08 18:15:53.445697: Epoch time: 41.06 s 
2024-05-08 18:15:54.529651:  
2024-05-08 18:15:54.529874: Epoch 506 
2024-05-08 18:15:54.529968: Current learning rate: 0.0053 
2024-05-08 18:16:35.530627: train_loss -0.8614 
2024-05-08 18:16:35.530801: val_loss -0.8664 
2024-05-08 18:16:35.530850: Pseudo dice [0.9273] 
2024-05-08 18:16:35.530903: Epoch time: 41.0 s 
2024-05-08 18:16:36.607476:  
2024-05-08 18:16:36.607729: Epoch 507 
2024-05-08 18:16:36.607824: Current learning rate: 0.00529 
2024-05-08 18:17:17.591732: train_loss -0.8566 
2024-05-08 18:17:17.592034: val_loss -0.8537 
2024-05-08 18:17:17.592087: Pseudo dice [0.9231] 
2024-05-08 18:17:17.592140: Epoch time: 40.99 s 
2024-05-08 18:17:18.684420:  
2024-05-08 18:17:18.684613: Epoch 508 
2024-05-08 18:17:18.684716: Current learning rate: 0.00528 
2024-05-08 18:17:59.658404: train_loss -0.8576 
2024-05-08 18:17:59.658577: val_loss -0.8603 
2024-05-08 18:17:59.658627: Pseudo dice [0.9234] 
2024-05-08 18:17:59.658679: Epoch time: 40.98 s 
2024-05-08 18:18:00.930342:  
2024-05-08 18:18:00.930492: Epoch 509 
2024-05-08 18:18:00.930599: Current learning rate: 0.00527 
2024-05-08 18:18:41.903014: train_loss -0.8527 
2024-05-08 18:18:41.903198: val_loss -0.8627 
2024-05-08 18:18:41.903246: Pseudo dice [0.9244] 
2024-05-08 18:18:41.903299: Epoch time: 40.97 s 
2024-05-08 18:18:42.981719:  
2024-05-08 18:18:42.981866: Epoch 510 
2024-05-08 18:18:42.981957: Current learning rate: 0.00526 
2024-05-08 18:19:23.971127: train_loss -0.8548 
2024-05-08 18:19:23.971310: val_loss -0.879 
2024-05-08 18:19:23.971359: Pseudo dice [0.9356] 
2024-05-08 18:19:23.971411: Epoch time: 40.99 s 
2024-05-08 18:19:23.971454: Yayy! New best EMA pseudo Dice: 0.9253 
2024-05-08 18:19:25.406494:  
2024-05-08 18:19:25.406696: Epoch 511 
2024-05-08 18:19:25.406827: Current learning rate: 0.00525 
2024-05-08 18:20:06.378098: train_loss -0.8588 
2024-05-08 18:20:06.378283: val_loss -0.863 
2024-05-08 18:20:06.378333: Pseudo dice [0.9236] 
2024-05-08 18:20:06.378386: Epoch time: 40.97 s 
2024-05-08 18:20:07.460364:  
2024-05-08 18:20:07.460506: Epoch 512 
2024-05-08 18:20:07.460596: Current learning rate: 0.00524 
2024-05-08 18:20:48.425090: train_loss -0.8532 
2024-05-08 18:20:48.425270: val_loss -0.8614 
2024-05-08 18:20:48.425330: Pseudo dice [0.9245] 
2024-05-08 18:20:48.425392: Epoch time: 40.97 s 
2024-05-08 18:20:49.507807:  
2024-05-08 18:20:49.507955: Epoch 513 
2024-05-08 18:20:49.508050: Current learning rate: 0.00523 
2024-05-08 18:21:30.487100: train_loss -0.8618 
2024-05-08 18:21:30.487284: val_loss -0.8599 
2024-05-08 18:21:30.487334: Pseudo dice [0.9228] 
2024-05-08 18:21:30.487386: Epoch time: 40.98 s 
2024-05-08 18:21:31.571939:  
2024-05-08 18:21:31.572075: Epoch 514 
2024-05-08 18:21:31.572166: Current learning rate: 0.00522 
2024-05-08 18:22:12.545529: train_loss -0.8616 
2024-05-08 18:22:12.545708: val_loss -0.8687 
2024-05-08 18:22:12.545757: Pseudo dice [0.9288] 
2024-05-08 18:22:12.545809: Epoch time: 40.97 s 
2024-05-08 18:22:13.631259:  
2024-05-08 18:22:13.631392: Epoch 515 
2024-05-08 18:22:13.631482: Current learning rate: 0.00521 
2024-05-08 18:22:54.595691: train_loss -0.8631 
2024-05-08 18:22:54.595871: val_loss -0.8674 
2024-05-08 18:22:54.595921: Pseudo dice [0.9301] 
2024-05-08 18:22:54.595974: Epoch time: 40.97 s 
2024-05-08 18:22:54.596016: Yayy! New best EMA pseudo Dice: 0.9257 
2024-05-08 18:22:56.229475:  
2024-05-08 18:22:56.229712: Epoch 516 
2024-05-08 18:22:56.229842: Current learning rate: 0.0052 
2024-05-08 18:23:37.219974: train_loss -0.8613 
2024-05-08 18:23:37.220180: val_loss -0.8668 
2024-05-08 18:23:37.220230: Pseudo dice [0.9272] 
2024-05-08 18:23:37.220283: Epoch time: 40.99 s 
2024-05-08 18:23:37.220325: Yayy! New best EMA pseudo Dice: 0.9259 
2024-05-08 18:23:38.654337:  
2024-05-08 18:23:38.654484: Epoch 517 
2024-05-08 18:23:38.654576: Current learning rate: 0.00519 
2024-05-08 18:24:19.705648: train_loss -0.8599 
2024-05-08 18:24:19.705855: val_loss -0.8667 
2024-05-08 18:24:19.705904: Pseudo dice [0.9248] 
2024-05-08 18:24:19.706085: Epoch time: 41.05 s 
2024-05-08 18:24:20.788596:  
2024-05-08 18:24:20.788757: Epoch 518 
2024-05-08 18:24:20.788859: Current learning rate: 0.00518 
2024-05-08 18:25:01.827453: train_loss -0.8647 
2024-05-08 18:25:01.827651: val_loss -0.8692 
2024-05-08 18:25:01.827701: Pseudo dice [0.9281] 
2024-05-08 18:25:01.827754: Epoch time: 41.04 s 
2024-05-08 18:25:01.827797: Yayy! New best EMA pseudo Dice: 0.926 
2024-05-08 18:25:03.264443:  
2024-05-08 18:25:03.264677: Epoch 519 
2024-05-08 18:25:03.264783: Current learning rate: 0.00518 
2024-05-08 18:25:44.273139: train_loss -0.86 
2024-05-08 18:25:44.273314: val_loss -0.8767 
2024-05-08 18:25:44.273363: Pseudo dice [0.9311] 
2024-05-08 18:25:44.273416: Epoch time: 41.01 s 
2024-05-08 18:25:44.273460: Yayy! New best EMA pseudo Dice: 0.9265 
2024-05-08 18:25:45.704895:  
2024-05-08 18:25:45.705024: Epoch 520 
2024-05-08 18:25:45.705116: Current learning rate: 0.00517 
2024-05-08 18:26:26.742136: train_loss -0.8625 
2024-05-08 18:26:26.742306: val_loss -0.8733 
2024-05-08 18:26:26.742356: Pseudo dice [0.9318] 
2024-05-08 18:26:26.742410: Epoch time: 41.04 s 
2024-05-08 18:26:26.742453: Yayy! New best EMA pseudo Dice: 0.927 
2024-05-08 18:26:28.336472:  
2024-05-08 18:26:28.336620: Epoch 521 
2024-05-08 18:26:28.336718: Current learning rate: 0.00516 
2024-05-08 18:27:09.404621: train_loss -0.8612 
2024-05-08 18:27:09.404829: val_loss -0.8611 
2024-05-08 18:27:09.404880: Pseudo dice [0.9242] 
2024-05-08 18:27:09.404933: Epoch time: 41.07 s 
2024-05-08 18:27:10.488858:  
2024-05-08 18:27:10.489005: Epoch 522 
2024-05-08 18:27:10.489096: Current learning rate: 0.00515 
2024-05-08 18:27:51.520236: train_loss -0.8504 
2024-05-08 18:27:51.520417: val_loss -0.867 
2024-05-08 18:27:51.520468: Pseudo dice [0.931] 
2024-05-08 18:27:51.520521: Epoch time: 41.03 s 
2024-05-08 18:27:51.520563: Yayy! New best EMA pseudo Dice: 0.9272 
2024-05-08 18:27:52.950420:  
2024-05-08 18:27:52.950566: Epoch 523 
2024-05-08 18:27:52.950663: Current learning rate: 0.00514 
2024-05-08 18:28:33.958585: train_loss -0.8538 
2024-05-08 18:28:33.958771: val_loss -0.8666 
2024-05-08 18:28:33.958823: Pseudo dice [0.9254] 
2024-05-08 18:28:33.958877: Epoch time: 41.01 s 
2024-05-08 18:28:35.046995:  
2024-05-08 18:28:35.047140: Epoch 524 
2024-05-08 18:28:35.047233: Current learning rate: 0.00513 
2024-05-08 18:29:16.071426: train_loss -0.8623 
2024-05-08 18:29:16.071619: val_loss -0.8575 
2024-05-08 18:29:16.071669: Pseudo dice [0.9198] 
2024-05-08 18:29:16.071721: Epoch time: 41.03 s 
2024-05-08 18:29:17.162981:  
2024-05-08 18:29:17.163290: Epoch 525 
2024-05-08 18:29:17.163514: Current learning rate: 0.00512 
2024-05-08 18:29:58.153286: train_loss -0.8617 
2024-05-08 18:29:58.153468: val_loss -0.8569 
2024-05-08 18:29:58.153523: Pseudo dice [0.9242] 
2024-05-08 18:29:58.153578: Epoch time: 40.99 s 
2024-05-08 18:29:59.232624:  
2024-05-08 18:29:59.232764: Epoch 526 
2024-05-08 18:29:59.232860: Current learning rate: 0.00511 
2024-05-08 18:30:40.273137: train_loss -0.8566 
2024-05-08 18:30:40.273316: val_loss -0.8867 
2024-05-08 18:30:40.273365: Pseudo dice [0.9377] 
2024-05-08 18:30:40.273417: Epoch time: 41.04 s 
2024-05-08 18:30:40.273460: Yayy! New best EMA pseudo Dice: 0.9272 
2024-05-08 18:30:41.886184:  
2024-05-08 18:30:41.886334: Epoch 527 
2024-05-08 18:30:41.886426: Current learning rate: 0.0051 
2024-05-08 18:31:22.983347: train_loss -0.8473 
2024-05-08 18:31:22.983536: val_loss -0.8531 
2024-05-08 18:31:22.983588: Pseudo dice [0.9222] 
2024-05-08 18:31:22.983641: Epoch time: 41.1 s 
2024-05-08 18:31:24.070406:  
2024-05-08 18:31:24.070678: Epoch 528 
2024-05-08 18:31:24.070776: Current learning rate: 0.00509 
2024-05-08 18:32:05.072767: train_loss -0.8505 
2024-05-08 18:32:05.072955: val_loss -0.8623 
2024-05-08 18:32:05.073004: Pseudo dice [0.9251] 
2024-05-08 18:32:05.073057: Epoch time: 41.0 s 
2024-05-08 18:32:06.206653:  
2024-05-08 18:32:06.206801: Epoch 529 
2024-05-08 18:32:06.206901: Current learning rate: 0.00508 
2024-05-08 18:32:47.215030: train_loss -0.8495 
2024-05-08 18:32:47.215219: val_loss -0.8555 
2024-05-08 18:32:47.215269: Pseudo dice [0.9187] 
2024-05-08 18:32:47.215322: Epoch time: 41.01 s 
2024-05-08 18:32:48.302066:  
2024-05-08 18:32:48.302269: Epoch 530 
2024-05-08 18:32:48.302365: Current learning rate: 0.00507 
2024-05-08 18:33:29.342154: train_loss -0.8529 
2024-05-08 18:33:29.342356: val_loss -0.8649 
2024-05-08 18:33:29.342406: Pseudo dice [0.9278] 
2024-05-08 18:33:29.342459: Epoch time: 41.04 s 
2024-05-08 18:33:30.430687:  
2024-05-08 18:33:30.430830: Epoch 531 
2024-05-08 18:33:30.430925: Current learning rate: 0.00506 
2024-05-08 18:34:11.406774: train_loss -0.8554 
2024-05-08 18:34:11.406973: val_loss -0.8646 
2024-05-08 18:34:11.407027: Pseudo dice [0.9278] 
2024-05-08 18:34:11.407080: Epoch time: 40.98 s 
2024-05-08 18:34:12.493448:  
2024-05-08 18:34:12.493583: Epoch 532 
2024-05-08 18:34:12.493677: Current learning rate: 0.00505 
2024-05-08 18:34:53.493076: train_loss -0.8621 
2024-05-08 18:34:53.493262: val_loss -0.8681 
2024-05-08 18:34:53.493311: Pseudo dice [0.9287] 
2024-05-08 18:34:53.493365: Epoch time: 41.0 s 
2024-05-08 18:34:54.583595:  
2024-05-08 18:34:54.583884: Epoch 533 
2024-05-08 18:34:54.584052: Current learning rate: 0.00504 
2024-05-08 18:35:35.621788: train_loss -0.8596 
2024-05-08 18:35:35.622090: val_loss -0.8458 
2024-05-08 18:35:35.622138: Pseudo dice [0.9139] 
2024-05-08 18:35:35.622191: Epoch time: 41.04 s 
2024-05-08 18:35:36.907993:  
2024-05-08 18:35:36.908158: Epoch 534 
2024-05-08 18:35:36.908246: Current learning rate: 0.00503 
2024-05-08 18:36:17.952899: train_loss -0.8596 
2024-05-08 18:36:17.953085: val_loss -0.863 
2024-05-08 18:36:17.953135: Pseudo dice [0.9247] 
2024-05-08 18:36:17.953187: Epoch time: 41.05 s 
2024-05-08 18:36:19.040917:  
2024-05-08 18:36:19.041068: Epoch 535 
2024-05-08 18:36:19.041164: Current learning rate: 0.00502 
2024-05-08 18:37:00.074738: train_loss -0.8556 
2024-05-08 18:37:00.074944: val_loss -0.8609 
2024-05-08 18:37:00.074993: Pseudo dice [0.9253] 
2024-05-08 18:37:00.075047: Epoch time: 41.03 s 
2024-05-08 18:37:01.160915:  
2024-05-08 18:37:01.161060: Epoch 536 
2024-05-08 18:37:01.161153: Current learning rate: 0.00501 
2024-05-08 18:37:42.220554: train_loss -0.8553 
2024-05-08 18:37:42.220734: val_loss -0.8669 
2024-05-08 18:37:42.220784: Pseudo dice [0.9255] 
2024-05-08 18:37:42.220837: Epoch time: 41.06 s 
2024-05-08 18:37:43.309149:  
2024-05-08 18:37:43.309358: Epoch 537 
2024-05-08 18:37:43.309450: Current learning rate: 0.005 
2024-05-08 18:38:24.328374: train_loss -0.8529 
2024-05-08 18:38:24.328586: val_loss -0.8604 
2024-05-08 18:38:24.328641: Pseudo dice [0.92] 
2024-05-08 18:38:24.328706: Epoch time: 41.02 s 
2024-05-08 18:38:25.441295:  
2024-05-08 18:38:25.441576: Epoch 538 
2024-05-08 18:38:25.441670: Current learning rate: 0.00499 
2024-05-08 18:39:06.439396: train_loss -0.853 
2024-05-08 18:39:06.439579: val_loss -0.8679 
2024-05-08 18:39:06.439628: Pseudo dice [0.9273] 
2024-05-08 18:39:06.439679: Epoch time: 41.0 s 
2024-05-08 18:39:07.525471:  
2024-05-08 18:39:07.525609: Epoch 539 
2024-05-08 18:39:07.525704: Current learning rate: 0.00498 
2024-05-08 18:39:48.521452: train_loss -0.8613 
2024-05-08 18:39:48.521635: val_loss -0.8785 
2024-05-08 18:39:48.521684: Pseudo dice [0.9319] 
2024-05-08 18:39:48.521737: Epoch time: 41.0 s 
2024-05-08 18:39:49.791237:  
2024-05-08 18:39:49.791371: Epoch 540 
2024-05-08 18:39:49.791461: Current learning rate: 0.00497 
2024-05-08 18:40:30.795464: train_loss -0.8541 
2024-05-08 18:40:30.795659: val_loss -0.862 
2024-05-08 18:40:30.795709: Pseudo dice [0.9256] 
2024-05-08 18:40:30.795764: Epoch time: 41.01 s 
2024-05-08 18:40:31.885519:  
2024-05-08 18:40:31.885660: Epoch 541 
2024-05-08 18:40:31.885751: Current learning rate: 0.00496 
2024-05-08 18:41:12.872883: train_loss -0.864 
2024-05-08 18:41:12.873065: val_loss -0.8527 
2024-05-08 18:41:12.873114: Pseudo dice [0.9211] 
2024-05-08 18:41:12.873167: Epoch time: 40.99 s 
2024-05-08 18:41:13.990533:  
2024-05-08 18:41:13.990674: Epoch 542 
2024-05-08 18:41:13.990765: Current learning rate: 0.00495 
2024-05-08 18:41:55.000551: train_loss -0.8615 
2024-05-08 18:41:55.000745: val_loss -0.8658 
2024-05-08 18:41:55.000805: Pseudo dice [0.9272] 
2024-05-08 18:41:55.000867: Epoch time: 41.01 s 
2024-05-08 18:41:56.089379:  
2024-05-08 18:41:56.089506: Epoch 543 
2024-05-08 18:41:56.089593: Current learning rate: 0.00494 
2024-05-08 18:42:37.094165: train_loss -0.8569 
2024-05-08 18:42:37.094352: val_loss -0.8519 
2024-05-08 18:42:37.094402: Pseudo dice [0.9205] 
2024-05-08 18:42:37.094455: Epoch time: 41.01 s 
2024-05-08 18:42:38.178673:  
2024-05-08 18:42:38.178853: Epoch 544 
2024-05-08 18:42:38.178944: Current learning rate: 0.00493 
2024-05-08 18:43:19.174154: train_loss -0.8617 
2024-05-08 18:43:19.174335: val_loss -0.8747 
2024-05-08 18:43:19.174385: Pseudo dice [0.9291] 
2024-05-08 18:43:19.174439: Epoch time: 41.0 s 
2024-05-08 18:43:20.257667:  
2024-05-08 18:43:20.257805: Epoch 545 
2024-05-08 18:43:20.257897: Current learning rate: 0.00492 
2024-05-08 18:44:01.259839: train_loss -0.86 
2024-05-08 18:44:01.260021: val_loss -0.8614 
2024-05-08 18:44:01.260071: Pseudo dice [0.9235] 
2024-05-08 18:44:01.260123: Epoch time: 41.0 s 
2024-05-08 18:44:02.529202:  
2024-05-08 18:44:02.529359: Epoch 546 
2024-05-08 18:44:02.529452: Current learning rate: 0.00491 
2024-05-08 18:44:43.574503: train_loss -0.8601 
2024-05-08 18:44:43.574685: val_loss -0.8672 
2024-05-08 18:44:43.574735: Pseudo dice [0.9251] 
2024-05-08 18:44:43.574789: Epoch time: 41.05 s 
2024-05-08 18:44:44.677291:  
2024-05-08 18:44:44.677482: Epoch 547 
2024-05-08 18:44:44.677577: Current learning rate: 0.0049 
2024-05-08 18:45:25.694421: train_loss -0.8529 
2024-05-08 18:45:25.694601: val_loss -0.8596 
2024-05-08 18:45:25.694652: Pseudo dice [0.9255] 
2024-05-08 18:45:25.694706: Epoch time: 41.02 s 
2024-05-08 18:45:26.780819:  
2024-05-08 18:45:26.781050: Epoch 548 
2024-05-08 18:45:26.781153: Current learning rate: 0.00489 
2024-05-08 18:46:07.805062: train_loss -0.8553 
2024-05-08 18:46:07.805241: val_loss -0.8585 
2024-05-08 18:46:07.805290: Pseudo dice [0.9279] 
2024-05-08 18:46:07.805345: Epoch time: 41.03 s 
2024-05-08 18:46:08.884035:  
2024-05-08 18:46:08.884176: Epoch 549 
2024-05-08 18:46:08.884270: Current learning rate: 0.00488 
2024-05-08 18:46:49.836299: train_loss -0.858 
2024-05-08 18:46:49.836514: val_loss -0.8695 
2024-05-08 18:46:49.836632: Pseudo dice [0.9271] 
2024-05-08 18:46:49.836710: Epoch time: 40.95 s 
2024-05-08 18:46:51.267730:  
2024-05-08 18:46:51.267872: Epoch 550 
2024-05-08 18:46:51.267964: Current learning rate: 0.00487 
2024-05-08 18:47:32.242986: train_loss -0.8592 
2024-05-08 18:47:32.243164: val_loss -0.8743 
2024-05-08 18:47:32.243213: Pseudo dice [0.9312] 
2024-05-08 18:47:32.243266: Epoch time: 40.98 s 
2024-05-08 18:47:33.326669:  
2024-05-08 18:47:33.326801: Epoch 551 
2024-05-08 18:47:33.326892: Current learning rate: 0.00486 
2024-05-08 18:48:14.315799: train_loss -0.8608 
2024-05-08 18:48:14.315979: val_loss -0.8657 
2024-05-08 18:48:14.316028: Pseudo dice [0.9287] 
2024-05-08 18:48:14.316081: Epoch time: 40.99 s 
2024-05-08 18:48:15.489222:  
2024-05-08 18:48:15.489467: Epoch 552 
2024-05-08 18:48:15.489563: Current learning rate: 0.00485 
2024-05-08 18:48:56.534873: train_loss -0.8513 
2024-05-08 18:48:56.535068: val_loss -0.8541 
2024-05-08 18:48:56.535118: Pseudo dice [0.9192] 
2024-05-08 18:48:56.535173: Epoch time: 41.05 s 
2024-05-08 18:48:57.818403:  
2024-05-08 18:48:57.818548: Epoch 553 
2024-05-08 18:48:57.818648: Current learning rate: 0.00484 
2024-05-08 18:49:38.794969: train_loss -0.852 
2024-05-08 18:49:38.795166: val_loss -0.8645 
2024-05-08 18:49:38.795225: Pseudo dice [0.9253] 
2024-05-08 18:49:38.795287: Epoch time: 40.98 s 
2024-05-08 18:49:39.871084:  
2024-05-08 18:49:39.871230: Epoch 554 
2024-05-08 18:49:39.871323: Current learning rate: 0.00484 
2024-05-08 18:50:20.905021: train_loss -0.8332 
2024-05-08 18:50:20.905225: val_loss -0.8418 
2024-05-08 18:50:20.905275: Pseudo dice [0.9154] 
2024-05-08 18:50:20.905329: Epoch time: 41.03 s 
2024-05-08 18:50:21.988984:  
2024-05-08 18:50:21.989134: Epoch 555 
2024-05-08 18:50:21.989224: Current learning rate: 0.00483 
2024-05-08 18:51:02.918020: train_loss -0.8313 
2024-05-08 18:51:02.918195: val_loss -0.8424 
2024-05-08 18:51:02.918243: Pseudo dice [0.9133] 
2024-05-08 18:51:02.918296: Epoch time: 40.93 s 
2024-05-08 18:51:04.004221:  
2024-05-08 18:51:04.004366: Epoch 556 
2024-05-08 18:51:04.004455: Current learning rate: 0.00482 
2024-05-08 18:51:44.926361: train_loss -0.8256 
2024-05-08 18:51:44.926545: val_loss -0.8413 
2024-05-08 18:51:44.926595: Pseudo dice [0.9095] 
2024-05-08 18:51:44.926649: Epoch time: 40.92 s 
2024-05-08 18:51:46.014556:  
2024-05-08 18:51:46.014699: Epoch 557 
2024-05-08 18:51:46.014791: Current learning rate: 0.00481 
2024-05-08 18:52:26.921103: train_loss -0.8208 
2024-05-08 18:52:26.921284: val_loss -0.8495 
2024-05-08 18:52:26.921333: Pseudo dice [0.9144] 
2024-05-08 18:52:26.921386: Epoch time: 40.91 s 
2024-05-08 18:52:28.001599:  
2024-05-08 18:52:28.001737: Epoch 558 
2024-05-08 18:52:28.001833: Current learning rate: 0.0048 
2024-05-08 18:53:08.898742: train_loss -0.8349 
2024-05-08 18:53:08.898918: val_loss -0.8517 
2024-05-08 18:53:08.898968: Pseudo dice [0.9176] 
2024-05-08 18:53:08.899022: Epoch time: 40.9 s 
2024-05-08 18:53:10.175800:  
2024-05-08 18:53:10.176005: Epoch 559 
2024-05-08 18:53:10.176100: Current learning rate: 0.00479 
2024-05-08 18:53:51.074557: train_loss -0.849 
2024-05-08 18:53:51.074739: val_loss -0.8593 
2024-05-08 18:53:51.074788: Pseudo dice [0.9221] 
2024-05-08 18:53:51.074840: Epoch time: 40.9 s 
2024-05-08 18:53:52.154946:  
2024-05-08 18:53:52.155173: Epoch 560 
2024-05-08 18:53:52.155267: Current learning rate: 0.00478 
2024-05-08 18:54:33.121384: train_loss -0.852 
2024-05-08 18:54:33.121564: val_loss -0.872 
2024-05-08 18:54:33.121614: Pseudo dice [0.9279] 
2024-05-08 18:54:33.121667: Epoch time: 40.97 s 
2024-05-08 18:54:34.206313:  
2024-05-08 18:54:34.206591: Epoch 561 
2024-05-08 18:54:34.206683: Current learning rate: 0.00477 
2024-05-08 18:55:15.164878: train_loss -0.8559 
2024-05-08 18:55:15.165057: val_loss -0.8555 
2024-05-08 18:55:15.165107: Pseudo dice [0.9238] 
2024-05-08 18:55:15.165160: Epoch time: 40.96 s 
2024-05-08 18:55:16.249565:  
2024-05-08 18:55:16.249813: Epoch 562 
2024-05-08 18:55:16.249907: Current learning rate: 0.00476 
2024-05-08 18:55:57.220341: train_loss -0.8486 
2024-05-08 18:55:57.220522: val_loss -0.8346 
2024-05-08 18:55:57.220581: Pseudo dice [0.908] 
2024-05-08 18:55:57.220636: Epoch time: 40.97 s 
2024-05-08 18:55:58.299021:  
2024-05-08 18:55:58.299221: Epoch 563 
2024-05-08 18:55:58.299315: Current learning rate: 0.00475 
2024-05-08 18:56:39.305349: train_loss -0.8269 
2024-05-08 18:56:39.305529: val_loss -0.8316 
2024-05-08 18:56:39.305579: Pseudo dice [0.9102] 
2024-05-08 18:56:39.305632: Epoch time: 41.01 s 
2024-05-08 18:56:40.407334:  
2024-05-08 18:56:40.407465: Epoch 564 
2024-05-08 18:56:40.407558: Current learning rate: 0.00474 
2024-05-08 18:57:21.461358: train_loss -0.8169 
2024-05-08 18:57:21.461539: val_loss -0.7976 
2024-05-08 18:57:21.461588: Pseudo dice [0.8825] 
2024-05-08 18:57:21.461642: Epoch time: 41.06 s 
2024-05-08 18:57:22.552465:  
2024-05-08 18:57:22.552699: Epoch 565 
2024-05-08 18:57:22.552796: Current learning rate: 0.00473 
2024-05-08 18:58:03.626078: train_loss -0.8123 
2024-05-08 18:58:03.626261: val_loss -0.8394 
2024-05-08 18:58:03.626311: Pseudo dice [0.912] 
2024-05-08 18:58:03.626364: Epoch time: 41.07 s 
2024-05-08 18:58:04.907952:  
2024-05-08 18:58:04.908231: Epoch 566 
2024-05-08 18:58:04.908339: Current learning rate: 0.00472 
2024-05-08 18:58:45.974730: train_loss -0.8319 
2024-05-08 18:58:45.975034: val_loss -0.8519 
2024-05-08 18:58:45.975111: Pseudo dice [0.9185] 
2024-05-08 18:58:45.975168: Epoch time: 41.07 s 
2024-05-08 18:58:47.054850:  
2024-05-08 18:58:47.055058: Epoch 567 
2024-05-08 18:58:47.055213: Current learning rate: 0.00471 
2024-05-08 18:59:28.114020: train_loss -0.8405 
2024-05-08 18:59:28.114204: val_loss -0.8428 
2024-05-08 18:59:28.114254: Pseudo dice [0.9155] 
2024-05-08 18:59:28.114307: Epoch time: 41.06 s 
2024-05-08 18:59:29.213520:  
2024-05-08 18:59:29.213662: Epoch 568 
2024-05-08 18:59:29.213756: Current learning rate: 0.0047 
2024-05-08 19:00:10.281085: train_loss -0.8432 
2024-05-08 19:00:10.281354: val_loss -0.8536 
2024-05-08 19:00:10.281442: Pseudo dice [0.9198] 
2024-05-08 19:00:10.281498: Epoch time: 41.07 s 
2024-05-08 19:00:11.366437:  
2024-05-08 19:00:11.366582: Epoch 569 
2024-05-08 19:00:11.366672: Current learning rate: 0.00469 
2024-05-08 19:00:52.452752: train_loss -0.8493 
2024-05-08 19:00:52.452947: val_loss -0.8571 
2024-05-08 19:00:52.452997: Pseudo dice [0.925] 
2024-05-08 19:00:52.453050: Epoch time: 41.09 s 
2024-05-08 19:00:53.546066:  
2024-05-08 19:00:53.546409: Epoch 570 
2024-05-08 19:00:53.546511: Current learning rate: 0.00468 
2024-05-08 19:01:34.637154: train_loss -0.846 
2024-05-08 19:01:34.637338: val_loss -0.8668 
2024-05-08 19:01:34.637387: Pseudo dice [0.9249] 
2024-05-08 19:01:34.637441: Epoch time: 41.09 s 
2024-05-08 19:01:35.726910:  
2024-05-08 19:01:35.727087: Epoch 571 
2024-05-08 19:01:35.727200: Current learning rate: 0.00467 
2024-05-08 19:02:16.844466: train_loss -0.8467 
2024-05-08 19:02:16.844656: val_loss -0.8492 
2024-05-08 19:02:16.844707: Pseudo dice [0.9266] 
2024-05-08 19:02:16.844762: Epoch time: 41.12 s 
2024-05-08 19:02:18.115769:  
2024-05-08 19:02:18.115920: Epoch 572 
2024-05-08 19:02:18.116014: Current learning rate: 0.00466 
2024-05-08 19:02:59.262191: train_loss -0.8535 
2024-05-08 19:02:59.262380: val_loss -0.8533 
2024-05-08 19:02:59.262429: Pseudo dice [0.9187] 
2024-05-08 19:02:59.262483: Epoch time: 41.15 s 
2024-05-08 19:03:00.361158:  
2024-05-08 19:03:00.361295: Epoch 573 
2024-05-08 19:03:00.361386: Current learning rate: 0.00465 
2024-05-08 19:03:41.455107: train_loss -0.8488 
2024-05-08 19:03:41.455288: val_loss -0.849 
2024-05-08 19:03:41.455337: Pseudo dice [0.9161] 
2024-05-08 19:03:41.455390: Epoch time: 41.09 s 
2024-05-08 19:03:42.587958:  
2024-05-08 19:03:42.588105: Epoch 574 
2024-05-08 19:03:42.588199: Current learning rate: 0.00464 
2024-05-08 19:04:23.676058: train_loss -0.8552 
2024-05-08 19:04:23.676237: val_loss -0.8508 
2024-05-08 19:04:23.676289: Pseudo dice [0.9174] 
2024-05-08 19:04:23.676342: Epoch time: 41.09 s 
2024-05-08 19:04:24.777397:  
2024-05-08 19:04:24.777542: Epoch 575 
2024-05-08 19:04:24.777650: Current learning rate: 0.00463 
2024-05-08 19:05:05.878962: train_loss -0.8436 
2024-05-08 19:05:05.879141: val_loss -0.8444 
2024-05-08 19:05:05.879189: Pseudo dice [0.9161] 
2024-05-08 19:05:05.879243: Epoch time: 41.1 s 
2024-05-08 19:05:06.987996:  
2024-05-08 19:05:06.988140: Epoch 576 
2024-05-08 19:05:06.988236: Current learning rate: 0.00462 
2024-05-08 19:05:48.092348: train_loss -0.8409 
2024-05-08 19:05:48.092530: val_loss -0.8405 
2024-05-08 19:05:48.092580: Pseudo dice [0.9138] 
2024-05-08 19:05:48.092634: Epoch time: 41.11 s 
2024-05-08 19:05:49.194193:  
2024-05-08 19:05:49.194331: Epoch 577 
2024-05-08 19:05:49.194429: Current learning rate: 0.00461 
2024-05-08 19:06:30.294622: train_loss -0.8468 
2024-05-08 19:06:30.294804: val_loss -0.8703 
2024-05-08 19:06:30.294852: Pseudo dice [0.927] 
2024-05-08 19:06:30.294907: Epoch time: 41.1 s 
2024-05-08 19:06:31.616690:  
2024-05-08 19:06:31.616999: Epoch 578 
2024-05-08 19:06:31.617133: Current learning rate: 0.0046 
2024-05-08 19:07:12.740430: train_loss -0.8489 
2024-05-08 19:07:12.740604: val_loss -0.8639 
2024-05-08 19:07:12.740663: Pseudo dice [0.9292] 
2024-05-08 19:07:12.740719: Epoch time: 41.12 s 
2024-05-08 19:07:13.845062:  
2024-05-08 19:07:13.845214: Epoch 579 
2024-05-08 19:07:13.845313: Current learning rate: 0.00459 
2024-05-08 19:07:55.009932: train_loss -0.8562 
2024-05-08 19:07:55.010113: val_loss -0.8702 
2024-05-08 19:07:55.010178: Pseudo dice [0.9279] 
2024-05-08 19:07:55.010233: Epoch time: 41.17 s 
2024-05-08 19:07:56.112220:  
2024-05-08 19:07:56.112374: Epoch 580 
2024-05-08 19:07:56.112463: Current learning rate: 0.00458 
2024-05-08 19:08:37.277105: train_loss -0.8565 
2024-05-08 19:08:37.277296: val_loss -0.8557 
2024-05-08 19:08:37.277344: Pseudo dice [0.9226] 
2024-05-08 19:08:37.277397: Epoch time: 41.17 s 
2024-05-08 19:08:38.393028:  
2024-05-08 19:08:38.393176: Epoch 581 
2024-05-08 19:08:38.393269: Current learning rate: 0.00457 
2024-05-08 19:09:19.551181: train_loss -0.8602 
2024-05-08 19:09:19.551363: val_loss -0.8689 
2024-05-08 19:09:19.551411: Pseudo dice [0.9264] 
2024-05-08 19:09:19.551465: Epoch time: 41.16 s 
2024-05-08 19:09:20.655784:  
2024-05-08 19:09:20.655922: Epoch 582 
2024-05-08 19:09:20.656014: Current learning rate: 0.00456 
2024-05-08 19:10:01.822804: train_loss -0.8618 
2024-05-08 19:10:01.823006: val_loss -0.8586 
2024-05-08 19:10:01.823056: Pseudo dice [0.924] 
2024-05-08 19:10:01.823111: Epoch time: 41.17 s 
2024-05-08 19:10:02.934728:  
2024-05-08 19:10:02.934920: Epoch 583 
2024-05-08 19:10:02.935015: Current learning rate: 0.00455 
2024-05-08 19:10:44.040304: train_loss -0.8485 
2024-05-08 19:10:44.040478: val_loss -0.8603 
2024-05-08 19:10:44.040528: Pseudo dice [0.9228] 
2024-05-08 19:10:44.040583: Epoch time: 41.11 s 
2024-05-08 19:10:45.323398:  
2024-05-08 19:10:45.323536: Epoch 584 
2024-05-08 19:10:45.323637: Current learning rate: 0.00454 
2024-05-08 19:11:26.471345: train_loss -0.8554 
2024-05-08 19:11:26.471526: val_loss -0.8657 
2024-05-08 19:11:26.471577: Pseudo dice [0.9281] 
2024-05-08 19:11:26.471629: Epoch time: 41.15 s 
2024-05-08 19:11:27.573383:  
2024-05-08 19:11:27.573567: Epoch 585 
2024-05-08 19:11:27.573658: Current learning rate: 0.00453 
2024-05-08 19:12:08.713262: train_loss -0.8531 
2024-05-08 19:12:08.713446: val_loss -0.861 
2024-05-08 19:12:08.713494: Pseudo dice [0.9235] 
2024-05-08 19:12:08.713549: Epoch time: 41.14 s 
2024-05-08 19:12:09.825350:  
2024-05-08 19:12:09.825487: Epoch 586 
2024-05-08 19:12:09.825574: Current learning rate: 0.00452 
2024-05-08 19:12:50.938131: train_loss -0.8561 
2024-05-08 19:12:50.938299: val_loss -0.8724 
2024-05-08 19:12:50.938348: Pseudo dice [0.9265] 
2024-05-08 19:12:50.938401: Epoch time: 41.11 s 
2024-05-08 19:12:52.040798:  
2024-05-08 19:12:52.040941: Epoch 587 
2024-05-08 19:12:52.041033: Current learning rate: 0.00451 
2024-05-08 19:13:33.121804: train_loss -0.8549 
2024-05-08 19:13:33.121987: val_loss -0.8729 
2024-05-08 19:13:33.122036: Pseudo dice [0.9314] 
2024-05-08 19:13:33.122090: Epoch time: 41.08 s 
2024-05-08 19:13:34.223695:  
2024-05-08 19:13:34.223843: Epoch 588 
2024-05-08 19:13:34.223936: Current learning rate: 0.0045 
2024-05-08 19:14:15.320539: train_loss -0.8578 
2024-05-08 19:14:15.320722: val_loss -0.8675 
2024-05-08 19:14:15.320771: Pseudo dice [0.9282] 
2024-05-08 19:14:15.320824: Epoch time: 41.1 s 
2024-05-08 19:14:16.458338:  
2024-05-08 19:14:16.458547: Epoch 589 
2024-05-08 19:14:16.458648: Current learning rate: 0.00449 
2024-05-08 19:14:57.620909: train_loss -0.8527 
2024-05-08 19:14:57.621109: val_loss -0.8479 
2024-05-08 19:14:57.621158: Pseudo dice [0.9151] 
2024-05-08 19:14:57.621279: Epoch time: 41.16 s 
2024-05-08 19:14:58.715718:  
2024-05-08 19:14:58.715909: Epoch 590 
2024-05-08 19:14:58.716012: Current learning rate: 0.00448 
2024-05-08 19:15:39.858276: train_loss -0.8543 
2024-05-08 19:15:39.858467: val_loss -0.8614 
2024-05-08 19:15:39.858517: Pseudo dice [0.9268] 
2024-05-08 19:15:39.858572: Epoch time: 41.14 s 
2024-05-08 19:15:41.159703:  
2024-05-08 19:15:41.159858: Epoch 591 
2024-05-08 19:15:41.159948: Current learning rate: 0.00447 
2024-05-08 19:16:22.342043: train_loss -0.8586 
2024-05-08 19:16:22.342225: val_loss -0.8661 
2024-05-08 19:16:22.342275: Pseudo dice [0.9241] 
2024-05-08 19:16:22.342328: Epoch time: 41.18 s 
2024-05-08 19:16:23.445629:  
2024-05-08 19:16:23.445801: Epoch 592 
2024-05-08 19:16:23.445916: Current learning rate: 0.00446 
2024-05-08 19:17:04.634518: train_loss -0.8595 
2024-05-08 19:17:04.634689: val_loss -0.867 
2024-05-08 19:17:04.634738: Pseudo dice [0.9286] 
2024-05-08 19:17:04.634793: Epoch time: 41.19 s 
2024-05-08 19:17:05.737719:  
2024-05-08 19:17:05.737852: Epoch 593 
2024-05-08 19:17:05.737942: Current learning rate: 0.00445 
2024-05-08 19:17:46.947737: train_loss -0.8602 
2024-05-08 19:17:46.947923: val_loss -0.8692 
2024-05-08 19:17:46.947973: Pseudo dice [0.9262] 
2024-05-08 19:17:46.948027: Epoch time: 41.21 s 
2024-05-08 19:17:48.041006:  
2024-05-08 19:17:48.041176: Epoch 594 
2024-05-08 19:17:48.041268: Current learning rate: 0.00444 
2024-05-08 19:18:29.283660: train_loss -0.8585 
2024-05-08 19:18:29.283838: val_loss -0.8709 
2024-05-08 19:18:29.283890: Pseudo dice [0.9301] 
2024-05-08 19:18:29.283943: Epoch time: 41.24 s 
2024-05-08 19:18:30.382487:  
2024-05-08 19:18:30.382678: Epoch 595 
2024-05-08 19:18:30.382775: Current learning rate: 0.00443 
2024-05-08 19:19:11.620481: train_loss -0.8566 
2024-05-08 19:19:11.620696: val_loss -0.8553 
2024-05-08 19:19:11.620745: Pseudo dice [0.9223] 
2024-05-08 19:19:11.620800: Epoch time: 41.24 s 
2024-05-08 19:19:12.717417:  
2024-05-08 19:19:12.717554: Epoch 596 
2024-05-08 19:19:12.717646: Current learning rate: 0.00442 
2024-05-08 19:19:53.950629: train_loss -0.8569 
2024-05-08 19:19:53.950810: val_loss -0.8761 
2024-05-08 19:19:53.950859: Pseudo dice [0.933] 
2024-05-08 19:19:53.950912: Epoch time: 41.23 s 
2024-05-08 19:19:55.243949:  
2024-05-08 19:19:55.244121: Epoch 597 
2024-05-08 19:19:55.244272: Current learning rate: 0.00441 
2024-05-08 19:20:36.501623: train_loss -0.8603 
2024-05-08 19:20:36.501808: val_loss -0.8623 
2024-05-08 19:20:36.501858: Pseudo dice [0.9224] 
2024-05-08 19:20:36.501911: Epoch time: 41.26 s 
2024-05-08 19:20:37.605772:  
2024-05-08 19:20:37.605949: Epoch 598 
2024-05-08 19:20:37.606044: Current learning rate: 0.0044 
2024-05-08 19:21:18.945129: train_loss -0.858 
2024-05-08 19:21:18.945318: val_loss -0.8595 
2024-05-08 19:21:18.945370: Pseudo dice [0.9241] 
2024-05-08 19:21:18.945424: Epoch time: 41.34 s 
2024-05-08 19:21:20.045440:  
2024-05-08 19:21:20.045574: Epoch 599 
2024-05-08 19:21:20.045664: Current learning rate: 0.00439 
2024-05-08 19:22:01.368074: train_loss -0.8674 
2024-05-08 19:22:01.368266: val_loss -0.8632 
2024-05-08 19:22:01.368320: Pseudo dice [0.9242] 
2024-05-08 19:22:01.368373: Epoch time: 41.32 s 
2024-05-08 19:22:02.820734:  
2024-05-08 19:22:02.820863: Epoch 600 
2024-05-08 19:22:02.820954: Current learning rate: 0.00438 
2024-05-08 19:22:44.165234: train_loss -0.8596 
2024-05-08 19:22:44.165432: val_loss -0.855 
2024-05-08 19:22:44.165485: Pseudo dice [0.9246] 
2024-05-08 19:22:44.165555: Epoch time: 41.35 s 
2024-05-08 19:22:45.291415:  
2024-05-08 19:22:45.291556: Epoch 601 
2024-05-08 19:22:45.291646: Current learning rate: 0.00437 
2024-05-08 19:23:26.545619: train_loss -0.8606 
2024-05-08 19:23:26.545795: val_loss -0.8625 
2024-05-08 19:23:26.545844: Pseudo dice [0.9223] 
2024-05-08 19:23:26.545898: Epoch time: 41.26 s 
2024-05-08 19:23:27.640803:  
2024-05-08 19:23:27.640932: Epoch 602 
2024-05-08 19:23:27.641020: Current learning rate: 0.00436 
2024-05-08 19:24:08.962975: train_loss -0.8566 
2024-05-08 19:24:08.963161: val_loss -0.865 
2024-05-08 19:24:08.963211: Pseudo dice [0.9294] 
2024-05-08 19:24:08.963265: Epoch time: 41.32 s 
2024-05-08 19:24:10.092654:  
2024-05-08 19:24:10.092784: Epoch 603 
2024-05-08 19:24:10.092875: Current learning rate: 0.00435 
2024-05-08 19:24:51.403874: train_loss -0.8576 
2024-05-08 19:24:51.404047: val_loss -0.8612 
2024-05-08 19:24:51.404096: Pseudo dice [0.9241] 
2024-05-08 19:24:51.404149: Epoch time: 41.31 s 
2024-05-08 19:24:52.702732:  
2024-05-08 19:24:52.702882: Epoch 604 
2024-05-08 19:24:52.702973: Current learning rate: 0.00434 
2024-05-08 19:25:33.932379: train_loss -0.8528 
2024-05-08 19:25:33.932561: val_loss -0.8771 
2024-05-08 19:25:33.932612: Pseudo dice [0.9334] 
2024-05-08 19:25:33.932670: Epoch time: 41.23 s 
2024-05-08 19:25:35.025288:  
2024-05-08 19:25:35.025435: Epoch 605 
2024-05-08 19:25:35.025546: Current learning rate: 0.00433 
2024-05-08 19:26:16.257843: train_loss -0.8584 
2024-05-08 19:26:16.258024: val_loss -0.8569 
2024-05-08 19:26:16.258075: Pseudo dice [0.9282] 
2024-05-08 19:26:16.258129: Epoch time: 41.23 s 
2024-05-08 19:26:17.363168:  
2024-05-08 19:26:17.363308: Epoch 606 
2024-05-08 19:26:17.363405: Current learning rate: 0.00432 
2024-05-08 19:26:58.610030: train_loss -0.8602 
2024-05-08 19:26:58.610213: val_loss -0.8779 
2024-05-08 19:26:58.610264: Pseudo dice [0.9329] 
2024-05-08 19:26:58.610319: Epoch time: 41.25 s 
2024-05-08 19:26:59.745879:  
2024-05-08 19:26:59.746074: Epoch 607 
2024-05-08 19:26:59.746167: Current learning rate: 0.00431 
2024-05-08 19:27:40.957421: train_loss -0.8602 
2024-05-08 19:27:40.957592: val_loss -0.8642 
2024-05-08 19:27:40.957642: Pseudo dice [0.9271] 
2024-05-08 19:27:40.957696: Epoch time: 41.21 s 
2024-05-08 19:27:42.046418:  
2024-05-08 19:27:42.046638: Epoch 608 
2024-05-08 19:27:42.046733: Current learning rate: 0.0043 
2024-05-08 19:28:23.413144: train_loss -0.8596 
2024-05-08 19:28:23.413324: val_loss -0.8696 
2024-05-08 19:28:23.413372: Pseudo dice [0.9272] 
2024-05-08 19:28:23.413425: Epoch time: 41.37 s 
2024-05-08 19:28:24.505400:  
2024-05-08 19:28:24.505729: Epoch 609 
2024-05-08 19:28:24.505828: Current learning rate: 0.00429 
2024-05-08 19:29:05.888152: train_loss -0.862 
2024-05-08 19:29:05.888331: val_loss -0.8624 
2024-05-08 19:29:05.888381: Pseudo dice [0.9299] 
2024-05-08 19:29:05.888434: Epoch time: 41.38 s 
2024-05-08 19:29:07.187947:  
2024-05-08 19:29:07.188093: Epoch 610 
2024-05-08 19:29:07.188182: Current learning rate: 0.00429 
2024-05-08 19:29:48.513676: train_loss -0.8506 
2024-05-08 19:29:48.513863: val_loss -0.8412 
2024-05-08 19:29:48.513913: Pseudo dice [0.9135] 
2024-05-08 19:29:48.513966: Epoch time: 41.33 s 
2024-05-08 19:29:49.620079:  
2024-05-08 19:29:49.620226: Epoch 611 
2024-05-08 19:29:49.620317: Current learning rate: 0.00428 
2024-05-08 19:30:30.936462: train_loss -0.8114 
2024-05-08 19:30:30.936643: val_loss -0.816 
2024-05-08 19:30:30.936764: Pseudo dice [0.8965] 
2024-05-08 19:30:30.936898: Epoch time: 41.32 s 
2024-05-08 19:30:32.236405:  
2024-05-08 19:30:32.236585: Epoch 612 
2024-05-08 19:30:32.236688: Current learning rate: 0.00427 
2024-05-08 19:31:13.536040: train_loss -0.8271 
2024-05-08 19:31:13.536221: val_loss -0.8494 
2024-05-08 19:31:13.536269: Pseudo dice [0.9194] 
2024-05-08 19:31:13.536322: Epoch time: 41.3 s 
2024-05-08 19:31:14.639373:  
2024-05-08 19:31:14.639514: Epoch 613 
2024-05-08 19:31:14.639605: Current learning rate: 0.00426 
2024-05-08 19:31:55.942330: train_loss -0.8323 
2024-05-08 19:31:55.942526: val_loss -0.8558 
2024-05-08 19:31:55.942576: Pseudo dice [0.9229] 
2024-05-08 19:31:55.942629: Epoch time: 41.3 s 
2024-05-08 19:31:57.045378:  
2024-05-08 19:31:57.045512: Epoch 614 
2024-05-08 19:31:57.045602: Current learning rate: 0.00425 
2024-05-08 19:32:38.303540: train_loss -0.847 
2024-05-08 19:32:38.303720: val_loss -0.8666 
2024-05-08 19:32:38.303770: Pseudo dice [0.9309] 
2024-05-08 19:32:38.303822: Epoch time: 41.26 s 
2024-05-08 19:32:39.405925:  
2024-05-08 19:32:39.406062: Epoch 615 
2024-05-08 19:32:39.406153: Current learning rate: 0.00424 
2024-05-08 19:33:20.605020: train_loss -0.8476 
2024-05-08 19:33:20.605200: val_loss -0.8591 
2024-05-08 19:33:20.605248: Pseudo dice [0.9211] 
2024-05-08 19:33:20.605301: Epoch time: 41.2 s 
2024-05-08 19:33:21.906376:  
2024-05-08 19:33:21.906518: Epoch 616 
2024-05-08 19:33:21.906617: Current learning rate: 0.00423 
2024-05-08 19:34:03.088982: train_loss -0.8576 
2024-05-08 19:34:03.089167: val_loss -0.8734 
2024-05-08 19:34:03.089215: Pseudo dice [0.9322] 
2024-05-08 19:34:03.089269: Epoch time: 41.18 s 
2024-05-08 19:34:04.216061:  
2024-05-08 19:34:04.216249: Epoch 617 
2024-05-08 19:34:04.216340: Current learning rate: 0.00422 
2024-05-08 19:34:45.375838: train_loss -0.8601 
2024-05-08 19:34:45.376013: val_loss -0.8559 
2024-05-08 19:34:45.376062: Pseudo dice [0.9195] 
2024-05-08 19:34:45.376119: Epoch time: 41.16 s 
2024-05-08 19:34:46.479740:  
2024-05-08 19:34:46.479880: Epoch 618 
2024-05-08 19:34:46.479971: Current learning rate: 0.00421 
2024-05-08 19:35:27.564462: train_loss -0.8595 
2024-05-08 19:35:27.564656: val_loss -0.864 
2024-05-08 19:35:27.564723: Pseudo dice [0.927] 
2024-05-08 19:35:27.564776: Epoch time: 41.09 s 
2024-05-08 19:35:28.678428:  
2024-05-08 19:35:28.678643: Epoch 619 
2024-05-08 19:35:28.678748: Current learning rate: 0.0042 
2024-05-08 19:36:09.784844: train_loss -0.8612 
2024-05-08 19:36:09.785020: val_loss -0.874 
2024-05-08 19:36:09.785071: Pseudo dice [0.9303] 
2024-05-08 19:36:09.785124: Epoch time: 41.11 s 
2024-05-08 19:36:10.897010:  
2024-05-08 19:36:10.897137: Epoch 620 
2024-05-08 19:36:10.897229: Current learning rate: 0.00419 
2024-05-08 19:36:51.981747: train_loss -0.8661 
2024-05-08 19:36:51.981925: val_loss -0.8554 
2024-05-08 19:36:51.981974: Pseudo dice [0.9215] 
2024-05-08 19:36:51.982027: Epoch time: 41.09 s 
2024-05-08 19:36:53.093163:  
2024-05-08 19:36:53.093380: Epoch 621 
2024-05-08 19:36:53.093622: Current learning rate: 0.00418 
2024-05-08 19:37:34.113713: train_loss -0.8619 
2024-05-08 19:37:34.113893: val_loss -0.863 
2024-05-08 19:37:34.113942: Pseudo dice [0.9235] 
2024-05-08 19:37:34.113994: Epoch time: 41.02 s 
2024-05-08 19:37:35.405072:  
2024-05-08 19:37:35.405219: Epoch 622 
2024-05-08 19:37:35.405316: Current learning rate: 0.00417 
2024-05-08 19:38:16.478680: train_loss -0.8608 
2024-05-08 19:38:16.478870: val_loss -0.8651 
2024-05-08 19:38:16.478923: Pseudo dice [0.9265] 
2024-05-08 19:38:16.478976: Epoch time: 41.07 s 
2024-05-08 19:38:17.586065:  
2024-05-08 19:38:17.586211: Epoch 623 
2024-05-08 19:38:17.586299: Current learning rate: 0.00416 
2024-05-08 19:38:58.673540: train_loss -0.8638 
2024-05-08 19:38:58.673717: val_loss -0.8542 
2024-05-08 19:38:58.673767: Pseudo dice [0.9216] 
2024-05-08 19:38:58.673819: Epoch time: 41.09 s 
2024-05-08 19:38:59.794790:  
2024-05-08 19:38:59.794986: Epoch 624 
2024-05-08 19:38:59.795080: Current learning rate: 0.00415 
2024-05-08 19:39:40.935651: train_loss -0.8561 
2024-05-08 19:39:40.935835: val_loss -0.8529 
2024-05-08 19:39:40.935884: Pseudo dice [0.927] 
2024-05-08 19:39:40.935937: Epoch time: 41.14 s 
2024-05-08 19:39:42.048149:  
2024-05-08 19:39:42.048343: Epoch 625 
2024-05-08 19:39:42.048438: Current learning rate: 0.00414 
2024-05-08 19:40:23.171634: train_loss -0.8663 
2024-05-08 19:40:23.171802: val_loss -0.8724 
2024-05-08 19:40:23.171850: Pseudo dice [0.9285] 
2024-05-08 19:40:23.171903: Epoch time: 41.12 s 
2024-05-08 19:40:24.289186:  
2024-05-08 19:40:24.289325: Epoch 626 
2024-05-08 19:40:24.289422: Current learning rate: 0.00413 
2024-05-08 19:41:05.450078: train_loss -0.8613 
2024-05-08 19:41:05.450258: val_loss -0.8627 
2024-05-08 19:41:05.450307: Pseudo dice [0.9249] 
2024-05-08 19:41:05.450361: Epoch time: 41.16 s 
2024-05-08 19:41:06.563842:  
2024-05-08 19:41:06.563980: Epoch 627 
2024-05-08 19:41:06.564072: Current learning rate: 0.00412 
2024-05-08 19:41:47.880440: train_loss -0.8593 
2024-05-08 19:41:47.880619: val_loss -0.8702 
2024-05-08 19:41:47.880676: Pseudo dice [0.9259] 
2024-05-08 19:41:47.880731: Epoch time: 41.32 s 
2024-05-08 19:41:49.182695:  
2024-05-08 19:41:49.182893: Epoch 628 
2024-05-08 19:41:49.182987: Current learning rate: 0.00411 
2024-05-08 19:42:30.541989: train_loss -0.8549 
2024-05-08 19:42:30.542177: val_loss -0.8669 
2024-05-08 19:42:30.542228: Pseudo dice [0.928] 
2024-05-08 19:42:30.542282: Epoch time: 41.36 s 
2024-05-08 19:42:31.655581:  
2024-05-08 19:42:31.655741: Epoch 629 
2024-05-08 19:42:31.655839: Current learning rate: 0.0041 
2024-05-08 19:43:12.893746: train_loss -0.8585 
2024-05-08 19:43:12.893934: val_loss -0.8633 
2024-05-08 19:43:12.893986: Pseudo dice [0.926] 
2024-05-08 19:43:12.894078: Epoch time: 41.24 s 
2024-05-08 19:43:14.023179:  
2024-05-08 19:43:14.023331: Epoch 630 
2024-05-08 19:43:14.023423: Current learning rate: 0.00409 
2024-05-08 19:43:55.349438: train_loss -0.86 
2024-05-08 19:43:55.349621: val_loss -0.8738 
2024-05-08 19:43:55.349669: Pseudo dice [0.9318] 
2024-05-08 19:43:55.349721: Epoch time: 41.33 s 
2024-05-08 19:43:56.461613:  
2024-05-08 19:43:56.461761: Epoch 631 
2024-05-08 19:43:56.461854: Current learning rate: 0.00408 
2024-05-08 19:44:37.688101: train_loss -0.8593 
2024-05-08 19:44:37.688285: val_loss -0.8597 
2024-05-08 19:44:37.688333: Pseudo dice [0.9204] 
2024-05-08 19:44:37.688388: Epoch time: 41.23 s 
2024-05-08 19:44:38.831775:  
2024-05-08 19:44:38.832261: Epoch 632 
2024-05-08 19:44:38.832359: Current learning rate: 0.00407 
2024-05-08 19:45:20.152473: train_loss -0.859 
2024-05-08 19:45:20.152651: val_loss -0.877 
2024-05-08 19:45:20.152751: Pseudo dice [0.9348] 
2024-05-08 19:45:20.152805: Epoch time: 41.32 s 
2024-05-08 19:45:21.267895:  
2024-05-08 19:45:21.268281: Epoch 633 
2024-05-08 19:45:21.268381: Current learning rate: 0.00406 
2024-05-08 19:46:02.505291: train_loss -0.8648 
2024-05-08 19:46:02.505467: val_loss -0.8639 
2024-05-08 19:46:02.505516: Pseudo dice [0.9283] 
2024-05-08 19:46:02.505569: Epoch time: 41.24 s 
2024-05-08 19:46:03.792260:  
2024-05-08 19:46:03.792470: Epoch 634 
2024-05-08 19:46:03.792569: Current learning rate: 0.00405 
2024-05-08 19:46:45.055240: train_loss -0.864 
2024-05-08 19:46:45.055428: val_loss -0.8685 
2024-05-08 19:46:45.055476: Pseudo dice [0.9274] 
2024-05-08 19:46:45.055529: Epoch time: 41.26 s 
2024-05-08 19:46:46.182331:  
2024-05-08 19:46:46.182474: Epoch 635 
2024-05-08 19:46:46.182566: Current learning rate: 0.00404 
2024-05-08 19:47:27.441653: train_loss -0.8643 
2024-05-08 19:47:27.441833: val_loss -0.8765 
2024-05-08 19:47:27.441884: Pseudo dice [0.9321] 
2024-05-08 19:47:27.441937: Epoch time: 41.26 s 
2024-05-08 19:47:28.546000:  
2024-05-08 19:47:28.546141: Epoch 636 
2024-05-08 19:47:28.546236: Current learning rate: 0.00403 
2024-05-08 19:48:09.820173: train_loss -0.8635 
2024-05-08 19:48:09.820349: val_loss -0.878 
2024-05-08 19:48:09.820398: Pseudo dice [0.9326] 
2024-05-08 19:48:09.820451: Epoch time: 41.28 s 
2024-05-08 19:48:09.820493: Yayy! New best EMA pseudo Dice: 0.9277 
2024-05-08 19:48:11.345389:  
2024-05-08 19:48:11.345579: Epoch 637 
2024-05-08 19:48:11.345674: Current learning rate: 0.00402 
2024-05-08 19:48:52.697569: train_loss -0.849 
2024-05-08 19:48:52.697750: val_loss -0.8458 
2024-05-08 19:48:52.697800: Pseudo dice [0.9182] 
2024-05-08 19:48:52.697853: Epoch time: 41.35 s 
2024-05-08 19:48:53.846781:  
2024-05-08 19:48:53.847033: Epoch 638 
2024-05-08 19:48:53.847166: Current learning rate: 0.00401 
2024-05-08 19:49:35.241520: train_loss -0.8526 
2024-05-08 19:49:35.241705: val_loss -0.8671 
2024-05-08 19:49:35.241759: Pseudo dice [0.9272] 
2024-05-08 19:49:35.241824: Epoch time: 41.4 s 
2024-05-08 19:49:36.366060:  
2024-05-08 19:49:36.366198: Epoch 639 
2024-05-08 19:49:36.366291: Current learning rate: 0.004 
2024-05-08 19:50:17.718489: train_loss -0.8615 
2024-05-08 19:50:17.718674: val_loss -0.8664 
2024-05-08 19:50:17.718726: Pseudo dice [0.9264] 
2024-05-08 19:50:17.718779: Epoch time: 41.35 s 
2024-05-08 19:50:18.822050:  
2024-05-08 19:50:18.822187: Epoch 640 
2024-05-08 19:50:18.822278: Current learning rate: 0.00399 
2024-05-08 19:51:00.174107: train_loss -0.8584 
2024-05-08 19:51:00.174294: val_loss -0.8732 
2024-05-08 19:51:00.174343: Pseudo dice [0.9308] 
2024-05-08 19:51:00.174397: Epoch time: 41.35 s 
2024-05-08 19:51:01.484225:  
2024-05-08 19:51:01.484376: Epoch 641 
2024-05-08 19:51:01.484468: Current learning rate: 0.00398 
2024-05-08 19:51:42.753151: train_loss -0.8643 
2024-05-08 19:51:42.753335: val_loss -0.8699 
2024-05-08 19:51:42.753384: Pseudo dice [0.9283] 
2024-05-08 19:51:42.753441: Epoch time: 41.27 s 
2024-05-08 19:51:43.852973:  
2024-05-08 19:51:43.853123: Epoch 642 
2024-05-08 19:51:43.853215: Current learning rate: 0.00397 
2024-05-08 19:52:25.202869: train_loss -0.8585 
2024-05-08 19:52:25.203064: val_loss -0.8573 
2024-05-08 19:52:25.203119: Pseudo dice [0.9232] 
2024-05-08 19:52:25.203179: Epoch time: 41.35 s 
2024-05-08 19:52:26.304737:  
2024-05-08 19:52:26.304877: Epoch 643 
2024-05-08 19:52:26.304973: Current learning rate: 0.00396 
2024-05-08 19:53:07.655350: train_loss -0.8603 
2024-05-08 19:53:07.655540: val_loss -0.8678 
2024-05-08 19:53:07.655589: Pseudo dice [0.9286] 
2024-05-08 19:53:07.655647: Epoch time: 41.35 s 
2024-05-08 19:53:08.757430:  
2024-05-08 19:53:08.757771: Epoch 644 
2024-05-08 19:53:08.757864: Current learning rate: 0.00395 
2024-05-08 19:53:50.094302: train_loss -0.8635 
2024-05-08 19:53:50.094483: val_loss -0.8761 
2024-05-08 19:53:50.094532: Pseudo dice [0.9345] 
2024-05-08 19:53:50.094591: Epoch time: 41.34 s 
2024-05-08 19:53:50.094721: Yayy! New best EMA pseudo Dice: 0.9278 
2024-05-08 19:53:51.577302:  
2024-05-08 19:53:51.577520: Epoch 645 
2024-05-08 19:53:51.577617: Current learning rate: 0.00394 
2024-05-08 19:54:32.958633: train_loss -0.8638 
2024-05-08 19:54:32.958810: val_loss -0.8738 
2024-05-08 19:54:32.958859: Pseudo dice [0.9298] 
2024-05-08 19:54:32.958915: Epoch time: 41.38 s 
2024-05-08 19:54:32.958958: Yayy! New best EMA pseudo Dice: 0.928 
2024-05-08 19:54:34.579279:  
2024-05-08 19:54:34.579426: Epoch 646 
2024-05-08 19:54:34.579522: Current learning rate: 0.00393 
2024-05-08 19:55:16.003537: train_loss -0.8602 
2024-05-08 19:55:16.003724: val_loss -0.8828 
2024-05-08 19:55:16.003774: Pseudo dice [0.9328] 
2024-05-08 19:55:16.003829: Epoch time: 41.43 s 
2024-05-08 19:55:16.003872: Yayy! New best EMA pseudo Dice: 0.9285 
2024-05-08 19:55:17.457348:  
2024-05-08 19:55:17.457544: Epoch 647 
2024-05-08 19:55:17.457640: Current learning rate: 0.00392 
2024-05-08 19:55:58.847195: train_loss -0.863 
2024-05-08 19:55:58.847376: val_loss -0.8533 
2024-05-08 19:55:58.847425: Pseudo dice [0.9221] 
2024-05-08 19:55:58.847481: Epoch time: 41.39 s 
2024-05-08 19:55:59.936913:  
2024-05-08 19:55:59.937118: Epoch 648 
2024-05-08 19:55:59.937216: Current learning rate: 0.00391 
2024-05-08 19:56:41.277439: train_loss -0.8657 
2024-05-08 19:56:41.277619: val_loss -0.8758 
2024-05-08 19:56:41.277668: Pseudo dice [0.9311] 
2024-05-08 19:56:41.277722: Epoch time: 41.34 s 
2024-05-08 19:56:42.367019:  
2024-05-08 19:56:42.367226: Epoch 649 
2024-05-08 19:56:42.367322: Current learning rate: 0.0039 
2024-05-08 19:57:23.620964: train_loss -0.8669 
2024-05-08 19:57:23.621140: val_loss -0.8768 
2024-05-08 19:57:23.621188: Pseudo dice [0.9342] 
2024-05-08 19:57:23.621241: Epoch time: 41.25 s 
2024-05-08 19:57:23.981792: Yayy! New best EMA pseudo Dice: 0.9288 
2024-05-08 19:57:25.414112:  
2024-05-08 19:57:25.414260: Epoch 650 
2024-05-08 19:57:25.414350: Current learning rate: 0.00389 
2024-05-08 19:58:06.594055: train_loss -0.86 
2024-05-08 19:58:06.594235: val_loss -0.8764 
2024-05-08 19:58:06.594285: Pseudo dice [0.9291] 
2024-05-08 19:58:06.594354: Epoch time: 41.18 s 
2024-05-08 19:58:06.594398: Yayy! New best EMA pseudo Dice: 0.9288 
2024-05-08 19:58:08.030951:  
2024-05-08 19:58:08.031091: Epoch 651 
2024-05-08 19:58:08.031185: Current learning rate: 0.00388 
2024-05-08 19:58:49.131749: train_loss -0.8655 
2024-05-08 19:58:49.131946: val_loss -0.8794 
2024-05-08 19:58:49.132003: Pseudo dice [0.9363] 
2024-05-08 19:58:49.132058: Epoch time: 41.1 s 
2024-05-08 19:58:49.132102: Yayy! New best EMA pseudo Dice: 0.9295 
2024-05-08 19:58:50.740186:  
2024-05-08 19:58:50.740340: Epoch 652 
2024-05-08 19:58:50.740433: Current learning rate: 0.00387 
2024-05-08 19:59:31.797928: train_loss -0.8557 
2024-05-08 19:59:31.798112: val_loss -0.8749 
2024-05-08 19:59:31.798160: Pseudo dice [0.932] 
2024-05-08 19:59:31.798216: Epoch time: 41.06 s 
2024-05-08 19:59:31.798259: Yayy! New best EMA pseudo Dice: 0.9298 
2024-05-08 19:59:33.237926:  
2024-05-08 19:59:33.238076: Epoch 653 
2024-05-08 19:59:33.238170: Current learning rate: 0.00386 
2024-05-08 20:00:14.254431: train_loss -0.8682 
2024-05-08 20:00:14.254645: val_loss -0.8779 
2024-05-08 20:00:14.254696: Pseudo dice [0.9332] 
2024-05-08 20:00:14.254752: Epoch time: 41.02 s 
2024-05-08 20:00:14.254795: Yayy! New best EMA pseudo Dice: 0.9301 
2024-05-08 20:00:15.705769:  
2024-05-08 20:00:15.705979: Epoch 654 
2024-05-08 20:00:15.706091: Current learning rate: 0.00385 
2024-05-08 20:00:56.728990: train_loss -0.8668 
2024-05-08 20:00:56.729168: val_loss -0.8598 
2024-05-08 20:00:56.729218: Pseudo dice [0.9235] 
2024-05-08 20:00:56.729274: Epoch time: 41.02 s 
2024-05-08 20:00:57.815735:  
2024-05-08 20:00:57.815874: Epoch 655 
2024-05-08 20:00:57.815969: Current learning rate: 0.00384 
2024-05-08 20:01:38.870018: train_loss -0.8626 
2024-05-08 20:01:38.870203: val_loss -0.8635 
2024-05-08 20:01:38.870269: Pseudo dice [0.9255] 
2024-05-08 20:01:38.870326: Epoch time: 41.06 s 
2024-05-08 20:01:39.958523:  
2024-05-08 20:01:39.958658: Epoch 656 
2024-05-08 20:01:39.958750: Current learning rate: 0.00383 
2024-05-08 20:02:20.931739: train_loss -0.8625 
2024-05-08 20:02:20.931922: val_loss -0.8686 
2024-05-08 20:02:20.931971: Pseudo dice [0.9287] 
2024-05-08 20:02:20.932027: Epoch time: 40.97 s 
2024-05-08 20:02:22.049120:  
2024-05-08 20:02:22.049322: Epoch 657 
2024-05-08 20:02:22.049416: Current learning rate: 0.00382 
2024-05-08 20:03:02.992807: train_loss -0.8681 
2024-05-08 20:03:02.992988: val_loss -0.8726 
2024-05-08 20:03:02.993038: Pseudo dice [0.9297] 
2024-05-08 20:03:02.993095: Epoch time: 40.94 s 
2024-05-08 20:03:04.275928:  
2024-05-08 20:03:04.276089: Epoch 658 
2024-05-08 20:03:04.276181: Current learning rate: 0.00381 
2024-05-08 20:03:45.266707: train_loss -0.86 
2024-05-08 20:03:45.266898: val_loss -0.8668 
2024-05-08 20:03:45.266947: Pseudo dice [0.9297] 
2024-05-08 20:03:45.267005: Epoch time: 40.99 s 
2024-05-08 20:03:46.357099:  
2024-05-08 20:03:46.357308: Epoch 659 
2024-05-08 20:03:46.357404: Current learning rate: 0.0038 
2024-05-08 20:04:27.785682: train_loss -0.8691 
2024-05-08 20:04:27.785870: val_loss -0.8749 
2024-05-08 20:04:27.785918: Pseudo dice [0.9341] 
2024-05-08 20:04:27.785970: Epoch time: 41.43 s 
2024-05-08 20:04:28.878163:  
2024-05-08 20:04:28.878311: Epoch 660 
2024-05-08 20:04:28.878413: Current learning rate: 0.00379 
2024-05-08 20:05:09.933134: train_loss -0.8668 
2024-05-08 20:05:09.933302: val_loss -0.8713 
2024-05-08 20:05:09.933358: Pseudo dice [0.9281] 
2024-05-08 20:05:09.933412: Epoch time: 41.06 s 
2024-05-08 20:05:11.041741:  
2024-05-08 20:05:11.041878: Epoch 661 
2024-05-08 20:05:11.041972: Current learning rate: 0.00378 
2024-05-08 20:05:52.102051: train_loss -0.8647 
2024-05-08 20:05:52.102236: val_loss -0.8724 
2024-05-08 20:05:52.102286: Pseudo dice [0.9331] 
2024-05-08 20:05:52.102339: Epoch time: 41.06 s 
2024-05-08 20:05:53.202516:  
2024-05-08 20:05:53.202652: Epoch 662 
2024-05-08 20:05:53.202744: Current learning rate: 0.00377 
2024-05-08 20:06:34.245961: train_loss -0.8697 
2024-05-08 20:06:34.246141: val_loss -0.8716 
2024-05-08 20:06:34.246203: Pseudo dice [0.9306] 
2024-05-08 20:06:34.246257: Epoch time: 41.04 s 
2024-05-08 20:06:35.367029:  
2024-05-08 20:06:35.367195: Epoch 663 
2024-05-08 20:06:35.367335: Current learning rate: 0.00376 
2024-05-08 20:07:16.417563: train_loss -0.8632 
2024-05-08 20:07:16.417742: val_loss -0.8732 
2024-05-08 20:07:16.417789: Pseudo dice [0.9317] 
2024-05-08 20:07:16.417842: Epoch time: 41.05 s 
2024-05-08 20:07:17.710598:  
2024-05-08 20:07:17.710835: Epoch 664 
2024-05-08 20:07:17.710929: Current learning rate: 0.00375 
2024-05-08 20:07:58.735140: train_loss -0.8648 
2024-05-08 20:07:58.735326: val_loss -0.8708 
2024-05-08 20:07:58.735375: Pseudo dice [0.9306] 
2024-05-08 20:07:58.735430: Epoch time: 41.03 s 
2024-05-08 20:07:58.735473: Yayy! New best EMA pseudo Dice: 0.9302 
2024-05-08 20:08:00.198441:  
2024-05-08 20:08:00.198654: Epoch 665 
2024-05-08 20:08:00.198749: Current learning rate: 0.00374 
2024-05-08 20:08:41.246454: train_loss -0.8696 
2024-05-08 20:08:41.246636: val_loss -0.8609 
2024-05-08 20:08:41.246686: Pseudo dice [0.9289] 
2024-05-08 20:08:41.246738: Epoch time: 41.05 s 
2024-05-08 20:08:42.349589:  
2024-05-08 20:08:42.349733: Epoch 666 
2024-05-08 20:08:42.349834: Current learning rate: 0.00373 
2024-05-08 20:09:23.387650: train_loss -0.8689 
2024-05-08 20:09:23.387837: val_loss -0.8617 
2024-05-08 20:09:23.387887: Pseudo dice [0.9277] 
2024-05-08 20:09:23.387940: Epoch time: 41.04 s 
2024-05-08 20:09:24.493293:  
2024-05-08 20:09:24.493440: Epoch 667 
2024-05-08 20:09:24.493531: Current learning rate: 0.00372 
2024-05-08 20:10:05.505719: train_loss -0.8716 
2024-05-08 20:10:05.505901: val_loss -0.8811 
2024-05-08 20:10:05.505951: Pseudo dice [0.9344] 
2024-05-08 20:10:05.506003: Epoch time: 41.01 s 
2024-05-08 20:10:05.506046: Yayy! New best EMA pseudo Dice: 0.9303 
2024-05-08 20:10:06.986802:  
2024-05-08 20:10:06.987067: Epoch 668 
2024-05-08 20:10:06.987165: Current learning rate: 0.00371 
2024-05-08 20:10:47.987836: train_loss -0.8674 
2024-05-08 20:10:47.988026: val_loss -0.874 
2024-05-08 20:10:47.988076: Pseudo dice [0.9352] 
2024-05-08 20:10:47.988129: Epoch time: 41.0 s 
2024-05-08 20:10:47.988174: Yayy! New best EMA pseudo Dice: 0.9308 
2024-05-08 20:10:49.466334:  
2024-05-08 20:10:49.466467: Epoch 669 
2024-05-08 20:10:49.466561: Current learning rate: 0.0037 
2024-05-08 20:11:30.456216: train_loss -0.8676 
2024-05-08 20:11:30.456481: val_loss -0.8701 
2024-05-08 20:11:30.456532: Pseudo dice [0.9289] 
2024-05-08 20:11:30.456585: Epoch time: 40.99 s 
2024-05-08 20:11:31.775476:  
2024-05-08 20:11:31.775686: Epoch 670 
2024-05-08 20:11:31.775790: Current learning rate: 0.00369 
2024-05-08 20:12:12.775402: train_loss -0.8622 
2024-05-08 20:12:12.775586: val_loss -0.8768 
2024-05-08 20:12:12.775636: Pseudo dice [0.93] 
2024-05-08 20:12:12.775690: Epoch time: 41.0 s 
2024-05-08 20:12:13.896116:  
2024-05-08 20:12:13.896262: Epoch 671 
2024-05-08 20:12:13.896353: Current learning rate: 0.00368 
2024-05-08 20:12:54.900457: train_loss -0.8698 
2024-05-08 20:12:54.900652: val_loss -0.8614 
2024-05-08 20:12:54.900703: Pseudo dice [0.9264] 
2024-05-08 20:12:54.900757: Epoch time: 41.01 s 
2024-05-08 20:12:56.024346:  
2024-05-08 20:12:56.024564: Epoch 672 
2024-05-08 20:12:56.024669: Current learning rate: 0.00367 
2024-05-08 20:13:37.033650: train_loss -0.8635 
2024-05-08 20:13:37.033837: val_loss -0.8763 
2024-05-08 20:13:37.033886: Pseudo dice [0.9317] 
2024-05-08 20:13:37.033939: Epoch time: 41.01 s 
2024-05-08 20:13:38.164982:  
2024-05-08 20:13:38.165118: Epoch 673 
2024-05-08 20:13:38.165213: Current learning rate: 0.00366 
2024-05-08 20:14:19.118694: train_loss -0.8603 
2024-05-08 20:14:19.118876: val_loss -0.8559 
2024-05-08 20:14:19.118925: Pseudo dice [0.9212] 
2024-05-08 20:14:19.118979: Epoch time: 40.95 s 
2024-05-08 20:14:20.245985:  
2024-05-08 20:14:20.246108: Epoch 674 
2024-05-08 20:14:20.246199: Current learning rate: 0.00365 
2024-05-08 20:15:01.157102: train_loss -0.8696 
2024-05-08 20:15:01.157280: val_loss -0.865 
2024-05-08 20:15:01.157331: Pseudo dice [0.9268] 
2024-05-08 20:15:01.157395: Epoch time: 40.91 s 
2024-05-08 20:15:02.274902:  
2024-05-08 20:15:02.275033: Epoch 675 
2024-05-08 20:15:02.275124: Current learning rate: 0.00364 
2024-05-08 20:15:43.118630: train_loss -0.8678 
2024-05-08 20:15:43.118826: val_loss -0.874 
2024-05-08 20:15:43.118876: Pseudo dice [0.9301] 
2024-05-08 20:15:43.118929: Epoch time: 40.84 s 
2024-05-08 20:15:44.433304:  
2024-05-08 20:15:44.433524: Epoch 676 
2024-05-08 20:15:44.433619: Current learning rate: 0.00363 
2024-05-08 20:16:25.358575: train_loss -0.864 
2024-05-08 20:16:25.358756: val_loss -0.8608 
2024-05-08 20:16:25.358804: Pseudo dice [0.925] 
2024-05-08 20:16:25.358857: Epoch time: 40.93 s 
2024-05-08 20:16:26.483454:  
2024-05-08 20:16:26.483595: Epoch 677 
2024-05-08 20:16:26.483686: Current learning rate: 0.00362 
2024-05-08 20:17:07.428977: train_loss -0.8673 
2024-05-08 20:17:07.429159: val_loss -0.8746 
2024-05-08 20:17:07.429208: Pseudo dice [0.9308] 
2024-05-08 20:17:07.429263: Epoch time: 40.95 s 
2024-05-08 20:17:08.546894:  
2024-05-08 20:17:08.547024: Epoch 678 
2024-05-08 20:17:08.547116: Current learning rate: 0.00361 
2024-05-08 20:17:49.559218: train_loss -0.8677 
2024-05-08 20:17:49.559402: val_loss -0.8604 
2024-05-08 20:17:49.559453: Pseudo dice [0.9244] 
2024-05-08 20:17:49.559506: Epoch time: 41.01 s 
2024-05-08 20:17:50.683833:  
2024-05-08 20:17:50.683963: Epoch 679 
2024-05-08 20:17:50.684055: Current learning rate: 0.0036 
2024-05-08 20:18:31.684932: train_loss -0.8636 
2024-05-08 20:18:31.685110: val_loss -0.8716 
2024-05-08 20:18:31.685159: Pseudo dice [0.9295] 
2024-05-08 20:18:31.685212: Epoch time: 41.0 s 
2024-05-08 20:18:32.811116:  
2024-05-08 20:18:32.811291: Epoch 680 
2024-05-08 20:18:32.811387: Current learning rate: 0.00359 
2024-05-08 20:19:13.776962: train_loss -0.869 
2024-05-08 20:19:13.777146: val_loss -0.8701 
2024-05-08 20:19:13.777196: Pseudo dice [0.9263] 
2024-05-08 20:19:13.777250: Epoch time: 40.97 s 
2024-05-08 20:19:14.903609:  
2024-05-08 20:19:14.903734: Epoch 681 
2024-05-08 20:19:14.903823: Current learning rate: 0.00358 
2024-05-08 20:19:56.333030: train_loss -0.8659 
2024-05-08 20:19:56.333205: val_loss -0.8741 
2024-05-08 20:19:56.333256: Pseudo dice [0.929] 
2024-05-08 20:19:56.333309: Epoch time: 41.43 s 
2024-05-08 20:19:57.661164:  
2024-05-08 20:19:57.661369: Epoch 682 
2024-05-08 20:19:57.661502: Current learning rate: 0.00357 
2024-05-08 20:20:38.687448: train_loss -0.8641 
2024-05-08 20:20:38.687658: val_loss -0.8721 
2024-05-08 20:20:38.687708: Pseudo dice [0.9309] 
2024-05-08 20:20:38.687761: Epoch time: 41.03 s 
2024-05-08 20:20:39.807401:  
2024-05-08 20:20:39.807583: Epoch 683 
2024-05-08 20:20:39.807674: Current learning rate: 0.00356 
2024-05-08 20:21:20.772581: train_loss -0.8603 
2024-05-08 20:21:20.772778: val_loss -0.8788 
2024-05-08 20:21:20.772842: Pseudo dice [0.9328] 
2024-05-08 20:21:20.772908: Epoch time: 40.97 s 
2024-05-08 20:21:21.916292:  
2024-05-08 20:21:21.916431: Epoch 684 
2024-05-08 20:21:21.916538: Current learning rate: 0.00355 
2024-05-08 20:22:02.857830: train_loss -0.8705 
2024-05-08 20:22:02.858022: val_loss -0.874 
2024-05-08 20:22:02.858086: Pseudo dice [0.9295] 
2024-05-08 20:22:02.858154: Epoch time: 40.94 s 
2024-05-08 20:22:03.982343:  
2024-05-08 20:22:03.982532: Epoch 685 
2024-05-08 20:22:03.982701: Current learning rate: 0.00354 
2024-05-08 20:22:44.994408: train_loss -0.8662 
2024-05-08 20:22:44.994585: val_loss -0.8662 
2024-05-08 20:22:44.994634: Pseudo dice [0.9267] 
2024-05-08 20:22:44.994687: Epoch time: 41.01 s 
2024-05-08 20:22:46.120005:  
2024-05-08 20:22:46.120128: Epoch 686 
2024-05-08 20:22:46.120217: Current learning rate: 0.00353 
2024-05-08 20:23:27.162413: train_loss -0.8545 
2024-05-08 20:23:27.162615: val_loss -0.8792 
2024-05-08 20:23:27.162664: Pseudo dice [0.9359] 
2024-05-08 20:23:27.162719: Epoch time: 41.04 s 
2024-05-08 20:23:28.285974:  
2024-05-08 20:23:28.286096: Epoch 687 
2024-05-08 20:23:28.286188: Current learning rate: 0.00352 
2024-05-08 20:24:09.350594: train_loss -0.8624 
2024-05-08 20:24:09.350784: val_loss -0.8799 
2024-05-08 20:24:09.350835: Pseudo dice [0.9333] 
2024-05-08 20:24:09.350888: Epoch time: 41.07 s 
2024-05-08 20:24:10.713048:  
2024-05-08 20:24:10.713266: Epoch 688 
2024-05-08 20:24:10.713361: Current learning rate: 0.00351 
2024-05-08 20:24:51.813733: train_loss -0.851 
2024-05-08 20:24:51.813917: val_loss -0.8781 
2024-05-08 20:24:51.813966: Pseudo dice [0.9312] 
2024-05-08 20:24:51.814019: Epoch time: 41.1 s 
2024-05-08 20:24:52.946840:  
2024-05-08 20:24:52.946977: Epoch 689 
2024-05-08 20:24:52.947068: Current learning rate: 0.0035 
2024-05-08 20:25:34.079488: train_loss -0.8617 
2024-05-08 20:25:34.079664: val_loss -0.8667 
2024-05-08 20:25:34.079713: Pseudo dice [0.927] 
2024-05-08 20:25:34.079767: Epoch time: 41.13 s 
2024-05-08 20:25:35.203352:  
2024-05-08 20:25:35.203485: Epoch 690 
2024-05-08 20:25:35.203576: Current learning rate: 0.00349 
2024-05-08 20:26:16.362835: train_loss -0.8545 
2024-05-08 20:26:16.363009: val_loss -0.8712 
2024-05-08 20:26:16.363059: Pseudo dice [0.9313] 
2024-05-08 20:26:16.363114: Epoch time: 41.16 s 
2024-05-08 20:26:17.497539:  
2024-05-08 20:26:17.497677: Epoch 691 
2024-05-08 20:26:17.497772: Current learning rate: 0.00348 
2024-05-08 20:26:58.689350: train_loss -0.8676 
2024-05-08 20:26:58.689526: val_loss -0.8797 
2024-05-08 20:26:58.689578: Pseudo dice [0.9342] 
2024-05-08 20:26:58.689635: Epoch time: 41.19 s 
2024-05-08 20:26:59.819638:  
2024-05-08 20:26:59.819765: Epoch 692 
2024-05-08 20:26:59.819853: Current learning rate: 0.00346 
2024-05-08 20:27:41.056440: train_loss -0.8615 
2024-05-08 20:27:41.056621: val_loss -0.8589 
2024-05-08 20:27:41.056676: Pseudo dice [0.927] 
2024-05-08 20:27:41.056738: Epoch time: 41.24 s 
2024-05-08 20:27:42.171467:  
2024-05-08 20:27:42.171596: Epoch 693 
2024-05-08 20:27:42.171687: Current learning rate: 0.00345 
2024-05-08 20:28:23.419308: train_loss -0.8677 
2024-05-08 20:28:23.419498: val_loss -0.8763 
2024-05-08 20:28:23.419549: Pseudo dice [0.9321] 
2024-05-08 20:28:23.419605: Epoch time: 41.25 s 
2024-05-08 20:28:24.739727:  
2024-05-08 20:28:24.739879: Epoch 694 
2024-05-08 20:28:24.739967: Current learning rate: 0.00344 
2024-05-08 20:29:06.012308: train_loss -0.872 
2024-05-08 20:29:06.012493: val_loss -0.869 
2024-05-08 20:29:06.012544: Pseudo dice [0.9266] 
2024-05-08 20:29:06.012601: Epoch time: 41.27 s 
2024-05-08 20:29:07.139187:  
2024-05-08 20:29:07.139331: Epoch 695 
2024-05-08 20:29:07.139422: Current learning rate: 0.00343 
2024-05-08 20:29:48.407382: train_loss -0.8683 
2024-05-08 20:29:48.407549: val_loss -0.8611 
2024-05-08 20:29:48.407597: Pseudo dice [0.9295] 
2024-05-08 20:29:48.407650: Epoch time: 41.27 s 
2024-05-08 20:29:49.538027:  
2024-05-08 20:29:49.538162: Epoch 696 
2024-05-08 20:29:49.538253: Current learning rate: 0.00342 
2024-05-08 20:30:30.783939: train_loss -0.8682 
2024-05-08 20:30:30.784126: val_loss -0.8714 
2024-05-08 20:30:30.784176: Pseudo dice [0.9296] 
2024-05-08 20:30:30.784231: Epoch time: 41.25 s 
2024-05-08 20:30:31.924207:  
2024-05-08 20:30:31.924347: Epoch 697 
2024-05-08 20:30:31.924440: Current learning rate: 0.00341 
2024-05-08 20:31:13.211234: train_loss -0.8604 
2024-05-08 20:31:13.211412: val_loss -0.8688 
2024-05-08 20:31:13.211462: Pseudo dice [0.931] 
2024-05-08 20:31:13.211514: Epoch time: 41.29 s 
2024-05-08 20:31:14.333384:  
2024-05-08 20:31:14.333517: Epoch 698 
2024-05-08 20:31:14.333605: Current learning rate: 0.0034 
2024-05-08 20:31:55.599491: train_loss -0.8666 
2024-05-08 20:31:55.599666: val_loss -0.8713 
2024-05-08 20:31:55.599721: Pseudo dice [0.9304] 
2024-05-08 20:31:55.599773: Epoch time: 41.27 s 
2024-05-08 20:31:56.740277:  
2024-05-08 20:31:56.740486: Epoch 699 
2024-05-08 20:31:56.740583: Current learning rate: 0.00339 
2024-05-08 20:32:38.052735: train_loss -0.8675 
2024-05-08 20:32:38.052894: val_loss -0.8811 
2024-05-08 20:32:38.052942: Pseudo dice [0.9347] 
2024-05-08 20:32:38.052996: Epoch time: 41.31 s 
2024-05-08 20:32:39.725582:  
2024-05-08 20:32:39.725852: Epoch 700 
2024-05-08 20:32:39.725968: Current learning rate: 0.00338 
2024-05-08 20:33:21.027537: train_loss -0.8669 
2024-05-08 20:33:21.027721: val_loss -0.8676 
2024-05-08 20:33:21.027770: Pseudo dice [0.9294] 
2024-05-08 20:33:21.027822: Epoch time: 41.3 s 
2024-05-08 20:33:22.149692:  
2024-05-08 20:33:22.149839: Epoch 701 
2024-05-08 20:33:22.149930: Current learning rate: 0.00337 
2024-05-08 20:34:03.451954: train_loss -0.8625 
2024-05-08 20:34:03.452140: val_loss -0.8513 
2024-05-08 20:34:03.452258: Pseudo dice [0.9287] 
2024-05-08 20:34:03.452317: Epoch time: 41.3 s 
2024-05-08 20:34:04.573975:  
2024-05-08 20:34:04.574127: Epoch 702 
2024-05-08 20:34:04.574217: Current learning rate: 0.00336 
2024-05-08 20:34:45.886023: train_loss -0.859 
2024-05-08 20:34:45.886201: val_loss -0.8717 
2024-05-08 20:34:45.886249: Pseudo dice [0.9308] 
2024-05-08 20:34:45.886301: Epoch time: 41.31 s 
2024-05-08 20:34:46.999193:  
2024-05-08 20:34:46.999336: Epoch 703 
2024-05-08 20:34:46.999438: Current learning rate: 0.00335 
2024-05-08 20:35:28.405331: train_loss -0.8674 
2024-05-08 20:35:28.405506: val_loss -0.877 
2024-05-08 20:35:28.405557: Pseudo dice [0.9334] 
2024-05-08 20:35:28.405611: Epoch time: 41.41 s 
2024-05-08 20:35:29.535247:  
2024-05-08 20:35:29.535379: Epoch 704 
2024-05-08 20:35:29.535468: Current learning rate: 0.00334 
2024-05-08 20:36:10.828034: train_loss -0.8638 
2024-05-08 20:36:10.828206: val_loss -0.8735 
2024-05-08 20:36:10.828257: Pseudo dice [0.9298] 
2024-05-08 20:36:10.828311: Epoch time: 41.29 s 
2024-05-08 20:36:12.129812:  
2024-05-08 20:36:12.129957: Epoch 705 
2024-05-08 20:36:12.130045: Current learning rate: 0.00333 
2024-05-08 20:36:53.445457: train_loss -0.8728 
2024-05-08 20:36:53.445643: val_loss -0.8724 
2024-05-08 20:36:53.445695: Pseudo dice [0.9296] 
2024-05-08 20:36:53.445749: Epoch time: 41.32 s 
2024-05-08 20:36:54.574664:  
2024-05-08 20:36:54.574810: Epoch 706 
2024-05-08 20:36:54.574909: Current learning rate: 0.00332 
2024-05-08 20:37:35.993030: train_loss -0.8651 
2024-05-08 20:37:35.993211: val_loss -0.8677 
2024-05-08 20:37:35.993260: Pseudo dice [0.9297] 
2024-05-08 20:37:35.993312: Epoch time: 41.42 s 
2024-05-08 20:37:37.128615:  
2024-05-08 20:37:37.128755: Epoch 707 
2024-05-08 20:37:37.128844: Current learning rate: 0.00331 
2024-05-08 20:38:18.533894: train_loss -0.8618 
2024-05-08 20:38:18.534076: val_loss -0.8866 
2024-05-08 20:38:18.534249: Pseudo dice [0.94] 
2024-05-08 20:38:18.534308: Epoch time: 41.41 s 
2024-05-08 20:38:18.534356: Yayy! New best EMA pseudo Dice: 0.9313 
2024-05-08 20:38:20.016389:  
2024-05-08 20:38:20.016528: Epoch 708 
2024-05-08 20:38:20.016618: Current learning rate: 0.0033 
2024-05-08 20:39:01.442741: train_loss -0.8613 
2024-05-08 20:39:01.442924: val_loss -0.8635 
2024-05-08 20:39:01.442975: Pseudo dice [0.9281] 
2024-05-08 20:39:01.443029: Epoch time: 41.43 s 
2024-05-08 20:39:02.575916:  
2024-05-08 20:39:02.576050: Epoch 709 
2024-05-08 20:39:02.576146: Current learning rate: 0.00329 
2024-05-08 20:39:43.920949: train_loss -0.867 
2024-05-08 20:39:43.921137: val_loss -0.865 
2024-05-08 20:39:43.921186: Pseudo dice [0.9258] 
2024-05-08 20:39:43.921241: Epoch time: 41.35 s 
2024-05-08 20:39:45.052018:  
2024-05-08 20:39:45.052142: Epoch 710 
2024-05-08 20:39:45.052229: Current learning rate: 0.00328 
2024-05-08 20:40:26.466825: train_loss -0.864 
2024-05-08 20:40:26.466996: val_loss -0.8574 
2024-05-08 20:40:26.467045: Pseudo dice [0.9216] 
2024-05-08 20:40:26.467098: Epoch time: 41.42 s 
2024-05-08 20:40:27.781670:  
2024-05-08 20:40:27.781880: Epoch 711 
2024-05-08 20:40:27.781974: Current learning rate: 0.00327 
2024-05-08 20:41:09.127678: train_loss -0.8667 
2024-05-08 20:41:09.127868: val_loss -0.8661 
2024-05-08 20:41:09.127917: Pseudo dice [0.9294] 
2024-05-08 20:41:09.127970: Epoch time: 41.35 s 
2024-05-08 20:41:10.250482:  
2024-05-08 20:41:10.250627: Epoch 712 
2024-05-08 20:41:10.250715: Current learning rate: 0.00326 
2024-05-08 20:41:51.643578: train_loss -0.8653 
2024-05-08 20:41:51.643766: val_loss -0.8724 
2024-05-08 20:41:51.643816: Pseudo dice [0.9313] 
2024-05-08 20:41:51.643881: Epoch time: 41.39 s 
2024-05-08 20:41:52.765935:  
2024-05-08 20:41:52.766126: Epoch 713 
2024-05-08 20:41:52.766222: Current learning rate: 0.00325 
2024-05-08 20:42:34.073342: train_loss -0.8691 
2024-05-08 20:42:34.073530: val_loss -0.8764 
2024-05-08 20:42:34.073579: Pseudo dice [0.9311] 
2024-05-08 20:42:34.073632: Epoch time: 41.31 s 
2024-05-08 20:42:35.185395:  
2024-05-08 20:42:35.185600: Epoch 714 
2024-05-08 20:42:35.185715: Current learning rate: 0.00324 
2024-05-08 20:43:16.360111: train_loss -0.8708 
2024-05-08 20:43:16.360291: val_loss -0.8643 
2024-05-08 20:43:16.360339: Pseudo dice [0.9282] 
2024-05-08 20:43:16.360392: Epoch time: 41.18 s 
2024-05-08 20:43:17.469859:  
2024-05-08 20:43:17.470070: Epoch 715 
2024-05-08 20:43:17.470188: Current learning rate: 0.00323 
2024-05-08 20:43:58.687086: train_loss -0.8729 
2024-05-08 20:43:58.687263: val_loss -0.8752 
2024-05-08 20:43:58.687313: Pseudo dice [0.9316] 
2024-05-08 20:43:58.687367: Epoch time: 41.22 s 
2024-05-08 20:43:59.796320:  
2024-05-08 20:43:59.796514: Epoch 716 
2024-05-08 20:43:59.796611: Current learning rate: 0.00322 
2024-05-08 20:44:40.925433: train_loss -0.8649 
2024-05-08 20:44:40.925612: val_loss -0.8762 
2024-05-08 20:44:40.925661: Pseudo dice [0.9296] 
2024-05-08 20:44:40.925714: Epoch time: 41.13 s 
2024-05-08 20:44:42.232172:  
2024-05-08 20:44:42.232320: Epoch 717 
2024-05-08 20:44:42.232412: Current learning rate: 0.00321 
2024-05-08 20:45:23.375939: train_loss -0.8633 
2024-05-08 20:45:23.376121: val_loss -0.8749 
2024-05-08 20:45:23.376171: Pseudo dice [0.9301] 
2024-05-08 20:45:23.376224: Epoch time: 41.14 s 
2024-05-08 20:45:24.493340:  
2024-05-08 20:45:24.493549: Epoch 718 
2024-05-08 20:45:24.493639: Current learning rate: 0.0032 
2024-05-08 20:46:05.615708: train_loss -0.8646 
2024-05-08 20:46:05.615947: val_loss -0.8809 
2024-05-08 20:46:05.616000: Pseudo dice [0.9328] 
2024-05-08 20:46:05.616052: Epoch time: 41.12 s 
2024-05-08 20:46:06.741613:  
2024-05-08 20:46:06.741760: Epoch 719 
2024-05-08 20:46:06.741853: Current learning rate: 0.00319 
2024-05-08 20:46:47.866239: train_loss -0.8638 
2024-05-08 20:46:47.866421: val_loss -0.8691 
2024-05-08 20:46:47.866471: Pseudo dice [0.9278] 
2024-05-08 20:46:47.866523: Epoch time: 41.13 s 
2024-05-08 20:46:48.978226:  
2024-05-08 20:46:48.978433: Epoch 720 
2024-05-08 20:46:48.978528: Current learning rate: 0.00318 
2024-05-08 20:47:30.063623: train_loss -0.8645 
2024-05-08 20:47:30.063796: val_loss -0.8632 
2024-05-08 20:47:30.063844: Pseudo dice [0.9294] 
2024-05-08 20:47:30.063897: Epoch time: 41.09 s 
2024-05-08 20:47:31.178051:  
2024-05-08 20:47:31.178248: Epoch 721 
2024-05-08 20:47:31.178347: Current learning rate: 0.00317 
2024-05-08 20:48:12.266308: train_loss -0.8662 
2024-05-08 20:48:12.266484: val_loss -0.8843 
2024-05-08 20:48:12.266532: Pseudo dice [0.9346] 
2024-05-08 20:48:12.266584: Epoch time: 41.09 s 
2024-05-08 20:48:13.384033:  
2024-05-08 20:48:13.384158: Epoch 722 
2024-05-08 20:48:13.384248: Current learning rate: 0.00316 
2024-05-08 20:48:54.452141: train_loss -0.8666 
2024-05-08 20:48:54.452300: val_loss -0.8765 
2024-05-08 20:48:54.452348: Pseudo dice [0.9352] 
2024-05-08 20:48:54.452402: Epoch time: 41.07 s 
2024-05-08 20:48:55.753317:  
2024-05-08 20:48:55.753494: Epoch 723 
2024-05-08 20:48:55.753632: Current learning rate: 0.00315 
2024-05-08 20:49:36.835834: train_loss -0.8637 
2024-05-08 20:49:36.836016: val_loss -0.8755 
2024-05-08 20:49:36.836065: Pseudo dice [0.9293] 
2024-05-08 20:49:36.836118: Epoch time: 41.08 s 
2024-05-08 20:49:37.950432:  
2024-05-08 20:49:37.950576: Epoch 724 
2024-05-08 20:49:37.950673: Current learning rate: 0.00314 
2024-05-08 20:50:19.012860: train_loss -0.8695 
2024-05-08 20:50:19.013046: val_loss -0.8708 
2024-05-08 20:50:19.013096: Pseudo dice [0.9279] 
2024-05-08 20:50:19.013149: Epoch time: 41.06 s 
2024-05-08 20:50:20.130104:  
2024-05-08 20:50:20.130392: Epoch 725 
2024-05-08 20:50:20.130488: Current learning rate: 0.00313 
2024-05-08 20:51:01.202233: train_loss -0.8675 
2024-05-08 20:51:01.202425: val_loss -0.8725 
2024-05-08 20:51:01.202475: Pseudo dice [0.9321] 
2024-05-08 20:51:01.202528: Epoch time: 41.07 s 
2024-05-08 20:51:02.322871:  
2024-05-08 20:51:02.323023: Epoch 726 
2024-05-08 20:51:02.323114: Current learning rate: 0.00312 
2024-05-08 20:51:43.391640: train_loss -0.87 
2024-05-08 20:51:43.391832: val_loss -0.8843 
2024-05-08 20:51:43.391881: Pseudo dice [0.9346] 
2024-05-08 20:51:43.391933: Epoch time: 41.07 s 
2024-05-08 20:51:44.514400:  
2024-05-08 20:51:44.514568: Epoch 727 
2024-05-08 20:51:44.514668: Current learning rate: 0.00311 
2024-05-08 20:52:25.552149: train_loss -0.8678 
2024-05-08 20:52:25.552331: val_loss -0.8661 
2024-05-08 20:52:25.552380: Pseudo dice [0.9321] 
2024-05-08 20:52:25.552432: Epoch time: 41.04 s 
2024-05-08 20:52:26.689254:  
2024-05-08 20:52:26.689441: Epoch 728 
2024-05-08 20:52:26.689576: Current learning rate: 0.0031 
2024-05-08 20:53:07.725342: train_loss -0.8683 
2024-05-08 20:53:07.725517: val_loss -0.8652 
2024-05-08 20:53:07.725566: Pseudo dice [0.9278] 
2024-05-08 20:53:07.725619: Epoch time: 41.04 s 
2024-05-08 20:53:08.836287:  
2024-05-08 20:53:08.836419: Epoch 729 
2024-05-08 20:53:08.836510: Current learning rate: 0.00309 
2024-05-08 20:53:49.861752: train_loss -0.8691 
2024-05-08 20:53:49.861930: val_loss -0.8687 
2024-05-08 20:53:49.861980: Pseudo dice [0.9257] 
2024-05-08 20:53:49.862034: Epoch time: 41.03 s 
2024-05-08 20:53:51.181584:  
2024-05-08 20:53:51.181732: Epoch 730 
2024-05-08 20:53:51.181824: Current learning rate: 0.00308 
2024-05-08 20:54:32.246416: train_loss -0.8672 
2024-05-08 20:54:32.246632: val_loss -0.8713 
2024-05-08 20:54:32.246684: Pseudo dice [0.9321] 
2024-05-08 20:54:32.246738: Epoch time: 41.07 s 
2024-05-08 20:54:33.360205:  
2024-05-08 20:54:33.360367: Epoch 731 
2024-05-08 20:54:33.360459: Current learning rate: 0.00307 
2024-05-08 20:55:14.447819: train_loss -0.8651 
2024-05-08 20:55:14.448002: val_loss -0.8667 
2024-05-08 20:55:14.448051: Pseudo dice [0.9276] 
2024-05-08 20:55:14.448105: Epoch time: 41.09 s 
2024-05-08 20:55:15.560602:  
2024-05-08 20:55:15.560833: Epoch 732 
2024-05-08 20:55:15.560956: Current learning rate: 0.00306 
2024-05-08 20:55:56.625251: train_loss -0.8638 
2024-05-08 20:55:56.625438: val_loss -0.8731 
2024-05-08 20:55:56.625487: Pseudo dice [0.9334] 
2024-05-08 20:55:56.625540: Epoch time: 41.07 s 
2024-05-08 20:55:57.737678:  
2024-05-08 20:55:57.737820: Epoch 733 
2024-05-08 20:55:57.737911: Current learning rate: 0.00305 
2024-05-08 20:56:38.726949: train_loss -0.8659 
2024-05-08 20:56:38.727130: val_loss -0.8732 
2024-05-08 20:56:38.727179: Pseudo dice [0.9256] 
2024-05-08 20:56:38.727232: Epoch time: 40.99 s 
2024-05-08 20:56:40.036208:  
2024-05-08 20:56:40.036349: Epoch 734 
2024-05-08 20:56:40.036441: Current learning rate: 0.00304 
2024-05-08 20:57:21.001001: train_loss -0.864 
2024-05-08 20:57:21.001178: val_loss -0.8728 
2024-05-08 20:57:21.001227: Pseudo dice [0.9355] 
2024-05-08 20:57:21.001280: Epoch time: 40.97 s 
2024-05-08 20:57:22.116835:  
2024-05-08 20:57:22.116960: Epoch 735 
2024-05-08 20:57:22.117051: Current learning rate: 0.00303 
2024-05-08 20:58:03.069596: train_loss -0.8701 
2024-05-08 20:58:03.069773: val_loss -0.883 
2024-05-08 20:58:03.069823: Pseudo dice [0.9348] 
2024-05-08 20:58:03.069880: Epoch time: 40.95 s 
2024-05-08 20:58:04.375755:  
2024-05-08 20:58:04.375960: Epoch 736 
2024-05-08 20:58:04.376055: Current learning rate: 0.00302 
2024-05-08 20:58:45.343633: train_loss -0.8616 
2024-05-08 20:58:45.343828: val_loss -0.872 
2024-05-08 20:58:45.343879: Pseudo dice [0.9305] 
2024-05-08 20:58:45.343932: Epoch time: 40.97 s 
2024-05-08 20:58:46.463028:  
2024-05-08 20:58:46.463236: Epoch 737 
2024-05-08 20:58:46.463370: Current learning rate: 0.00301 
2024-05-08 20:59:27.418643: train_loss -0.867 
2024-05-08 20:59:27.418820: val_loss -0.8792 
2024-05-08 20:59:27.418869: Pseudo dice [0.9336] 
2024-05-08 20:59:27.418925: Epoch time: 40.96 s 
2024-05-08 20:59:28.532264:  
2024-05-08 20:59:28.532569: Epoch 738 
2024-05-08 20:59:28.532674: Current learning rate: 0.003 
2024-05-08 21:00:09.498792: train_loss -0.8674 
2024-05-08 21:00:09.498970: val_loss -0.8888 
2024-05-08 21:00:09.499020: Pseudo dice [0.9372] 
2024-05-08 21:00:09.499076: Epoch time: 40.97 s 
2024-05-08 21:00:09.499119: Yayy! New best EMA pseudo Dice: 0.9318 
2024-05-08 21:00:10.965275:  
2024-05-08 21:00:10.965412: Epoch 739 
2024-05-08 21:00:10.965506: Current learning rate: 0.00299 
2024-05-08 21:00:51.895862: train_loss -0.8661 
2024-05-08 21:00:51.896040: val_loss -0.8558 
2024-05-08 21:00:51.896089: Pseudo dice [0.9221] 
2024-05-08 21:00:51.896142: Epoch time: 40.93 s 
2024-05-08 21:00:53.015365:  
2024-05-08 21:00:53.015546: Epoch 740 
2024-05-08 21:00:53.015642: Current learning rate: 0.00297 
2024-05-08 21:01:33.955953: train_loss -0.8609 
2024-05-08 21:01:33.956226: val_loss -0.8608 
2024-05-08 21:01:33.956277: Pseudo dice [0.9207] 
2024-05-08 21:01:33.956330: Epoch time: 40.94 s 
2024-05-08 21:01:35.067055:  
2024-05-08 21:01:35.067262: Epoch 741 
2024-05-08 21:01:35.067358: Current learning rate: 0.00296 
2024-05-08 21:02:15.984943: train_loss -0.857 
2024-05-08 21:02:15.985114: val_loss -0.8768 
2024-05-08 21:02:15.985163: Pseudo dice [0.9313] 
2024-05-08 21:02:15.985217: Epoch time: 40.92 s 
2024-05-08 21:02:17.295724:  
2024-05-08 21:02:17.295884: Epoch 742 
2024-05-08 21:02:17.295977: Current learning rate: 0.00295 
2024-05-08 21:02:58.284873: train_loss -0.8686 
2024-05-08 21:02:58.285054: val_loss -0.866 
2024-05-08 21:02:58.285103: Pseudo dice [0.9276] 
2024-05-08 21:02:58.285157: Epoch time: 40.99 s 
2024-05-08 21:02:59.396888:  
2024-05-08 21:02:59.397133: Epoch 743 
2024-05-08 21:02:59.397227: Current learning rate: 0.00294 
2024-05-08 21:03:40.369198: train_loss -0.8646 
2024-05-08 21:03:40.369384: val_loss -0.868 
2024-05-08 21:03:40.369434: Pseudo dice [0.9305] 
2024-05-08 21:03:40.369488: Epoch time: 40.97 s 
2024-05-08 21:03:41.486506:  
2024-05-08 21:03:41.486711: Epoch 744 
2024-05-08 21:03:41.486804: Current learning rate: 0.00293 
2024-05-08 21:04:22.455691: train_loss -0.8739 
2024-05-08 21:04:22.455854: val_loss -0.8764 
2024-05-08 21:04:22.455902: Pseudo dice [0.9325] 
2024-05-08 21:04:22.455955: Epoch time: 40.97 s 
2024-05-08 21:04:23.570916:  
2024-05-08 21:04:23.571055: Epoch 745 
2024-05-08 21:04:23.571148: Current learning rate: 0.00292 
2024-05-08 21:05:04.533537: train_loss -0.8675 
2024-05-08 21:05:04.533707: val_loss -0.8826 
2024-05-08 21:05:04.533756: Pseudo dice [0.9331] 
2024-05-08 21:05:04.533810: Epoch time: 40.96 s 
2024-05-08 21:05:05.665322:  
2024-05-08 21:05:05.665460: Epoch 746 
2024-05-08 21:05:05.665550: Current learning rate: 0.00291 
2024-05-08 21:05:46.633923: train_loss -0.8691 
2024-05-08 21:05:46.634108: val_loss -0.8761 
2024-05-08 21:05:46.634157: Pseudo dice [0.933] 
2024-05-08 21:05:46.634211: Epoch time: 40.97 s 
2024-05-08 21:05:47.748164:  
2024-05-08 21:05:47.748293: Epoch 747 
2024-05-08 21:05:47.748384: Current learning rate: 0.0029 
2024-05-08 21:06:28.717566: train_loss -0.8686 
2024-05-08 21:06:28.717743: val_loss -0.8755 
2024-05-08 21:06:28.717791: Pseudo dice [0.9306] 
2024-05-08 21:06:28.717844: Epoch time: 40.97 s 
2024-05-08 21:06:30.025241:  
2024-05-08 21:06:30.025402: Epoch 748 
2024-05-08 21:06:30.025491: Current learning rate: 0.00289 
2024-05-08 21:07:10.997980: train_loss -0.8631 
2024-05-08 21:07:10.998176: val_loss -0.8766 
2024-05-08 21:07:10.998225: Pseudo dice [0.9348] 
2024-05-08 21:07:10.998278: Epoch time: 40.97 s 
2024-05-08 21:07:12.156322:  
2024-05-08 21:07:12.156535: Epoch 749 
2024-05-08 21:07:12.156630: Current learning rate: 0.00288 
2024-05-08 21:07:53.119673: train_loss -0.8678 
2024-05-08 21:07:53.119853: val_loss -0.8808 
2024-05-08 21:07:53.119903: Pseudo dice [0.9342] 
2024-05-08 21:07:53.119956: Epoch time: 40.96 s 
2024-05-08 21:07:54.592739:  
2024-05-08 21:07:54.592882: Epoch 750 
2024-05-08 21:07:54.592973: Current learning rate: 0.00287 
2024-05-08 21:08:35.557459: train_loss -0.868 
2024-05-08 21:08:35.557628: val_loss -0.8555 
2024-05-08 21:08:35.557679: Pseudo dice [0.9213] 
2024-05-08 21:08:35.557745: Epoch time: 40.97 s 
2024-05-08 21:08:36.675619:  
2024-05-08 21:08:36.675744: Epoch 751 
2024-05-08 21:08:36.675832: Current learning rate: 0.00286 
2024-05-08 21:09:17.639218: train_loss -0.8707 
2024-05-08 21:09:17.639386: val_loss -0.8741 
2024-05-08 21:09:17.639437: Pseudo dice [0.933] 
2024-05-08 21:09:17.639491: Epoch time: 40.96 s 
2024-05-08 21:09:18.751164:  
2024-05-08 21:09:18.751352: Epoch 752 
2024-05-08 21:09:18.751450: Current learning rate: 0.00285 
2024-05-08 21:09:59.706921: train_loss -0.8674 
2024-05-08 21:09:59.707098: val_loss -0.8694 
2024-05-08 21:09:59.707146: Pseudo dice [0.9279] 
2024-05-08 21:09:59.707199: Epoch time: 40.96 s 
2024-05-08 21:10:00.815683:  
2024-05-08 21:10:00.815874: Epoch 753 
2024-05-08 21:10:00.815966: Current learning rate: 0.00284 
2024-05-08 21:10:41.738678: train_loss -0.8681 
2024-05-08 21:10:41.738841: val_loss -0.874 
2024-05-08 21:10:41.738889: Pseudo dice [0.9311] 
2024-05-08 21:10:41.738943: Epoch time: 40.92 s 
2024-05-08 21:10:43.046057:  
2024-05-08 21:10:43.046206: Epoch 754 
2024-05-08 21:10:43.046297: Current learning rate: 0.00283 
2024-05-08 21:11:24.030318: train_loss -0.8636 
2024-05-08 21:11:24.030515: val_loss -0.8656 
2024-05-08 21:11:24.030565: Pseudo dice [0.926] 
2024-05-08 21:11:24.030617: Epoch time: 40.99 s 
2024-05-08 21:11:25.144155:  
2024-05-08 21:11:25.144466: Epoch 755 
2024-05-08 21:11:25.144564: Current learning rate: 0.00282 
2024-05-08 21:12:06.163333: train_loss -0.8636 
2024-05-08 21:12:06.163515: val_loss -0.8695 
2024-05-08 21:12:06.163563: Pseudo dice [0.9329] 
2024-05-08 21:12:06.163615: Epoch time: 41.02 s 
2024-05-08 21:12:07.275155:  
2024-05-08 21:12:07.275338: Epoch 756 
2024-05-08 21:12:07.275486: Current learning rate: 0.00281 
2024-05-08 21:12:48.276440: train_loss -0.8649 
2024-05-08 21:12:48.276612: val_loss -0.8688 
2024-05-08 21:12:48.276667: Pseudo dice [0.931] 
2024-05-08 21:12:48.276720: Epoch time: 41.0 s 
2024-05-08 21:12:49.388227:  
2024-05-08 21:12:49.388363: Epoch 757 
2024-05-08 21:12:49.388457: Current learning rate: 0.0028 
2024-05-08 21:13:30.400753: train_loss -0.8657 
2024-05-08 21:13:30.400940: val_loss -0.8779 
2024-05-08 21:13:30.400990: Pseudo dice [0.9368] 
2024-05-08 21:13:30.401045: Epoch time: 41.01 s 
2024-05-08 21:13:31.515230:  
2024-05-08 21:13:31.515370: Epoch 758 
2024-05-08 21:13:31.515461: Current learning rate: 0.00279 
2024-05-08 21:14:12.524166: train_loss -0.8686 
2024-05-08 21:14:12.524333: val_loss -0.8759 
2024-05-08 21:14:12.524381: Pseudo dice [0.9315] 
2024-05-08 21:14:12.524436: Epoch time: 41.01 s 
2024-05-08 21:14:13.654387:  
2024-05-08 21:14:13.654516: Epoch 759 
2024-05-08 21:14:13.654608: Current learning rate: 0.00278 
2024-05-08 21:14:54.764854: train_loss -0.8601 
2024-05-08 21:14:54.765032: val_loss -0.875 
2024-05-08 21:14:54.765082: Pseudo dice [0.9314] 
2024-05-08 21:14:54.765134: Epoch time: 41.11 s 
2024-05-08 21:14:55.888827:  
2024-05-08 21:14:55.888979: Epoch 760 
2024-05-08 21:14:55.889071: Current learning rate: 0.00277 
2024-05-08 21:15:36.989057: train_loss -0.8664 
2024-05-08 21:15:36.989230: val_loss -0.8736 
2024-05-08 21:15:36.989279: Pseudo dice [0.931] 
2024-05-08 21:15:36.989331: Epoch time: 41.1 s 
2024-05-08 21:15:38.106685:  
2024-05-08 21:15:38.106830: Epoch 761 
2024-05-08 21:15:38.106921: Current learning rate: 0.00276 
2024-05-08 21:16:19.158609: train_loss -0.869 
2024-05-08 21:16:19.158776: val_loss -0.8782 
2024-05-08 21:16:19.158825: Pseudo dice [0.9315] 
2024-05-08 21:16:19.158878: Epoch time: 41.05 s 
2024-05-08 21:16:20.282805:  
2024-05-08 21:16:20.282948: Epoch 762 
2024-05-08 21:16:20.283042: Current learning rate: 0.00275 
2024-05-08 21:17:01.344447: train_loss -0.8661 
2024-05-08 21:17:01.344639: val_loss -0.8757 
2024-05-08 21:17:01.344697: Pseudo dice [0.9316] 
2024-05-08 21:17:01.344751: Epoch time: 41.06 s 
2024-05-08 21:17:02.479592:  
2024-05-08 21:17:02.479849: Epoch 763 
2024-05-08 21:17:02.479991: Current learning rate: 0.00274 
2024-05-08 21:17:43.492947: train_loss -0.8714 
2024-05-08 21:17:43.493152: val_loss -0.8768 
2024-05-08 21:17:43.493201: Pseudo dice [0.9353] 
2024-05-08 21:17:43.493255: Epoch time: 41.01 s 
2024-05-08 21:17:44.634224:  
2024-05-08 21:17:44.634356: Epoch 764 
2024-05-08 21:17:44.634447: Current learning rate: 0.00273 
2024-05-08 21:18:25.984546: train_loss -0.8739 
2024-05-08 21:18:25.984741: val_loss -0.8731 
2024-05-08 21:18:25.984791: Pseudo dice [0.9315] 
2024-05-08 21:18:25.984843: Epoch time: 41.35 s 
2024-05-08 21:18:27.292186:  
2024-05-08 21:18:27.292401: Epoch 765 
2024-05-08 21:18:27.292528: Current learning rate: 0.00272 
2024-05-08 21:19:08.313312: train_loss -0.8706 
2024-05-08 21:19:08.313507: val_loss -0.8817 
2024-05-08 21:19:08.313557: Pseudo dice [0.934] 
2024-05-08 21:19:08.313609: Epoch time: 41.02 s 
2024-05-08 21:19:08.313653: Yayy! New best EMA pseudo Dice: 0.9318 
2024-05-08 21:19:09.818895:  
2024-05-08 21:19:09.819027: Epoch 766 
2024-05-08 21:19:09.819122: Current learning rate: 0.00271 
2024-05-08 21:19:51.370548: train_loss -0.8659 
2024-05-08 21:19:51.370726: val_loss -0.8744 
2024-05-08 21:19:51.370775: Pseudo dice [0.9302] 
2024-05-08 21:19:51.370829: Epoch time: 41.55 s 
2024-05-08 21:19:52.531436:  
2024-05-08 21:19:52.531616: Epoch 767 
2024-05-08 21:19:52.531715: Current learning rate: 0.0027 
2024-05-08 21:20:33.610162: train_loss -0.869 
2024-05-08 21:20:33.610344: val_loss -0.8781 
2024-05-08 21:20:33.610394: Pseudo dice [0.9328] 
2024-05-08 21:20:33.610448: Epoch time: 41.08 s 
2024-05-08 21:20:34.748189:  
2024-05-08 21:20:34.748377: Epoch 768 
2024-05-08 21:20:34.748481: Current learning rate: 0.00268 
2024-05-08 21:21:15.830830: train_loss -0.867 
2024-05-08 21:21:15.831015: val_loss -0.8777 
2024-05-08 21:21:15.831066: Pseudo dice [0.9347] 
2024-05-08 21:21:15.831119: Epoch time: 41.08 s 
2024-05-08 21:21:15.831162: Yayy! New best EMA pseudo Dice: 0.9321 
2024-05-08 21:21:17.357141:  
2024-05-08 21:21:17.357347: Epoch 769 
2024-05-08 21:21:17.357465: Current learning rate: 0.00267 
2024-05-08 21:21:58.427485: train_loss -0.8678 
2024-05-08 21:21:58.427666: val_loss -0.8701 
2024-05-08 21:21:58.427716: Pseudo dice [0.9292] 
2024-05-08 21:21:58.427769: Epoch time: 41.07 s 
2024-05-08 21:21:59.569795:  
2024-05-08 21:21:59.569921: Epoch 770 
2024-05-08 21:21:59.570014: Current learning rate: 0.00266 
2024-05-08 21:22:40.599371: train_loss -0.8694 
2024-05-08 21:22:40.599556: val_loss -0.8889 
2024-05-08 21:22:40.599606: Pseudo dice [0.9379] 
2024-05-08 21:22:40.599659: Epoch time: 41.03 s 
2024-05-08 21:22:40.599704: Yayy! New best EMA pseudo Dice: 0.9324 
2024-05-08 21:22:42.275496:  
2024-05-08 21:22:42.275704: Epoch 771 
2024-05-08 21:22:42.275805: Current learning rate: 0.00265 
2024-05-08 21:23:23.286823: train_loss -0.87 
2024-05-08 21:23:23.287008: val_loss -0.8834 
2024-05-08 21:23:23.287057: Pseudo dice [0.9379] 
2024-05-08 21:23:23.287110: Epoch time: 41.01 s 
2024-05-08 21:23:23.287152: Yayy! New best EMA pseudo Dice: 0.9329 
2024-05-08 21:23:24.783877:  
2024-05-08 21:23:24.784017: Epoch 772 
2024-05-08 21:23:24.784112: Current learning rate: 0.00264 
2024-05-08 21:24:05.799212: train_loss -0.8713 
2024-05-08 21:24:05.799412: val_loss -0.8664 
2024-05-08 21:24:05.799463: Pseudo dice [0.9291] 
2024-05-08 21:24:05.799518: Epoch time: 41.02 s 
2024-05-08 21:24:06.951373:  
2024-05-08 21:24:06.951512: Epoch 773 
2024-05-08 21:24:06.951605: Current learning rate: 0.00263 
2024-05-08 21:24:48.326274: train_loss -0.8662 
2024-05-08 21:24:48.326463: val_loss -0.8711 
2024-05-08 21:24:48.326518: Pseudo dice [0.9303] 
2024-05-08 21:24:48.326571: Epoch time: 41.38 s 
2024-05-08 21:24:49.477393:  
2024-05-08 21:24:49.477526: Epoch 774 
2024-05-08 21:24:49.477628: Current learning rate: 0.00262 
2024-05-08 21:25:30.481670: train_loss -0.871 
2024-05-08 21:25:30.481849: val_loss -0.8783 
2024-05-08 21:25:30.481899: Pseudo dice [0.9322] 
2024-05-08 21:25:30.481952: Epoch time: 41.01 s 
2024-05-08 21:25:31.627888:  
2024-05-08 21:25:31.628015: Epoch 775 
2024-05-08 21:25:31.628106: Current learning rate: 0.00261 
2024-05-08 21:26:12.632608: train_loss -0.8674 
2024-05-08 21:26:12.632797: val_loss -0.8727 
2024-05-08 21:26:12.632847: Pseudo dice [0.933] 
2024-05-08 21:26:12.632900: Epoch time: 41.01 s 
2024-05-08 21:26:13.778150:  
2024-05-08 21:26:13.778279: Epoch 776 
2024-05-08 21:26:13.778373: Current learning rate: 0.0026 
2024-05-08 21:26:54.792341: train_loss -0.8684 
2024-05-08 21:26:54.792522: val_loss -0.8706 
2024-05-08 21:26:54.792571: Pseudo dice [0.93] 
2024-05-08 21:26:54.792624: Epoch time: 41.02 s 
2024-05-08 21:26:56.156524:  
2024-05-08 21:26:56.156688: Epoch 777 
2024-05-08 21:26:56.156777: Current learning rate: 0.00259 
2024-05-08 21:27:37.179438: train_loss -0.8623 
2024-05-08 21:27:37.179624: val_loss -0.8876 
2024-05-08 21:27:37.179675: Pseudo dice [0.936] 
2024-05-08 21:27:37.179729: Epoch time: 41.02 s 
2024-05-08 21:27:38.326844:  
2024-05-08 21:27:38.326999: Epoch 778 
2024-05-08 21:27:38.327101: Current learning rate: 0.00258 
2024-05-08 21:28:19.403532: train_loss -0.8722 
2024-05-08 21:28:19.403721: val_loss -0.8664 
2024-05-08 21:28:19.403770: Pseudo dice [0.93] 
2024-05-08 21:28:19.403823: Epoch time: 41.08 s 
2024-05-08 21:28:20.544665:  
2024-05-08 21:28:20.544912: Epoch 779 
2024-05-08 21:28:20.545009: Current learning rate: 0.00257 
2024-05-08 21:29:02.015063: train_loss -0.8699 
2024-05-08 21:29:02.015250: val_loss -0.8722 
2024-05-08 21:29:02.015300: Pseudo dice [0.9289] 
2024-05-08 21:29:02.015352: Epoch time: 41.47 s 
2024-05-08 21:29:03.150965:  
2024-05-08 21:29:03.151237: Epoch 780 
2024-05-08 21:29:03.151413: Current learning rate: 0.00256 
2024-05-08 21:29:44.660578: train_loss -0.8668 
2024-05-08 21:29:44.660768: val_loss -0.8683 
2024-05-08 21:29:44.660817: Pseudo dice [0.9283] 
2024-05-08 21:29:44.660873: Epoch time: 41.51 s 
2024-05-08 21:29:45.800243:  
2024-05-08 21:29:45.800379: Epoch 781 
2024-05-08 21:29:45.800474: Current learning rate: 0.00255 
2024-05-08 21:30:26.879840: train_loss -0.8671 
2024-05-08 21:30:26.880006: val_loss -0.8843 
2024-05-08 21:30:26.880055: Pseudo dice [0.9365] 
2024-05-08 21:30:26.880110: Epoch time: 41.08 s 
2024-05-08 21:30:28.014268:  
2024-05-08 21:30:28.014392: Epoch 782 
2024-05-08 21:30:28.014487: Current learning rate: 0.00254 
2024-05-08 21:31:09.069947: train_loss -0.8725 
2024-05-08 21:31:09.070127: val_loss -0.8692 
2024-05-08 21:31:09.070177: Pseudo dice [0.9308] 
2024-05-08 21:31:09.070229: Epoch time: 41.06 s 
2024-05-08 21:31:10.396936:  
2024-05-08 21:31:10.397075: Epoch 783 
2024-05-08 21:31:10.397166: Current learning rate: 0.00253 
2024-05-08 21:31:51.444383: train_loss -0.8756 
2024-05-08 21:31:51.444548: val_loss -0.8764 
2024-05-08 21:31:51.444597: Pseudo dice [0.9315] 
2024-05-08 21:31:51.444656: Epoch time: 41.05 s 
2024-05-08 21:31:52.581312:  
2024-05-08 21:31:52.581441: Epoch 784 
2024-05-08 21:31:52.581531: Current learning rate: 0.00252 
2024-05-08 21:32:33.684337: train_loss -0.8705 
2024-05-08 21:32:33.684523: val_loss -0.8845 
2024-05-08 21:32:33.684572: Pseudo dice [0.936] 
2024-05-08 21:32:33.684625: Epoch time: 41.1 s 
2024-05-08 21:32:34.815957:  
2024-05-08 21:32:34.816230: Epoch 785 
2024-05-08 21:32:34.816361: Current learning rate: 0.00251 
2024-05-08 21:33:15.866410: train_loss -0.8733 
2024-05-08 21:33:15.866588: val_loss -0.8765 
2024-05-08 21:33:15.866637: Pseudo dice [0.9335] 
2024-05-08 21:33:15.866692: Epoch time: 41.05 s 
2024-05-08 21:33:17.003036:  
2024-05-08 21:33:17.003237: Epoch 786 
2024-05-08 21:33:17.003334: Current learning rate: 0.0025 
2024-05-08 21:33:58.086507: train_loss -0.8678 
2024-05-08 21:33:58.086688: val_loss -0.8738 
2024-05-08 21:33:58.086738: Pseudo dice [0.9319] 
2024-05-08 21:33:58.086792: Epoch time: 41.08 s 
2024-05-08 21:33:59.221965:  
2024-05-08 21:33:59.222260: Epoch 787 
2024-05-08 21:33:59.222357: Current learning rate: 0.00249 
2024-05-08 21:34:40.286891: train_loss -0.8742 
2024-05-08 21:34:40.287071: val_loss -0.8771 
2024-05-08 21:34:40.287119: Pseudo dice [0.9336] 
2024-05-08 21:34:40.287172: Epoch time: 41.07 s 
2024-05-08 21:34:41.623819:  
2024-05-08 21:34:41.624018: Epoch 788 
2024-05-08 21:34:41.624141: Current learning rate: 0.00248 
2024-05-08 21:35:22.728898: train_loss -0.8652 
2024-05-08 21:35:22.729080: val_loss -0.8843 
2024-05-08 21:35:22.729130: Pseudo dice [0.9359] 
2024-05-08 21:35:22.729185: Epoch time: 41.11 s 
2024-05-08 21:35:23.869011:  
2024-05-08 21:35:23.869206: Epoch 789 
2024-05-08 21:35:23.869308: Current learning rate: 0.00247 
2024-05-08 21:36:05.322690: train_loss -0.8683 
2024-05-08 21:36:05.322874: val_loss -0.8704 
2024-05-08 21:36:05.322923: Pseudo dice [0.93] 
2024-05-08 21:36:05.322977: Epoch time: 41.45 s 
2024-05-08 21:36:06.481157:  
2024-05-08 21:36:06.481297: Epoch 790 
2024-05-08 21:36:06.481389: Current learning rate: 0.00245 
2024-05-08 21:36:47.614549: train_loss -0.8707 
2024-05-08 21:36:47.614731: val_loss -0.8646 
2024-05-08 21:36:47.614781: Pseudo dice [0.9323] 
2024-05-08 21:36:47.614838: Epoch time: 41.13 s 
2024-05-08 21:36:48.773549:  
2024-05-08 21:36:48.773678: Epoch 791 
2024-05-08 21:36:48.773774: Current learning rate: 0.00244 
2024-05-08 21:37:29.997584: train_loss -0.8701 
2024-05-08 21:37:29.997766: val_loss -0.8801 
2024-05-08 21:37:29.997815: Pseudo dice [0.9373] 
2024-05-08 21:37:29.997873: Epoch time: 41.23 s 
2024-05-08 21:37:29.997916: Yayy! New best EMA pseudo Dice: 0.933 
2024-05-08 21:37:31.492280:  
2024-05-08 21:37:31.492472: Epoch 792 
2024-05-08 21:37:31.492571: Current learning rate: 0.00243 
2024-05-08 21:38:12.517043: train_loss -0.8727 
2024-05-08 21:38:12.517223: val_loss -0.882 
2024-05-08 21:38:12.517272: Pseudo dice [0.9327] 
2024-05-08 21:38:12.517325: Epoch time: 41.03 s 
2024-05-08 21:38:13.660996:  
2024-05-08 21:38:13.661137: Epoch 793 
2024-05-08 21:38:13.661230: Current learning rate: 0.00242 
2024-05-08 21:38:54.667066: train_loss -0.874 
2024-05-08 21:38:54.667250: val_loss -0.8795 
2024-05-08 21:38:54.667309: Pseudo dice [0.9335] 
2024-05-08 21:38:54.667361: Epoch time: 41.01 s 
2024-05-08 21:38:54.667405: Yayy! New best EMA pseudo Dice: 0.933 
2024-05-08 21:38:56.387522:  
2024-05-08 21:38:56.387675: Epoch 794 
2024-05-08 21:38:56.387765: Current learning rate: 0.00241 
2024-05-08 21:39:37.435679: train_loss -0.8765 
2024-05-08 21:39:37.435868: val_loss -0.8876 
2024-05-08 21:39:37.435920: Pseudo dice [0.9414] 
2024-05-08 21:39:37.435978: Epoch time: 41.05 s 
2024-05-08 21:39:37.436020: Yayy! New best EMA pseudo Dice: 0.9339 
2024-05-08 21:39:38.929267:  
2024-05-08 21:39:38.929429: Epoch 795 
2024-05-08 21:39:38.929521: Current learning rate: 0.0024 
2024-05-08 21:40:19.984832: train_loss -0.876 
2024-05-08 21:40:19.985011: val_loss -0.8777 
2024-05-08 21:40:19.985063: Pseudo dice [0.9344] 
2024-05-08 21:40:19.985116: Epoch time: 41.06 s 
2024-05-08 21:40:19.985164: Yayy! New best EMA pseudo Dice: 0.9339 
2024-05-08 21:40:21.489230:  
2024-05-08 21:40:21.489445: Epoch 796 
2024-05-08 21:40:21.489540: Current learning rate: 0.00239 
2024-05-08 21:41:03.039777: train_loss -0.8708 
2024-05-08 21:41:03.040203: val_loss -0.8648 
2024-05-08 21:41:03.040259: Pseudo dice [0.9263] 
2024-05-08 21:41:03.040316: Epoch time: 41.55 s 
2024-05-08 21:41:04.303228:  
2024-05-08 21:41:04.303446: Epoch 797 
2024-05-08 21:41:04.303541: Current learning rate: 0.00238 
2024-05-08 21:41:45.353754: train_loss -0.8673 
2024-05-08 21:41:45.353920: val_loss -0.8706 
2024-05-08 21:41:45.353970: Pseudo dice [0.931] 
2024-05-08 21:41:45.354026: Epoch time: 41.05 s 
2024-05-08 21:41:46.503355:  
2024-05-08 21:41:46.503494: Epoch 798 
2024-05-08 21:41:46.503587: Current learning rate: 0.00237 
2024-05-08 21:42:27.539085: train_loss -0.8767 
2024-05-08 21:42:27.539256: val_loss -0.8752 
2024-05-08 21:42:27.539306: Pseudo dice [0.9315] 
2024-05-08 21:42:27.539360: Epoch time: 41.04 s 
2024-05-08 21:42:28.686750:  
2024-05-08 21:42:28.686882: Epoch 799 
2024-05-08 21:42:28.686977: Current learning rate: 0.00236 
2024-05-08 21:43:09.710199: train_loss -0.8718 
2024-05-08 21:43:09.710379: val_loss -0.8793 
2024-05-08 21:43:09.710429: Pseudo dice [0.9355] 
2024-05-08 21:43:09.710482: Epoch time: 41.02 s 
2024-05-08 21:43:11.405560:  
2024-05-08 21:43:11.405907: Epoch 800 
2024-05-08 21:43:11.406005: Current learning rate: 0.00235 
2024-05-08 21:43:52.902758: train_loss -0.8669 
2024-05-08 21:43:52.902937: val_loss -0.8771 
2024-05-08 21:43:52.902986: Pseudo dice [0.9304] 
2024-05-08 21:43:52.903038: Epoch time: 41.5 s 
2024-05-08 21:43:54.047180:  
2024-05-08 21:43:54.047397: Epoch 801 
2024-05-08 21:43:54.047497: Current learning rate: 0.00234 
2024-05-08 21:44:35.133608: train_loss -0.869 
2024-05-08 21:44:35.133790: val_loss -0.8671 
2024-05-08 21:44:35.133839: Pseudo dice [0.928] 
2024-05-08 21:44:35.133892: Epoch time: 41.09 s 
2024-05-08 21:44:36.278137:  
2024-05-08 21:44:36.278285: Epoch 802 
2024-05-08 21:44:36.278530: Current learning rate: 0.00233 
2024-05-08 21:45:17.370826: train_loss -0.87 
2024-05-08 21:45:17.371008: val_loss -0.8728 
2024-05-08 21:45:17.371058: Pseudo dice [0.9331] 
2024-05-08 21:45:17.371111: Epoch time: 41.09 s 
2024-05-08 21:45:18.521220:  
2024-05-08 21:45:18.521471: Epoch 803 
2024-05-08 21:45:18.521609: Current learning rate: 0.00232 
2024-05-08 21:45:59.610482: train_loss -0.8721 
2024-05-08 21:45:59.610663: val_loss -0.881 
2024-05-08 21:45:59.610713: Pseudo dice [0.9361] 
2024-05-08 21:45:59.610767: Epoch time: 41.09 s 
2024-05-08 21:46:00.767557:  
2024-05-08 21:46:00.767707: Epoch 804 
2024-05-08 21:46:00.767808: Current learning rate: 0.00231 
2024-05-08 21:46:41.861008: train_loss -0.8709 
2024-05-08 21:46:41.861194: val_loss -0.8686 
2024-05-08 21:46:41.861242: Pseudo dice [0.9305] 
2024-05-08 21:46:41.861294: Epoch time: 41.09 s 
2024-05-08 21:46:43.187738:  
2024-05-08 21:46:43.187891: Epoch 805 
2024-05-08 21:46:43.187987: Current learning rate: 0.0023 
2024-05-08 21:47:24.263941: train_loss -0.8695 
2024-05-08 21:47:24.264127: val_loss -0.87 
2024-05-08 21:47:24.264178: Pseudo dice [0.9333] 
2024-05-08 21:47:24.264232: Epoch time: 41.08 s 
2024-05-08 21:47:25.416588:  
2024-05-08 21:47:25.416736: Epoch 806 
2024-05-08 21:47:25.416835: Current learning rate: 0.00229 
2024-05-08 21:48:06.498519: train_loss -0.8622 
2024-05-08 21:48:06.498703: val_loss -0.8744 
2024-05-08 21:48:06.498757: Pseudo dice [0.9311] 
2024-05-08 21:48:06.498809: Epoch time: 41.08 s 
2024-05-08 21:48:07.646195:  
2024-05-08 21:48:07.646430: Epoch 807 
2024-05-08 21:48:07.646525: Current learning rate: 0.00228 
2024-05-08 21:48:48.699789: train_loss -0.8733 
2024-05-08 21:48:48.699966: val_loss -0.8779 
2024-05-08 21:48:48.700016: Pseudo dice [0.9307] 
2024-05-08 21:48:48.700070: Epoch time: 41.05 s 
2024-05-08 21:48:49.850500:  
2024-05-08 21:48:49.850631: Epoch 808 
2024-05-08 21:48:49.850727: Current learning rate: 0.00226 
2024-05-08 21:49:30.916480: train_loss -0.8724 
2024-05-08 21:49:30.916661: val_loss -0.8729 
2024-05-08 21:49:30.916717: Pseudo dice [0.9302] 
2024-05-08 21:49:30.916771: Epoch time: 41.07 s 
2024-05-08 21:49:32.066770:  
2024-05-08 21:49:32.066907: Epoch 809 
2024-05-08 21:49:32.067004: Current learning rate: 0.00225 
2024-05-08 21:50:13.104896: train_loss -0.8725 
2024-05-08 21:50:13.105079: val_loss -0.8666 
2024-05-08 21:50:13.105127: Pseudo dice [0.9281] 
2024-05-08 21:50:13.105180: Epoch time: 41.04 s 
2024-05-08 21:50:14.254958:  
2024-05-08 21:50:14.255176: Epoch 810 
2024-05-08 21:50:14.255270: Current learning rate: 0.00224 
2024-05-08 21:50:55.328424: train_loss -0.8722 
2024-05-08 21:50:55.328607: val_loss -0.8824 
2024-05-08 21:50:55.328662: Pseudo dice [0.9375] 
2024-05-08 21:50:55.328718: Epoch time: 41.07 s 
2024-05-08 21:50:56.473246:  
2024-05-08 21:50:56.473383: Epoch 811 
2024-05-08 21:50:56.473476: Current learning rate: 0.00223 
2024-05-08 21:51:37.520540: train_loss -0.8759 
2024-05-08 21:51:37.520726: val_loss -0.888 
2024-05-08 21:51:37.520776: Pseudo dice [0.94] 
2024-05-08 21:51:37.520828: Epoch time: 41.05 s 
2024-05-08 21:51:38.874104:  
2024-05-08 21:51:38.874245: Epoch 812 
2024-05-08 21:51:38.874337: Current learning rate: 0.00222 
2024-05-08 21:52:19.921094: train_loss -0.8693 
2024-05-08 21:52:19.921277: val_loss -0.8627 
2024-05-08 21:52:19.921327: Pseudo dice [0.9283] 
2024-05-08 21:52:19.921396: Epoch time: 41.05 s 
2024-05-08 21:52:21.065876:  
2024-05-08 21:52:21.066010: Epoch 813 
2024-05-08 21:52:21.066100: Current learning rate: 0.00221 
2024-05-08 21:53:02.125494: train_loss -0.8754 
2024-05-08 21:53:02.125674: val_loss -0.8783 
2024-05-08 21:53:02.125724: Pseudo dice [0.9332] 
2024-05-08 21:53:02.125776: Epoch time: 41.06 s 
2024-05-08 21:53:03.268855:  
2024-05-08 21:53:03.269063: Epoch 814 
2024-05-08 21:53:03.269157: Current learning rate: 0.0022 
2024-05-08 21:53:44.343313: train_loss -0.8687 
2024-05-08 21:53:44.343496: val_loss -0.8685 
2024-05-08 21:53:44.343550: Pseudo dice [0.9317] 
2024-05-08 21:53:44.343606: Epoch time: 41.08 s 
2024-05-08 21:53:45.495726:  
2024-05-08 21:53:45.495868: Epoch 815 
2024-05-08 21:53:45.495959: Current learning rate: 0.00219 
2024-05-08 21:54:26.557785: train_loss -0.8749 
2024-05-08 21:54:26.557972: val_loss -0.882 
2024-05-08 21:54:26.558023: Pseudo dice [0.936] 
2024-05-08 21:54:26.558083: Epoch time: 41.06 s 
2024-05-08 21:54:27.708287:  
2024-05-08 21:54:27.708421: Epoch 816 
2024-05-08 21:54:27.708518: Current learning rate: 0.00218 
2024-05-08 21:55:08.785558: train_loss -0.876 
2024-05-08 21:55:08.785824: val_loss -0.8695 
2024-05-08 21:55:08.785878: Pseudo dice [0.9286] 
2024-05-08 21:55:08.785931: Epoch time: 41.08 s 
2024-05-08 21:55:10.099655:  
2024-05-08 21:55:10.099838: Epoch 817 
2024-05-08 21:55:10.099929: Current learning rate: 0.00217 
2024-05-08 21:55:51.173831: train_loss -0.8711 
2024-05-08 21:55:51.174006: val_loss -0.8842 
2024-05-08 21:55:51.174057: Pseudo dice [0.9368] 
2024-05-08 21:55:51.174110: Epoch time: 41.08 s 
2024-05-08 21:55:52.324082:  
2024-05-08 21:55:52.324223: Epoch 818 
2024-05-08 21:55:52.324316: Current learning rate: 0.00216 
2024-05-08 21:56:33.420493: train_loss -0.8718 
2024-05-08 21:56:33.420677: val_loss -0.8791 
2024-05-08 21:56:33.420733: Pseudo dice [0.9351] 
2024-05-08 21:56:33.420786: Epoch time: 41.1 s 
2024-05-08 21:56:34.570693:  
2024-05-08 21:56:34.570904: Epoch 819 
2024-05-08 21:56:34.570997: Current learning rate: 0.00215 
2024-05-08 21:57:15.637260: train_loss -0.8721 
2024-05-08 21:57:15.637436: val_loss -0.8846 
2024-05-08 21:57:15.637484: Pseudo dice [0.9366] 
2024-05-08 21:57:15.637537: Epoch time: 41.07 s 
2024-05-08 21:57:16.716238:  
2024-05-08 21:57:16.716368: Epoch 820 
2024-05-08 21:57:16.716462: Current learning rate: 0.00214 
2024-05-08 21:57:57.790787: train_loss -0.866 
2024-05-08 21:57:57.790970: val_loss -0.8819 
2024-05-08 21:57:57.791018: Pseudo dice [0.9358] 
2024-05-08 21:57:57.791072: Epoch time: 41.08 s 
2024-05-08 21:57:58.869935:  
2024-05-08 21:57:58.870150: Epoch 821 
2024-05-08 21:57:58.870277: Current learning rate: 0.00213 
2024-05-08 21:58:39.938869: train_loss -0.8747 
2024-05-08 21:58:39.939057: val_loss -0.8669 
2024-05-08 21:58:39.939106: Pseudo dice [0.9322] 
2024-05-08 21:58:39.939162: Epoch time: 41.07 s 
2024-05-08 21:58:41.018443:  
2024-05-08 21:58:41.018576: Epoch 822 
2024-05-08 21:58:41.018676: Current learning rate: 0.00212 
2024-05-08 21:59:22.076785: train_loss -0.8746 
2024-05-08 21:59:22.076967: val_loss -0.876 
2024-05-08 21:59:22.077019: Pseudo dice [0.9317] 
2024-05-08 21:59:22.077077: Epoch time: 41.06 s 
2024-05-08 21:59:23.164737:  
2024-05-08 21:59:23.164875: Epoch 823 
2024-05-08 21:59:23.164969: Current learning rate: 0.0021 
2024-05-08 22:00:04.200712: train_loss -0.8674 
2024-05-08 22:00:04.200892: val_loss -0.8674 
2024-05-08 22:00:04.200942: Pseudo dice [0.9292] 
2024-05-08 22:00:04.200996: Epoch time: 41.04 s 
2024-05-08 22:00:05.487111:  
2024-05-08 22:00:05.487263: Epoch 824 
2024-05-08 22:00:05.487357: Current learning rate: 0.00209 
2024-05-08 22:00:46.510988: train_loss -0.8677 
2024-05-08 22:00:46.511171: val_loss -0.8828 
2024-05-08 22:00:46.511221: Pseudo dice [0.9378] 
2024-05-08 22:00:46.511274: Epoch time: 41.02 s 
2024-05-08 22:00:47.591502:  
2024-05-08 22:00:47.591702: Epoch 825 
2024-05-08 22:00:47.591798: Current learning rate: 0.00208 
2024-05-08 22:01:28.606756: train_loss -0.8711 
2024-05-08 22:01:28.606954: val_loss -0.8738 
2024-05-08 22:01:28.607004: Pseudo dice [0.9315] 
2024-05-08 22:01:28.607057: Epoch time: 41.02 s 
2024-05-08 22:01:29.727146:  
2024-05-08 22:01:29.727349: Epoch 826 
2024-05-08 22:01:29.727442: Current learning rate: 0.00207 
2024-05-08 22:02:10.760527: train_loss -0.8741 
2024-05-08 22:02:10.760715: val_loss -0.8799 
2024-05-08 22:02:10.760765: Pseudo dice [0.9337] 
2024-05-08 22:02:10.760818: Epoch time: 41.03 s 
2024-05-08 22:02:11.835482:  
2024-05-08 22:02:11.835622: Epoch 827 
2024-05-08 22:02:11.835711: Current learning rate: 0.00206 
2024-05-08 22:02:52.865331: train_loss -0.8711 
2024-05-08 22:02:52.865505: val_loss -0.8851 
2024-05-08 22:02:52.865554: Pseudo dice [0.9367] 
2024-05-08 22:02:52.865608: Epoch time: 41.03 s 
2024-05-08 22:02:53.945215:  
2024-05-08 22:02:53.945358: Epoch 828 
2024-05-08 22:02:53.945462: Current learning rate: 0.00205 
2024-05-08 22:03:34.945346: train_loss -0.8642 
2024-05-08 22:03:34.945524: val_loss -0.8705 
2024-05-08 22:03:34.945573: Pseudo dice [0.9319] 
2024-05-08 22:03:34.945627: Epoch time: 41.0 s 
2024-05-08 22:03:36.021668:  
2024-05-08 22:03:36.021805: Epoch 829 
2024-05-08 22:03:36.021896: Current learning rate: 0.00204 
2024-05-08 22:04:17.026927: train_loss -0.8669 
2024-05-08 22:04:17.027106: val_loss -0.8737 
2024-05-08 22:04:17.027155: Pseudo dice [0.9304] 
2024-05-08 22:04:17.027209: Epoch time: 41.01 s 
2024-05-08 22:04:18.305286:  
2024-05-08 22:04:18.305457: Epoch 830 
2024-05-08 22:04:18.305560: Current learning rate: 0.00203 
2024-05-08 22:04:59.336558: train_loss -0.8728 
2024-05-08 22:04:59.336750: val_loss -0.8756 
2024-05-08 22:04:59.336799: Pseudo dice [0.9347] 
2024-05-08 22:04:59.336853: Epoch time: 41.03 s 
2024-05-08 22:05:00.417097:  
2024-05-08 22:05:00.417428: Epoch 831 
2024-05-08 22:05:00.417521: Current learning rate: 0.00202 
2024-05-08 22:05:41.472225: train_loss -0.8757 
2024-05-08 22:05:41.472404: val_loss -0.8771 
2024-05-08 22:05:41.472453: Pseudo dice [0.9362] 
2024-05-08 22:05:41.472507: Epoch time: 41.06 s 
2024-05-08 22:05:42.546404:  
2024-05-08 22:05:42.546530: Epoch 832 
2024-05-08 22:05:42.546618: Current learning rate: 0.00201 
2024-05-08 22:06:23.599307: train_loss -0.8753 
2024-05-08 22:06:23.599482: val_loss -0.887 
2024-05-08 22:06:23.599531: Pseudo dice [0.9392] 
2024-05-08 22:06:23.599582: Epoch time: 41.05 s 
2024-05-08 22:06:23.599624: Yayy! New best EMA pseudo Dice: 0.9342 
2024-05-08 22:06:25.030278:  
2024-05-08 22:06:25.030471: Epoch 833 
2024-05-08 22:06:25.030568: Current learning rate: 0.002 
2024-05-08 22:07:06.036865: train_loss -0.8736 
2024-05-08 22:07:06.037035: val_loss -0.8704 
2024-05-08 22:07:06.037083: Pseudo dice [0.9313] 
2024-05-08 22:07:06.037135: Epoch time: 41.01 s 
2024-05-08 22:07:07.127553:  
2024-05-08 22:07:07.127687: Epoch 834 
2024-05-08 22:07:07.127784: Current learning rate: 0.00199 
2024-05-08 22:07:48.157312: train_loss -0.8724 
2024-05-08 22:07:48.157498: val_loss -0.8801 
2024-05-08 22:07:48.157548: Pseudo dice [0.9368] 
2024-05-08 22:07:48.157601: Epoch time: 41.03 s 
2024-05-08 22:07:48.157644: Yayy! New best EMA pseudo Dice: 0.9342 
2024-05-08 22:07:49.588953:  
2024-05-08 22:07:49.589087: Epoch 835 
2024-05-08 22:07:49.589273: Current learning rate: 0.00198 
2024-05-08 22:08:30.587557: train_loss -0.8736 
2024-05-08 22:08:30.587743: val_loss -0.8904 
2024-05-08 22:08:30.587792: Pseudo dice [0.9401] 
2024-05-08 22:08:30.587845: Epoch time: 41.0 s 
2024-05-08 22:08:30.587888: Yayy! New best EMA pseudo Dice: 0.9348 
2024-05-08 22:08:32.192880:  
2024-05-08 22:08:32.193023: Epoch 836 
2024-05-08 22:08:32.193125: Current learning rate: 0.00196 
2024-05-08 22:09:13.242614: train_loss -0.8725 
2024-05-08 22:09:13.242805: val_loss -0.8833 
2024-05-08 22:09:13.242857: Pseudo dice [0.9359] 
2024-05-08 22:09:13.242910: Epoch time: 41.05 s 
2024-05-08 22:09:13.242954: Yayy! New best EMA pseudo Dice: 0.9349 
2024-05-08 22:09:14.681057:  
2024-05-08 22:09:14.681199: Epoch 837 
2024-05-08 22:09:14.681293: Current learning rate: 0.00195 
2024-05-08 22:09:55.745473: train_loss -0.8754 
2024-05-08 22:09:55.745667: val_loss -0.8787 
2024-05-08 22:09:55.745717: Pseudo dice [0.9314] 
2024-05-08 22:09:55.745769: Epoch time: 41.07 s 
2024-05-08 22:09:56.849644:  
2024-05-08 22:09:56.849946: Epoch 838 
2024-05-08 22:09:56.850038: Current learning rate: 0.00194 
2024-05-08 22:10:37.923332: train_loss -0.8787 
2024-05-08 22:10:37.923512: val_loss -0.8932 
2024-05-08 22:10:37.923561: Pseudo dice [0.9412] 
2024-05-08 22:10:37.923613: Epoch time: 41.07 s 
2024-05-08 22:10:37.923655: Yayy! New best EMA pseudo Dice: 0.9352 
2024-05-08 22:10:39.357928:  
2024-05-08 22:10:39.358114: Epoch 839 
2024-05-08 22:10:39.358211: Current learning rate: 0.00193 
2024-05-08 22:11:20.426877: train_loss -0.8784 
2024-05-08 22:11:20.427150: val_loss -0.8793 
2024-05-08 22:11:20.427202: Pseudo dice [0.9363] 
2024-05-08 22:11:20.427255: Epoch time: 41.07 s 
2024-05-08 22:11:20.427299: Yayy! New best EMA pseudo Dice: 0.9353 
2024-05-08 22:11:21.866657:  
2024-05-08 22:11:21.866851: Epoch 840 
2024-05-08 22:11:21.866952: Current learning rate: 0.00192 
2024-05-08 22:12:02.916839: train_loss -0.8729 
2024-05-08 22:12:02.917016: val_loss -0.8858 
2024-05-08 22:12:02.917065: Pseudo dice [0.9392] 
2024-05-08 22:12:02.917118: Epoch time: 41.05 s 
2024-05-08 22:12:02.917161: Yayy! New best EMA pseudo Dice: 0.9357 
2024-05-08 22:12:04.353325:  
2024-05-08 22:12:04.353461: Epoch 841 
2024-05-08 22:12:04.353553: Current learning rate: 0.00191 
2024-05-08 22:12:45.366633: train_loss -0.8761 
2024-05-08 22:12:45.366812: val_loss -0.8824 
2024-05-08 22:12:45.366862: Pseudo dice [0.9376] 
2024-05-08 22:12:45.366914: Epoch time: 41.01 s 
2024-05-08 22:12:45.366957: Yayy! New best EMA pseudo Dice: 0.9359 
2024-05-08 22:12:46.988018:  
2024-05-08 22:12:46.988225: Epoch 842 
2024-05-08 22:12:46.988323: Current learning rate: 0.0019 
2024-05-08 22:13:27.999109: train_loss -0.8732 
2024-05-08 22:13:27.999296: val_loss -0.8692 
2024-05-08 22:13:27.999345: Pseudo dice [0.9314] 
2024-05-08 22:13:27.999457: Epoch time: 41.01 s 
2024-05-08 22:13:29.076078:  
2024-05-08 22:13:29.076212: Epoch 843 
2024-05-08 22:13:29.076306: Current learning rate: 0.00189 
2024-05-08 22:14:10.102085: train_loss -0.8748 
2024-05-08 22:14:10.102271: val_loss -0.8854 
2024-05-08 22:14:10.102320: Pseudo dice [0.9387] 
2024-05-08 22:14:10.102373: Epoch time: 41.03 s 
2024-05-08 22:14:11.176274:  
2024-05-08 22:14:11.176407: Epoch 844 
2024-05-08 22:14:11.176497: Current learning rate: 0.00188 
2024-05-08 22:14:52.215978: train_loss -0.8712 
2024-05-08 22:14:52.216153: val_loss -0.8842 
2024-05-08 22:14:52.216203: Pseudo dice [0.9365] 
2024-05-08 22:14:52.216256: Epoch time: 41.04 s 
2024-05-08 22:14:53.298325:  
2024-05-08 22:14:53.298470: Epoch 845 
2024-05-08 22:14:53.298573: Current learning rate: 0.00187 
2024-05-08 22:15:34.309100: train_loss -0.8737 
2024-05-08 22:15:34.309281: val_loss -0.8749 
2024-05-08 22:15:34.309331: Pseudo dice [0.9338] 
2024-05-08 22:15:34.309386: Epoch time: 41.01 s 
2024-05-08 22:15:35.392007:  
2024-05-08 22:15:35.392140: Epoch 846 
2024-05-08 22:15:35.392237: Current learning rate: 0.00186 
2024-05-08 22:16:16.356090: train_loss -0.8761 
2024-05-08 22:16:16.356273: val_loss -0.8818 
2024-05-08 22:16:16.356323: Pseudo dice [0.9337] 
2024-05-08 22:16:16.356376: Epoch time: 40.97 s 
2024-05-08 22:16:17.421729:  
2024-05-08 22:16:17.422003: Epoch 847 
2024-05-08 22:16:17.422112: Current learning rate: 0.00185 
2024-05-08 22:16:58.372945: train_loss -0.878 
2024-05-08 22:16:58.373127: val_loss -0.8817 
2024-05-08 22:16:58.373176: Pseudo dice [0.9356] 
2024-05-08 22:16:58.373228: Epoch time: 40.95 s 
2024-05-08 22:16:59.628309:  
2024-05-08 22:16:59.628453: Epoch 848 
2024-05-08 22:16:59.628545: Current learning rate: 0.00184 
2024-05-08 22:17:40.665268: train_loss -0.8722 
2024-05-08 22:17:40.665456: val_loss -0.8789 
2024-05-08 22:17:40.665504: Pseudo dice [0.9348] 
2024-05-08 22:17:40.665563: Epoch time: 41.04 s 
2024-05-08 22:17:41.736687:  
2024-05-08 22:17:41.736817: Epoch 849 
2024-05-08 22:17:41.736905: Current learning rate: 0.00182 
2024-05-08 22:18:22.773493: train_loss -0.8717 
2024-05-08 22:18:22.773668: val_loss -0.8759 
2024-05-08 22:18:22.773717: Pseudo dice [0.9338] 
2024-05-08 22:18:22.773769: Epoch time: 41.04 s 
2024-05-08 22:18:24.217274:  
2024-05-08 22:18:24.217420: Epoch 850 
2024-05-08 22:18:24.217515: Current learning rate: 0.00181 
2024-05-08 22:19:05.219667: train_loss -0.8732 
2024-05-08 22:19:05.219853: val_loss -0.8773 
2024-05-08 22:19:05.219903: Pseudo dice [0.9346] 
2024-05-08 22:19:05.219956: Epoch time: 41.0 s 
2024-05-08 22:19:06.286256:  
2024-05-08 22:19:06.286400: Epoch 851 
2024-05-08 22:19:06.286503: Current learning rate: 0.0018 
2024-05-08 22:19:47.339291: train_loss -0.8747 
2024-05-08 22:19:47.339468: val_loss -0.8791 
2024-05-08 22:19:47.339516: Pseudo dice [0.9342] 
2024-05-08 22:19:47.339569: Epoch time: 41.05 s 
2024-05-08 22:19:48.426242:  
2024-05-08 22:19:48.426377: Epoch 852 
2024-05-08 22:19:48.426475: Current learning rate: 0.00179 
2024-05-08 22:20:29.480195: train_loss -0.8694 
2024-05-08 22:20:29.480409: val_loss -0.8816 
2024-05-08 22:20:29.480459: Pseudo dice [0.9346] 
2024-05-08 22:20:29.480511: Epoch time: 41.05 s 
2024-05-08 22:20:30.543765:  
2024-05-08 22:20:30.543897: Epoch 853 
2024-05-08 22:20:30.543993: Current learning rate: 0.00178 
2024-05-08 22:21:11.616085: train_loss -0.8691 
2024-05-08 22:21:11.616274: val_loss -0.8725 
2024-05-08 22:21:11.616323: Pseudo dice [0.9289] 
2024-05-08 22:21:11.616377: Epoch time: 41.07 s 
2024-05-08 22:21:12.885495:  
2024-05-08 22:21:12.885649: Epoch 854 
2024-05-08 22:21:12.885745: Current learning rate: 0.00177 
2024-05-08 22:21:53.948967: train_loss -0.8694 
2024-05-08 22:21:53.949154: val_loss -0.8873 
2024-05-08 22:21:53.949204: Pseudo dice [0.9364] 
2024-05-08 22:21:53.949257: Epoch time: 41.06 s 
2024-05-08 22:21:55.010913:  
2024-05-08 22:21:55.011062: Epoch 855 
2024-05-08 22:21:55.011154: Current learning rate: 0.00176 
2024-05-08 22:22:36.024303: train_loss -0.8733 
2024-05-08 22:22:36.024484: val_loss -0.8812 
2024-05-08 22:22:36.024537: Pseudo dice [0.9356] 
2024-05-08 22:22:36.024590: Epoch time: 41.01 s 
2024-05-08 22:22:37.102554:  
2024-05-08 22:22:37.102696: Epoch 856 
2024-05-08 22:22:37.102790: Current learning rate: 0.00175 
2024-05-08 22:23:18.112476: train_loss -0.878 
2024-05-08 22:23:18.112670: val_loss -0.8795 
2024-05-08 22:23:18.112721: Pseudo dice [0.933] 
2024-05-08 22:23:18.112775: Epoch time: 41.01 s 
2024-05-08 22:23:19.183414:  
2024-05-08 22:23:19.183555: Epoch 857 
2024-05-08 22:23:19.183648: Current learning rate: 0.00174 
2024-05-08 22:24:00.205184: train_loss -0.8799 
2024-05-08 22:24:00.205367: val_loss -0.8864 
2024-05-08 22:24:00.205418: Pseudo dice [0.937] 
2024-05-08 22:24:00.205472: Epoch time: 41.02 s 
2024-05-08 22:24:01.272661:  
2024-05-08 22:24:01.272796: Epoch 858 
2024-05-08 22:24:01.272886: Current learning rate: 0.00173 
2024-05-08 22:24:42.272364: train_loss -0.8778 
2024-05-08 22:24:42.272544: val_loss -0.8799 
2024-05-08 22:24:42.272592: Pseudo dice [0.9363] 
2024-05-08 22:24:42.272644: Epoch time: 41.0 s 
2024-05-08 22:24:43.338486:  
2024-05-08 22:24:43.338629: Epoch 859 
2024-05-08 22:24:43.338721: Current learning rate: 0.00172 
2024-05-08 22:25:24.340827: train_loss -0.8756 
2024-05-08 22:25:24.341002: val_loss -0.8838 
2024-05-08 22:25:24.341051: Pseudo dice [0.9342] 
2024-05-08 22:25:24.341103: Epoch time: 41.0 s 
2024-05-08 22:25:25.413012:  
2024-05-08 22:25:25.413198: Epoch 860 
2024-05-08 22:25:25.413340: Current learning rate: 0.0017 
2024-05-08 22:26:06.415906: train_loss -0.8715 
2024-05-08 22:26:06.416071: val_loss -0.8933 
2024-05-08 22:26:06.416123: Pseudo dice [0.9412] 
2024-05-08 22:26:06.416251: Epoch time: 41.0 s 
2024-05-08 22:26:07.703310:  
2024-05-08 22:26:07.703543: Epoch 861 
2024-05-08 22:26:07.703824: Current learning rate: 0.00169 
2024-05-08 22:26:48.710320: train_loss -0.8765 
2024-05-08 22:26:48.710505: val_loss -0.8806 
2024-05-08 22:26:48.710555: Pseudo dice [0.9357] 
2024-05-08 22:26:48.710608: Epoch time: 41.01 s 
2024-05-08 22:26:49.779316:  
2024-05-08 22:26:49.779469: Epoch 862 
2024-05-08 22:26:49.779561: Current learning rate: 0.00168 
2024-05-08 22:27:30.775693: train_loss -0.8749 
2024-05-08 22:27:30.775876: val_loss -0.886 
2024-05-08 22:27:30.775925: Pseudo dice [0.9388] 
2024-05-08 22:27:30.775978: Epoch time: 41.0 s 
2024-05-08 22:27:31.843761:  
2024-05-08 22:27:31.843900: Epoch 863 
2024-05-08 22:27:31.843991: Current learning rate: 0.00167 
2024-05-08 22:28:12.886991: train_loss -0.8706 
2024-05-08 22:28:12.887164: val_loss -0.8797 
2024-05-08 22:28:12.887212: Pseudo dice [0.9336] 
2024-05-08 22:28:12.887264: Epoch time: 41.04 s 
2024-05-08 22:28:13.956200:  
2024-05-08 22:28:13.956341: Epoch 864 
2024-05-08 22:28:13.956436: Current learning rate: 0.00166 
2024-05-08 22:28:55.016104: train_loss -0.8729 
2024-05-08 22:28:55.016293: val_loss -0.8851 
2024-05-08 22:28:55.016342: Pseudo dice [0.9372] 
2024-05-08 22:28:55.016396: Epoch time: 41.06 s 
2024-05-08 22:28:56.081133:  
2024-05-08 22:28:56.081276: Epoch 865 
2024-05-08 22:28:56.081372: Current learning rate: 0.00165 
2024-05-08 22:29:37.144024: train_loss -0.8745 
2024-05-08 22:29:37.144205: val_loss -0.8763 
2024-05-08 22:29:37.144255: Pseudo dice [0.9338] 
2024-05-08 22:29:37.144308: Epoch time: 41.06 s 
2024-05-08 22:29:38.210335:  
2024-05-08 22:29:38.210470: Epoch 866 
2024-05-08 22:29:38.210561: Current learning rate: 0.00164 
2024-05-08 22:30:19.272860: train_loss -0.8733 
2024-05-08 22:30:19.273043: val_loss -0.8874 
2024-05-08 22:30:19.273094: Pseudo dice [0.9386] 
2024-05-08 22:30:19.273147: Epoch time: 41.06 s 
2024-05-08 22:30:19.273190: Yayy! New best EMA pseudo Dice: 0.9359 
2024-05-08 22:30:20.903247:  
2024-05-08 22:30:20.903532: Epoch 867 
2024-05-08 22:30:20.903631: Current learning rate: 0.00163 
2024-05-08 22:31:02.011283: train_loss -0.8763 
2024-05-08 22:31:02.011466: val_loss -0.8761 
2024-05-08 22:31:02.011519: Pseudo dice [0.9343] 
2024-05-08 22:31:02.011572: Epoch time: 41.11 s 
2024-05-08 22:31:03.086563:  
2024-05-08 22:31:03.086701: Epoch 868 
2024-05-08 22:31:03.086797: Current learning rate: 0.00162 
2024-05-08 22:31:44.176543: train_loss -0.874 
2024-05-08 22:31:44.176734: val_loss -0.8726 
2024-05-08 22:31:44.176789: Pseudo dice [0.9313] 
2024-05-08 22:31:44.176842: Epoch time: 41.09 s 
2024-05-08 22:31:45.244561:  
2024-05-08 22:31:45.244707: Epoch 869 
2024-05-08 22:31:45.244808: Current learning rate: 0.00161 
2024-05-08 22:32:26.320570: train_loss -0.8717 
2024-05-08 22:32:26.320762: val_loss -0.8691 
2024-05-08 22:32:26.320815: Pseudo dice [0.9304] 
2024-05-08 22:32:26.320870: Epoch time: 41.08 s 
2024-05-08 22:32:27.387219:  
2024-05-08 22:32:27.387361: Epoch 870 
2024-05-08 22:32:27.387453: Current learning rate: 0.00159 
2024-05-08 22:33:08.443018: train_loss -0.8759 
2024-05-08 22:33:08.443202: val_loss -0.891 
2024-05-08 22:33:08.443254: Pseudo dice [0.9414] 
2024-05-08 22:33:08.443308: Epoch time: 41.06 s 
2024-05-08 22:33:09.511561:  
2024-05-08 22:33:09.511699: Epoch 871 
2024-05-08 22:33:09.511793: Current learning rate: 0.00158 
2024-05-08 22:33:50.536769: train_loss -0.8752 
2024-05-08 22:33:50.536955: val_loss -0.8802 
2024-05-08 22:33:50.537004: Pseudo dice [0.9364] 
2024-05-08 22:33:50.537057: Epoch time: 41.03 s 
2024-05-08 22:33:51.607432:  
2024-05-08 22:33:51.607583: Epoch 872 
2024-05-08 22:33:51.607684: Current learning rate: 0.00157 
2024-05-08 22:34:32.647393: train_loss -0.873 
2024-05-08 22:34:32.647570: val_loss -0.8803 
2024-05-08 22:34:32.647619: Pseudo dice [0.9337] 
2024-05-08 22:34:32.647672: Epoch time: 41.04 s 
2024-05-08 22:34:33.720346:  
2024-05-08 22:34:33.720475: Epoch 873 
2024-05-08 22:34:33.720568: Current learning rate: 0.00156 
2024-05-08 22:35:15.120013: train_loss -0.8754 
2024-05-08 22:35:15.120216: val_loss -0.88 
2024-05-08 22:35:15.120267: Pseudo dice [0.9346] 
2024-05-08 22:35:15.120320: Epoch time: 41.4 s 
2024-05-08 22:35:16.396166:  
2024-05-08 22:35:16.396312: Epoch 874 
2024-05-08 22:35:16.396420: Current learning rate: 0.00155 
2024-05-08 22:35:57.434424: train_loss -0.8739 
2024-05-08 22:35:57.434607: val_loss -0.8854 
2024-05-08 22:35:57.434672: Pseudo dice [0.937] 
2024-05-08 22:35:57.434727: Epoch time: 41.04 s 
2024-05-08 22:35:58.502079:  
2024-05-08 22:35:58.502223: Epoch 875 
2024-05-08 22:35:58.502314: Current learning rate: 0.00154 
2024-05-08 22:36:39.535070: train_loss -0.8748 
2024-05-08 22:36:39.535261: val_loss -0.8889 
2024-05-08 22:36:39.535310: Pseudo dice [0.9413] 
2024-05-08 22:36:39.535365: Epoch time: 41.03 s 
2024-05-08 22:36:39.535410: Yayy! New best EMA pseudo Dice: 0.936 
2024-05-08 22:36:40.962501:  
2024-05-08 22:36:40.962683: Epoch 876 
2024-05-08 22:36:40.962774: Current learning rate: 0.00153 
2024-05-08 22:37:21.981349: train_loss -0.8771 
2024-05-08 22:37:21.981529: val_loss -0.8852 
2024-05-08 22:37:21.981577: Pseudo dice [0.936] 
2024-05-08 22:37:21.981631: Epoch time: 41.02 s 
2024-05-08 22:37:21.981675: Yayy! New best EMA pseudo Dice: 0.936 
2024-05-08 22:37:23.414906:  
2024-05-08 22:37:23.415099: Epoch 877 
2024-05-08 22:37:23.415197: Current learning rate: 0.00152 
2024-05-08 22:38:04.446759: train_loss -0.8791 
2024-05-08 22:38:04.446940: val_loss -0.887 
2024-05-08 22:38:04.446990: Pseudo dice [0.9409] 
2024-05-08 22:38:04.447043: Epoch time: 41.03 s 
2024-05-08 22:38:04.447087: Yayy! New best EMA pseudo Dice: 0.9365 
2024-05-08 22:38:05.878584:  
2024-05-08 22:38:05.878825: Epoch 878 
2024-05-08 22:38:05.878937: Current learning rate: 0.00151 
2024-05-08 22:38:46.900629: train_loss -0.8792 
2024-05-08 22:38:46.900821: val_loss -0.8757 
2024-05-08 22:38:46.900870: Pseudo dice [0.9349] 
2024-05-08 22:38:46.900924: Epoch time: 41.02 s 
2024-05-08 22:38:47.967151:  
2024-05-08 22:38:47.967297: Epoch 879 
2024-05-08 22:38:47.967390: Current learning rate: 0.00149 
2024-05-08 22:39:28.945781: train_loss -0.8761 
2024-05-08 22:39:28.945957: val_loss -0.8753 
2024-05-08 22:39:28.946006: Pseudo dice [0.938] 
2024-05-08 22:39:28.946059: Epoch time: 40.98 s 
2024-05-08 22:39:30.196572:  
2024-05-08 22:39:30.196731: Epoch 880 
2024-05-08 22:39:30.196824: Current learning rate: 0.00148 
2024-05-08 22:40:11.209506: train_loss -0.8765 
2024-05-08 22:40:11.209691: val_loss -0.8823 
2024-05-08 22:40:11.209741: Pseudo dice [0.9339] 
2024-05-08 22:40:11.209795: Epoch time: 41.01 s 
2024-05-08 22:40:12.277893:  
2024-05-08 22:40:12.278027: Epoch 881 
2024-05-08 22:40:12.278120: Current learning rate: 0.00147 
2024-05-08 22:40:53.280056: train_loss -0.876 
2024-05-08 22:40:53.280239: val_loss -0.891 
2024-05-08 22:40:53.280289: Pseudo dice [0.942] 
2024-05-08 22:40:53.280342: Epoch time: 41.0 s 
2024-05-08 22:40:53.280385: Yayy! New best EMA pseudo Dice: 0.9368 
2024-05-08 22:40:54.716907:  
2024-05-08 22:40:54.717046: Epoch 882 
2024-05-08 22:40:54.717146: Current learning rate: 0.00146 
2024-05-08 22:41:36.162353: train_loss -0.8808 
2024-05-08 22:41:36.162541: val_loss -0.8912 
2024-05-08 22:41:36.162591: Pseudo dice [0.9415] 
2024-05-08 22:41:36.162644: Epoch time: 41.45 s 
2024-05-08 22:41:36.162688: Yayy! New best EMA pseudo Dice: 0.9373 
2024-05-08 22:41:37.586552:  
2024-05-08 22:41:37.586771: Epoch 883 
2024-05-08 22:41:37.586868: Current learning rate: 0.00145 
2024-05-08 22:42:18.673009: train_loss -0.8802 
2024-05-08 22:42:18.673187: val_loss -0.8862 
2024-05-08 22:42:18.673236: Pseudo dice [0.9378] 
2024-05-08 22:42:18.673290: Epoch time: 41.09 s 
2024-05-08 22:42:18.673333: Yayy! New best EMA pseudo Dice: 0.9373 
2024-05-08 22:42:20.107456:  
2024-05-08 22:42:20.107596: Epoch 884 
2024-05-08 22:42:20.107689: Current learning rate: 0.00144 
2024-05-08 22:43:01.192376: train_loss -0.8786 
2024-05-08 22:43:01.192555: val_loss -0.8744 
2024-05-08 22:43:01.192604: Pseudo dice [0.9289] 
2024-05-08 22:43:01.192663: Epoch time: 41.09 s 
2024-05-08 22:43:02.261220:  
2024-05-08 22:43:02.261350: Epoch 885 
2024-05-08 22:43:02.261443: Current learning rate: 0.00143 
2024-05-08 22:43:43.311323: train_loss -0.8792 
2024-05-08 22:43:43.311497: val_loss -0.8811 
2024-05-08 22:43:43.311548: Pseudo dice [0.9332] 
2024-05-08 22:43:43.311603: Epoch time: 41.05 s 
2024-05-08 22:43:44.576375:  
2024-05-08 22:43:44.576587: Epoch 886 
2024-05-08 22:43:44.576698: Current learning rate: 0.00142 
2024-05-08 22:44:25.631719: train_loss -0.8756 
2024-05-08 22:44:25.631907: val_loss -0.8771 
2024-05-08 22:44:25.631959: Pseudo dice [0.9323] 
2024-05-08 22:44:25.632013: Epoch time: 41.06 s 
2024-05-08 22:44:26.702254:  
2024-05-08 22:44:26.702514: Epoch 887 
2024-05-08 22:44:26.702614: Current learning rate: 0.00141 
2024-05-08 22:45:07.768272: train_loss -0.8793 
2024-05-08 22:45:07.768461: val_loss -0.8802 
2024-05-08 22:45:07.768512: Pseudo dice [0.9315] 
2024-05-08 22:45:07.768565: Epoch time: 41.07 s 
2024-05-08 22:45:08.836667:  
2024-05-08 22:45:08.836811: Epoch 888 
2024-05-08 22:45:08.836915: Current learning rate: 0.00139 
2024-05-08 22:45:50.291977: train_loss -0.8779 
2024-05-08 22:45:50.292154: val_loss -0.8814 
2024-05-08 22:45:50.292204: Pseudo dice [0.9339] 
2024-05-08 22:45:50.292256: Epoch time: 41.46 s 
2024-05-08 22:45:51.359143:  
2024-05-08 22:45:51.359275: Epoch 889 
2024-05-08 22:45:51.359366: Current learning rate: 0.00138 
2024-05-08 22:46:32.473599: train_loss -0.8792 
2024-05-08 22:46:32.473781: val_loss -0.8711 
2024-05-08 22:46:32.473830: Pseudo dice [0.9288] 
2024-05-08 22:46:32.473884: Epoch time: 41.12 s 
2024-05-08 22:46:33.543003:  
2024-05-08 22:46:33.543145: Epoch 890 
2024-05-08 22:46:33.543238: Current learning rate: 0.00137 
2024-05-08 22:47:14.666406: train_loss -0.8847 
2024-05-08 22:47:14.666588: val_loss -0.8833 
2024-05-08 22:47:14.666638: Pseudo dice [0.9343] 
2024-05-08 22:47:14.666692: Epoch time: 41.12 s 
2024-05-08 22:47:15.746638:  
2024-05-08 22:47:15.746781: Epoch 891 
2024-05-08 22:47:15.746883: Current learning rate: 0.00136 
2024-05-08 22:47:56.880023: train_loss -0.8741 
2024-05-08 22:47:56.880202: val_loss -0.8867 
2024-05-08 22:47:56.880250: Pseudo dice [0.9384] 
2024-05-08 22:47:56.880305: Epoch time: 41.13 s 
2024-05-08 22:47:57.950355:  
2024-05-08 22:47:57.950491: Epoch 892 
2024-05-08 22:47:57.950585: Current learning rate: 0.00135 
2024-05-08 22:48:39.465791: train_loss -0.8779 
2024-05-08 22:48:39.465974: val_loss -0.8814 
2024-05-08 22:48:39.466023: Pseudo dice [0.937] 
2024-05-08 22:48:39.466084: Epoch time: 41.52 s 
2024-05-08 22:48:40.739735:  
2024-05-08 22:48:40.739922: Epoch 893 
2024-05-08 22:48:40.740018: Current learning rate: 0.00134 
2024-05-08 22:49:21.836290: train_loss -0.8773 
2024-05-08 22:49:21.836482: val_loss -0.8831 
2024-05-08 22:49:21.836531: Pseudo dice [0.9355] 
2024-05-08 22:49:21.836584: Epoch time: 41.1 s 
2024-05-08 22:49:22.902412:  
2024-05-08 22:49:22.902563: Epoch 894 
2024-05-08 22:49:22.902655: Current learning rate: 0.00133 
2024-05-08 22:50:03.994892: train_loss -0.8824 
2024-05-08 22:50:03.995087: val_loss -0.8838 
2024-05-08 22:50:03.995137: Pseudo dice [0.936] 
2024-05-08 22:50:03.995192: Epoch time: 41.09 s 
2024-05-08 22:50:05.063459:  
2024-05-08 22:50:05.063601: Epoch 895 
2024-05-08 22:50:05.063694: Current learning rate: 0.00132 
2024-05-08 22:50:46.147518: train_loss -0.8776 
2024-05-08 22:50:46.147695: val_loss -0.8851 
2024-05-08 22:50:46.147744: Pseudo dice [0.9371] 
2024-05-08 22:50:46.147799: Epoch time: 41.09 s 
2024-05-08 22:50:47.213547:  
2024-05-08 22:50:47.213690: Epoch 896 
2024-05-08 22:50:47.213782: Current learning rate: 0.0013 
2024-05-08 22:51:28.301153: train_loss -0.8809 
2024-05-08 22:51:28.301331: val_loss -0.891 
2024-05-08 22:51:28.301380: Pseudo dice [0.9407] 
2024-05-08 22:51:28.301435: Epoch time: 41.09 s 
2024-05-08 22:51:29.367273:  
2024-05-08 22:51:29.367413: Epoch 897 
2024-05-08 22:51:29.367506: Current learning rate: 0.00129 
2024-05-08 22:52:10.843708: train_loss -0.8822 
2024-05-08 22:52:10.843878: val_loss -0.8723 
2024-05-08 22:52:10.843926: Pseudo dice [0.9284] 
2024-05-08 22:52:10.843979: Epoch time: 41.48 s 
2024-05-08 22:52:11.912413:  
2024-05-08 22:52:11.912542: Epoch 898 
2024-05-08 22:52:11.912637: Current learning rate: 0.00128 
2024-05-08 22:52:52.993281: train_loss -0.8783 
2024-05-08 22:52:52.993464: val_loss -0.8762 
2024-05-08 22:52:52.993511: Pseudo dice [0.937] 
2024-05-08 22:52:52.993578: Epoch time: 41.08 s 
2024-05-08 22:52:54.262601:  
2024-05-08 22:52:54.262835: Epoch 899 
2024-05-08 22:52:54.262949: Current learning rate: 0.00127 
2024-05-08 22:53:35.307530: train_loss -0.8763 
2024-05-08 22:53:35.307717: val_loss -0.8859 
2024-05-08 22:53:35.307772: Pseudo dice [0.9389] 
2024-05-08 22:53:35.307832: Epoch time: 41.05 s 
2024-05-08 22:53:36.752706:  
2024-05-08 22:53:36.752920: Epoch 900 
2024-05-08 22:53:36.753022: Current learning rate: 0.00126 
2024-05-08 22:54:17.875690: train_loss -0.8759 
2024-05-08 22:54:17.875871: val_loss -0.8926 
2024-05-08 22:54:17.875920: Pseudo dice [0.9393] 
2024-05-08 22:54:17.875974: Epoch time: 41.12 s 
2024-05-08 22:54:18.943499:  
2024-05-08 22:54:18.943692: Epoch 901 
2024-05-08 22:54:18.943788: Current learning rate: 0.00125 
2024-05-08 22:55:00.068037: train_loss -0.8814 
2024-05-08 22:55:00.068229: val_loss -0.8878 
2024-05-08 22:55:00.068285: Pseudo dice [0.941] 
2024-05-08 22:55:00.068338: Epoch time: 41.13 s 
2024-05-08 22:55:01.133956:  
2024-05-08 22:55:01.134159: Epoch 902 
2024-05-08 22:55:01.134261: Current learning rate: 0.00124 
2024-05-08 22:55:42.249081: train_loss -0.8768 
2024-05-08 22:55:42.249262: val_loss -0.8894 
2024-05-08 22:55:42.249310: Pseudo dice [0.9388] 
2024-05-08 22:55:42.249363: Epoch time: 41.12 s 
2024-05-08 22:55:43.317609:  
2024-05-08 22:55:43.317820: Epoch 903 
2024-05-08 22:55:43.317917: Current learning rate: 0.00122 
2024-05-08 22:56:24.432216: train_loss -0.8791 
2024-05-08 22:56:24.432389: val_loss -0.8909 
2024-05-08 22:56:24.432438: Pseudo dice [0.9403] 
2024-05-08 22:56:24.432493: Epoch time: 41.12 s 
2024-05-08 22:56:25.499412:  
2024-05-08 22:56:25.499546: Epoch 904 
2024-05-08 22:56:25.499638: Current learning rate: 0.00121 
2024-05-08 22:57:06.600282: train_loss -0.8786 
2024-05-08 22:57:06.600463: val_loss -0.8948 
2024-05-08 22:57:06.600512: Pseudo dice [0.9404] 
2024-05-08 22:57:06.600564: Epoch time: 41.1 s 
2024-05-08 22:57:06.600606: Yayy! New best EMA pseudo Dice: 0.9375 
2024-05-08 22:57:08.035477:  
2024-05-08 22:57:08.035607: Epoch 905 
2024-05-08 22:57:08.035699: Current learning rate: 0.0012 
2024-05-08 22:57:49.144302: train_loss -0.8775 
2024-05-08 22:57:49.144483: val_loss -0.8924 
2024-05-08 22:57:49.144532: Pseudo dice [0.942] 
2024-05-08 22:57:49.144585: Epoch time: 41.11 s 
2024-05-08 22:57:49.144628: Yayy! New best EMA pseudo Dice: 0.9379 
2024-05-08 22:57:50.761929:  
2024-05-08 22:57:50.762111: Epoch 906 
2024-05-08 22:57:50.762235: Current learning rate: 0.00119 
2024-05-08 22:58:31.870167: train_loss -0.8761 
2024-05-08 22:58:31.870350: val_loss -0.8867 
2024-05-08 22:58:31.870402: Pseudo dice [0.9399] 
2024-05-08 22:58:31.870456: Epoch time: 41.11 s 
2024-05-08 22:58:31.870499: Yayy! New best EMA pseudo Dice: 0.9381 
2024-05-08 22:58:33.304749:  
2024-05-08 22:58:33.304933: Epoch 907 
2024-05-08 22:58:33.305026: Current learning rate: 0.00118 
2024-05-08 22:59:14.379411: train_loss -0.8798 
2024-05-08 22:59:14.379591: val_loss -0.8891 
2024-05-08 22:59:14.379640: Pseudo dice [0.9388] 
2024-05-08 22:59:14.379693: Epoch time: 41.08 s 
2024-05-08 22:59:14.379735: Yayy! New best EMA pseudo Dice: 0.9382 
2024-05-08 22:59:15.810483:  
2024-05-08 22:59:15.810688: Epoch 908 
2024-05-08 22:59:15.810785: Current learning rate: 0.00117 
2024-05-08 22:59:56.914739: train_loss -0.8775 
2024-05-08 22:59:56.914923: val_loss -0.8805 
2024-05-08 22:59:56.914972: Pseudo dice [0.939] 
2024-05-08 22:59:56.915025: Epoch time: 41.11 s 
2024-05-08 22:59:56.915068: Yayy! New best EMA pseudo Dice: 0.9383 
2024-05-08 22:59:58.358345:  
2024-05-08 22:59:58.358482: Epoch 909 
2024-05-08 22:59:58.358575: Current learning rate: 0.00116 
2024-05-08 23:00:39.468661: train_loss -0.8804 
2024-05-08 23:00:39.468840: val_loss -0.8866 
2024-05-08 23:00:39.468889: Pseudo dice [0.938] 
2024-05-08 23:00:39.468943: Epoch time: 41.11 s 
2024-05-08 23:00:40.547015:  
2024-05-08 23:00:40.547166: Epoch 910 
2024-05-08 23:00:40.547266: Current learning rate: 0.00115 
2024-05-08 23:01:21.692385: train_loss -0.882 
2024-05-08 23:01:21.692582: val_loss -0.8855 
2024-05-08 23:01:21.692631: Pseudo dice [0.9386] 
2024-05-08 23:01:21.692693: Epoch time: 41.15 s 
2024-05-08 23:01:21.692735: Yayy! New best EMA pseudo Dice: 0.9383 
2024-05-08 23:01:23.282580:  
2024-05-08 23:01:23.282732: Epoch 911 
2024-05-08 23:01:23.282824: Current learning rate: 0.00113 
2024-05-08 23:02:04.392251: train_loss -0.8803 
2024-05-08 23:02:04.392443: val_loss -0.878 
2024-05-08 23:02:04.392494: Pseudo dice [0.934] 
2024-05-08 23:02:04.392549: Epoch time: 41.11 s 
2024-05-08 23:02:05.460974:  
2024-05-08 23:02:05.461185: Epoch 912 
2024-05-08 23:02:05.461279: Current learning rate: 0.00112 
2024-05-08 23:02:46.572653: train_loss -0.8767 
2024-05-08 23:02:46.572832: val_loss -0.8797 
2024-05-08 23:02:46.572881: Pseudo dice [0.9356] 
2024-05-08 23:02:46.572933: Epoch time: 41.11 s 
2024-05-08 23:02:47.643232:  
2024-05-08 23:02:47.643366: Epoch 913 
2024-05-08 23:02:47.643456: Current learning rate: 0.00111 
2024-05-08 23:03:28.738766: train_loss -0.8836 
2024-05-08 23:03:28.738947: val_loss -0.8935 
2024-05-08 23:03:28.738997: Pseudo dice [0.938] 
2024-05-08 23:03:28.739049: Epoch time: 41.1 s 
2024-05-08 23:03:30.001389:  
2024-05-08 23:03:30.001533: Epoch 914 
2024-05-08 23:03:30.001628: Current learning rate: 0.0011 
2024-05-08 23:04:11.088033: train_loss -0.8785 
2024-05-08 23:04:11.088218: val_loss -0.8902 
2024-05-08 23:04:11.088267: Pseudo dice [0.9395] 
2024-05-08 23:04:11.088323: Epoch time: 41.09 s 
2024-05-08 23:04:12.151723:  
2024-05-08 23:04:12.151939: Epoch 915 
2024-05-08 23:04:12.152035: Current learning rate: 0.00109 
2024-05-08 23:04:53.238789: train_loss -0.877 
2024-05-08 23:04:53.238965: val_loss -0.8802 
2024-05-08 23:04:53.239016: Pseudo dice [0.9329] 
2024-05-08 23:04:53.239070: Epoch time: 41.09 s 
2024-05-08 23:04:54.306632:  
2024-05-08 23:04:54.306817: Epoch 916 
2024-05-08 23:04:54.306911: Current learning rate: 0.00108 
2024-05-08 23:05:35.384775: train_loss -0.8818 
2024-05-08 23:05:35.384959: val_loss -0.8842 
2024-05-08 23:05:35.385007: Pseudo dice [0.9377] 
2024-05-08 23:05:35.385060: Epoch time: 41.08 s 
2024-05-08 23:05:36.451095:  
2024-05-08 23:05:36.451230: Epoch 917 
2024-05-08 23:05:36.451326: Current learning rate: 0.00106 
2024-05-08 23:06:17.522497: train_loss -0.8771 
2024-05-08 23:06:17.522676: val_loss -0.8839 
2024-05-08 23:06:17.522725: Pseudo dice [0.9355] 
2024-05-08 23:06:17.522779: Epoch time: 41.07 s 
2024-05-08 23:06:18.786283:  
2024-05-08 23:06:18.786537: Epoch 918 
2024-05-08 23:06:18.786630: Current learning rate: 0.00105 
2024-05-08 23:06:59.872924: train_loss -0.8746 
2024-05-08 23:06:59.873113: val_loss -0.8841 
2024-05-08 23:06:59.873162: Pseudo dice [0.9349] 
2024-05-08 23:06:59.873215: Epoch time: 41.09 s 
2024-05-08 23:07:00.935752:  
2024-05-08 23:07:00.935892: Epoch 919 
2024-05-08 23:07:00.935991: Current learning rate: 0.00104 
2024-05-08 23:07:41.998391: train_loss -0.8784 
2024-05-08 23:07:41.998567: val_loss -0.8909 
2024-05-08 23:07:41.998616: Pseudo dice [0.9399] 
2024-05-08 23:07:41.998669: Epoch time: 41.06 s 
2024-05-08 23:07:43.064635:  
2024-05-08 23:07:43.064789: Epoch 920 
2024-05-08 23:07:43.064882: Current learning rate: 0.00103 
2024-05-08 23:08:24.129111: train_loss -0.8755 
2024-05-08 23:08:24.129295: val_loss -0.8813 
2024-05-08 23:08:24.129344: Pseudo dice [0.9357] 
2024-05-08 23:08:24.129398: Epoch time: 41.07 s 
2024-05-08 23:08:25.199559:  
2024-05-08 23:08:25.199699: Epoch 921 
2024-05-08 23:08:25.199793: Current learning rate: 0.00102 
2024-05-08 23:09:06.201285: train_loss -0.8807 
2024-05-08 23:09:06.201461: val_loss -0.8824 
2024-05-08 23:09:06.201511: Pseudo dice [0.9361] 
2024-05-08 23:09:06.201565: Epoch time: 41.0 s 
2024-05-08 23:09:07.285258:  
2024-05-08 23:09:07.285402: Epoch 922 
2024-05-08 23:09:07.285503: Current learning rate: 0.00101 
2024-05-08 23:09:48.290743: train_loss -0.8819 
2024-05-08 23:09:48.290924: val_loss -0.8823 
2024-05-08 23:09:48.290973: Pseudo dice [0.9354] 
2024-05-08 23:09:48.291026: Epoch time: 41.01 s 
2024-05-08 23:09:49.357413:  
2024-05-08 23:09:49.357557: Epoch 923 
2024-05-08 23:09:49.357650: Current learning rate: 0.001 
2024-05-08 23:10:30.360822: train_loss -0.881 
2024-05-08 23:10:30.361006: val_loss -0.8742 
2024-05-08 23:10:30.361056: Pseudo dice [0.929] 
2024-05-08 23:10:30.361108: Epoch time: 41.0 s 
2024-05-08 23:10:31.452204:  
2024-05-08 23:10:31.452411: Epoch 924 
2024-05-08 23:10:31.452505: Current learning rate: 0.00098 
2024-05-08 23:11:12.462296: train_loss -0.8754 
2024-05-08 23:11:12.462476: val_loss -0.873 
2024-05-08 23:11:12.462527: Pseudo dice [0.9314] 
2024-05-08 23:11:12.462582: Epoch time: 41.01 s 
2024-05-08 23:11:13.728472:  
2024-05-08 23:11:13.728615: Epoch 925 
2024-05-08 23:11:13.728715: Current learning rate: 0.00097 
2024-05-08 23:11:54.742890: train_loss -0.8794 
2024-05-08 23:11:54.743072: val_loss -0.8889 
2024-05-08 23:11:54.743121: Pseudo dice [0.9392] 
2024-05-08 23:11:54.743174: Epoch time: 41.02 s 
2024-05-08 23:11:55.808877:  
2024-05-08 23:11:55.809019: Epoch 926 
2024-05-08 23:11:55.809113: Current learning rate: 0.00096 
2024-05-08 23:12:36.823782: train_loss -0.8774 
2024-05-08 23:12:36.823962: val_loss -0.8783 
2024-05-08 23:12:36.824013: Pseudo dice [0.9327] 
2024-05-08 23:12:36.824068: Epoch time: 41.02 s 
2024-05-08 23:12:37.889150:  
2024-05-08 23:12:37.889283: Epoch 927 
2024-05-08 23:12:37.889378: Current learning rate: 0.00095 
2024-05-08 23:13:18.915903: train_loss -0.8781 
2024-05-08 23:13:18.916079: val_loss -0.8872 
2024-05-08 23:13:18.916127: Pseudo dice [0.9417] 
2024-05-08 23:13:18.916181: Epoch time: 41.03 s 
2024-05-08 23:13:19.979220:  
2024-05-08 23:13:19.979357: Epoch 928 
2024-05-08 23:13:19.979455: Current learning rate: 0.00094 
2024-05-08 23:14:01.009487: train_loss -0.88 
2024-05-08 23:14:01.009666: val_loss -0.8881 
2024-05-08 23:14:01.009716: Pseudo dice [0.9385] 
2024-05-08 23:14:01.009768: Epoch time: 41.03 s 
2024-05-08 23:14:02.077922:  
2024-05-08 23:14:02.078055: Epoch 929 
2024-05-08 23:14:02.078147: Current learning rate: 0.00092 
2024-05-08 23:14:43.101861: train_loss -0.878 
2024-05-08 23:14:43.102032: val_loss -0.885 
2024-05-08 23:14:43.102081: Pseudo dice [0.9385] 
2024-05-08 23:14:43.102134: Epoch time: 41.03 s 
2024-05-08 23:14:44.165448:  
2024-05-08 23:14:44.165580: Epoch 930 
2024-05-08 23:14:44.165673: Current learning rate: 0.00091 
2024-05-08 23:15:25.195304: train_loss -0.8821 
2024-05-08 23:15:25.195484: val_loss -0.8885 
2024-05-08 23:15:25.195534: Pseudo dice [0.9363] 
2024-05-08 23:15:25.195588: Epoch time: 41.03 s 
2024-05-08 23:15:26.438265:  
2024-05-08 23:15:26.438422: Epoch 931 
2024-05-08 23:15:26.438521: Current learning rate: 0.0009 
2024-05-08 23:16:07.471333: train_loss -0.8787 
2024-05-08 23:16:07.471517: val_loss -0.8784 
2024-05-08 23:16:07.471566: Pseudo dice [0.9348] 
2024-05-08 23:16:07.471620: Epoch time: 41.03 s 
2024-05-08 23:16:08.537131:  
2024-05-08 23:16:08.537276: Epoch 932 
2024-05-08 23:16:08.537369: Current learning rate: 0.00089 
2024-05-08 23:16:49.618545: train_loss -0.8781 
2024-05-08 23:16:49.618725: val_loss -0.8852 
2024-05-08 23:16:49.618773: Pseudo dice [0.9398] 
2024-05-08 23:16:49.618828: Epoch time: 41.08 s 
2024-05-08 23:16:50.685958:  
2024-05-08 23:16:50.686097: Epoch 933 
2024-05-08 23:16:50.686200: Current learning rate: 0.00088 
2024-05-08 23:17:31.788304: train_loss -0.8823 
2024-05-08 23:17:31.788483: val_loss -0.8932 
2024-05-08 23:17:31.788543: Pseudo dice [0.9412] 
2024-05-08 23:17:31.788596: Epoch time: 41.1 s 
2024-05-08 23:17:32.857162:  
2024-05-08 23:17:32.857298: Epoch 934 
2024-05-08 23:17:32.857390: Current learning rate: 0.00087 
2024-05-08 23:18:13.938724: train_loss -0.8806 
2024-05-08 23:18:13.938898: val_loss -0.8766 
2024-05-08 23:18:13.938946: Pseudo dice [0.936] 
2024-05-08 23:18:13.938998: Epoch time: 41.08 s 
2024-05-08 23:18:15.029756:  
2024-05-08 23:18:15.029890: Epoch 935 
2024-05-08 23:18:15.029979: Current learning rate: 0.00085 
2024-05-08 23:18:56.126222: train_loss -0.8813 
2024-05-08 23:18:56.126406: val_loss -0.8915 
2024-05-08 23:18:56.126455: Pseudo dice [0.941] 
2024-05-08 23:18:56.126520: Epoch time: 41.1 s 
2024-05-08 23:18:57.193414:  
2024-05-08 23:18:57.193547: Epoch 936 
2024-05-08 23:18:57.193637: Current learning rate: 0.00084 
2024-05-08 23:19:38.303021: train_loss -0.8839 
2024-05-08 23:19:38.303196: val_loss -0.8815 
2024-05-08 23:19:38.303246: Pseudo dice [0.9328] 
2024-05-08 23:19:38.303300: Epoch time: 41.11 s 
2024-05-08 23:19:39.399646:  
2024-05-08 23:19:39.399981: Epoch 937 
2024-05-08 23:19:39.400175: Current learning rate: 0.00083 
2024-05-08 23:20:20.502450: train_loss -0.8779 
2024-05-08 23:20:20.502631: val_loss -0.8849 
2024-05-08 23:20:20.502681: Pseudo dice [0.9371] 
2024-05-08 23:20:20.502735: Epoch time: 41.1 s 
2024-05-08 23:20:21.796024:  
2024-05-08 23:20:21.796214: Epoch 938 
2024-05-08 23:20:21.796318: Current learning rate: 0.00082 
2024-05-08 23:21:02.905155: train_loss -0.8838 
2024-05-08 23:21:02.905337: val_loss -0.8827 
2024-05-08 23:21:02.905385: Pseudo dice [0.9362] 
2024-05-08 23:21:02.905439: Epoch time: 41.11 s 
2024-05-08 23:21:03.976728:  
2024-05-08 23:21:03.976958: Epoch 939 
2024-05-08 23:21:03.977054: Current learning rate: 0.00081 
2024-05-08 23:21:45.088848: train_loss -0.8799 
2024-05-08 23:21:45.089033: val_loss -0.8851 
2024-05-08 23:21:45.089082: Pseudo dice [0.9369] 
2024-05-08 23:21:45.089134: Epoch time: 41.11 s 
2024-05-08 23:21:46.157421:  
2024-05-08 23:21:46.157570: Epoch 940 
2024-05-08 23:21:46.157665: Current learning rate: 0.00079 
2024-05-08 23:22:27.264179: train_loss -0.8802 
2024-05-08 23:22:27.264355: val_loss -0.8818 
2024-05-08 23:22:27.264404: Pseudo dice [0.9338] 
2024-05-08 23:22:27.264457: Epoch time: 41.11 s 
2024-05-08 23:22:28.333717:  
2024-05-08 23:22:28.333857: Epoch 941 
2024-05-08 23:22:28.333949: Current learning rate: 0.00078 
2024-05-08 23:23:09.438996: train_loss -0.8785 
2024-05-08 23:23:09.439172: val_loss -0.8855 
2024-05-08 23:23:09.439220: Pseudo dice [0.937] 
2024-05-08 23:23:09.439272: Epoch time: 41.11 s 
2024-05-08 23:23:10.507632:  
2024-05-08 23:23:10.507774: Epoch 942 
2024-05-08 23:23:10.507873: Current learning rate: 0.00077 
2024-05-08 23:23:51.633620: train_loss -0.8832 
2024-05-08 23:23:51.633807: val_loss -0.886 
2024-05-08 23:23:51.633859: Pseudo dice [0.9369] 
2024-05-08 23:23:51.633913: Epoch time: 41.13 s 
2024-05-08 23:23:52.699586:  
2024-05-08 23:23:52.699720: Epoch 943 
2024-05-08 23:23:52.699815: Current learning rate: 0.00076 
2024-05-08 23:24:33.812709: train_loss -0.8764 
2024-05-08 23:24:33.812896: val_loss -0.8919 
2024-05-08 23:24:33.812946: Pseudo dice [0.9385] 
2024-05-08 23:24:33.812999: Epoch time: 41.11 s 
2024-05-08 23:24:34.879014:  
2024-05-08 23:24:34.879145: Epoch 944 
2024-05-08 23:24:34.879238: Current learning rate: 0.00075 
2024-05-08 23:25:15.936858: train_loss -0.8808 
2024-05-08 23:25:15.937053: val_loss -0.8837 
2024-05-08 23:25:15.937102: Pseudo dice [0.9352] 
2024-05-08 23:25:15.937155: Epoch time: 41.06 s 
2024-05-08 23:25:17.201562:  
2024-05-08 23:25:17.201769: Epoch 945 
2024-05-08 23:25:17.201863: Current learning rate: 0.00074 
2024-05-08 23:25:58.253412: train_loss -0.8882 
2024-05-08 23:25:58.253597: val_loss -0.8982 
2024-05-08 23:25:58.253649: Pseudo dice [0.9454] 
2024-05-08 23:25:58.253702: Epoch time: 41.05 s 
2024-05-08 23:25:59.319614:  
2024-05-08 23:25:59.319774: Epoch 946 
2024-05-08 23:25:59.319874: Current learning rate: 0.00072 
2024-05-08 23:26:40.371530: train_loss -0.8789 
2024-05-08 23:26:40.371716: val_loss -0.8869 
2024-05-08 23:26:40.371766: Pseudo dice [0.9385] 
2024-05-08 23:26:40.371818: Epoch time: 41.05 s 
2024-05-08 23:26:41.441990:  
2024-05-08 23:26:41.442138: Epoch 947 
2024-05-08 23:26:41.442229: Current learning rate: 0.00071 
2024-05-08 23:27:22.502313: train_loss -0.8842 
2024-05-08 23:27:22.502496: val_loss -0.8843 
2024-05-08 23:27:22.502549: Pseudo dice [0.9369] 
2024-05-08 23:27:22.502603: Epoch time: 41.06 s 
2024-05-08 23:27:23.568484:  
2024-05-08 23:27:23.568629: Epoch 948 
2024-05-08 23:27:23.568728: Current learning rate: 0.0007 
2024-05-08 23:28:04.624728: train_loss -0.8783 
2024-05-08 23:28:04.624914: val_loss -0.8868 
2024-05-08 23:28:04.624963: Pseudo dice [0.9356] 
2024-05-08 23:28:04.625015: Epoch time: 41.06 s 
2024-05-08 23:28:05.691988:  
2024-05-08 23:28:05.692126: Epoch 949 
2024-05-08 23:28:05.692217: Current learning rate: 0.00069 
2024-05-08 23:28:46.754585: train_loss -0.8793 
2024-05-08 23:28:46.754770: val_loss -0.8804 
2024-05-08 23:28:46.754819: Pseudo dice [0.9333] 
2024-05-08 23:28:46.754872: Epoch time: 41.06 s 
2024-05-08 23:28:48.181559:  
2024-05-08 23:28:48.181879: Epoch 950 
2024-05-08 23:28:48.181984: Current learning rate: 0.00067 
2024-05-08 23:29:29.242274: train_loss -0.8789 
2024-05-08 23:29:29.242459: val_loss -0.8944 
2024-05-08 23:29:29.242509: Pseudo dice [0.9425] 
2024-05-08 23:29:29.242563: Epoch time: 41.06 s 
2024-05-08 23:29:30.497511:  
2024-05-08 23:29:30.497653: Epoch 951 
2024-05-08 23:29:30.497746: Current learning rate: 0.00066 
2024-05-08 23:30:11.579720: train_loss -0.8819 
2024-05-08 23:30:11.579905: val_loss -0.8957 
2024-05-08 23:30:11.579958: Pseudo dice [0.9418] 
2024-05-08 23:30:11.580014: Epoch time: 41.08 s 
2024-05-08 23:30:12.646855:  
2024-05-08 23:30:12.647012: Epoch 952 
2024-05-08 23:30:12.647106: Current learning rate: 0.00065 
2024-05-08 23:30:53.751009: train_loss -0.8857 
2024-05-08 23:30:53.751191: val_loss -0.881 
2024-05-08 23:30:53.751241: Pseudo dice [0.9365] 
2024-05-08 23:30:53.751293: Epoch time: 41.11 s 
2024-05-08 23:30:54.816405:  
2024-05-08 23:30:54.816534: Epoch 953 
2024-05-08 23:30:54.816633: Current learning rate: 0.00064 
2024-05-08 23:31:35.918387: train_loss -0.8828 
2024-05-08 23:31:35.918568: val_loss -0.887 
2024-05-08 23:31:35.918619: Pseudo dice [0.9398] 
2024-05-08 23:31:35.918673: Epoch time: 41.1 s 
2024-05-08 23:31:37.004020:  
2024-05-08 23:31:37.004199: Epoch 954 
2024-05-08 23:31:37.004294: Current learning rate: 0.00063 
2024-05-08 23:32:18.100584: train_loss -0.8818 
2024-05-08 23:32:18.100768: val_loss -0.885 
2024-05-08 23:32:18.100820: Pseudo dice [0.9399] 
2024-05-08 23:32:18.100873: Epoch time: 41.1 s 
2024-05-08 23:32:19.186525:  
2024-05-08 23:32:19.186649: Epoch 955 
2024-05-08 23:32:19.186745: Current learning rate: 0.00061 
2024-05-08 23:33:00.261433: train_loss -0.8822 
2024-05-08 23:33:00.261610: val_loss -0.8897 
2024-05-08 23:33:00.261659: Pseudo dice [0.9394] 
2024-05-08 23:33:00.261714: Epoch time: 41.08 s 
2024-05-08 23:33:00.261756: Yayy! New best EMA pseudo Dice: 0.9383 
2024-05-08 23:33:01.704965:  
2024-05-08 23:33:01.705264: Epoch 956 
2024-05-08 23:33:01.705357: Current learning rate: 0.0006 
2024-05-08 23:33:42.773885: train_loss -0.8824 
2024-05-08 23:33:42.774066: val_loss -0.8702 
2024-05-08 23:33:42.774116: Pseudo dice [0.937] 
2024-05-08 23:33:42.774169: Epoch time: 41.07 s 
2024-05-08 23:33:44.036906:  
2024-05-08 23:33:44.037044: Epoch 957 
2024-05-08 23:33:44.037134: Current learning rate: 0.00059 
2024-05-08 23:34:25.110861: train_loss -0.8803 
2024-05-08 23:34:25.111074: val_loss -0.8776 
2024-05-08 23:34:25.111149: Pseudo dice [0.9369] 
2024-05-08 23:34:25.111265: Epoch time: 41.07 s 
2024-05-08 23:34:26.200350:  
2024-05-08 23:34:26.200492: Epoch 958 
2024-05-08 23:34:26.200581: Current learning rate: 0.00058 
2024-05-08 23:35:07.279145: train_loss -0.8826 
2024-05-08 23:35:07.279325: val_loss -0.8888 
2024-05-08 23:35:07.279374: Pseudo dice [0.9394] 
2024-05-08 23:35:07.279426: Epoch time: 41.08 s 
2024-05-08 23:35:08.377254:  
2024-05-08 23:35:08.377444: Epoch 959 
2024-05-08 23:35:08.377578: Current learning rate: 0.00056 
2024-05-08 23:35:49.465405: train_loss -0.8808 
2024-05-08 23:35:49.465583: val_loss -0.8817 
2024-05-08 23:35:49.465631: Pseudo dice [0.9376] 
2024-05-08 23:35:49.465684: Epoch time: 41.09 s 
2024-05-08 23:35:50.551325:  
2024-05-08 23:35:50.551463: Epoch 960 
2024-05-08 23:35:50.551556: Current learning rate: 0.00055 
2024-05-08 23:36:31.647423: train_loss -0.8785 
2024-05-08 23:36:31.647602: val_loss -0.8751 
2024-05-08 23:36:31.647651: Pseudo dice [0.9323] 
2024-05-08 23:36:31.647705: Epoch time: 41.1 s 
2024-05-08 23:36:32.743410:  
2024-05-08 23:36:32.743551: Epoch 961 
2024-05-08 23:36:32.743642: Current learning rate: 0.00054 
2024-05-08 23:37:13.823543: train_loss -0.8793 
2024-05-08 23:37:13.823724: val_loss -0.8899 
2024-05-08 23:37:13.823775: Pseudo dice [0.9402] 
2024-05-08 23:37:13.823830: Epoch time: 41.08 s 
2024-05-08 23:37:14.911789:  
2024-05-08 23:37:14.912013: Epoch 962 
2024-05-08 23:37:14.912114: Current learning rate: 0.00053 
2024-05-08 23:37:56.345716: train_loss -0.8821 
2024-05-08 23:37:56.345894: val_loss -0.876 
2024-05-08 23:37:56.345942: Pseudo dice [0.9326] 
2024-05-08 23:37:56.345995: Epoch time: 41.44 s 
2024-05-08 23:37:57.448356:  
2024-05-08 23:37:57.448490: Epoch 963 
2024-05-08 23:37:57.448584: Current learning rate: 0.00051 
2024-05-08 23:38:38.523403: train_loss -0.8862 
2024-05-08 23:38:38.523587: val_loss -0.8891 
2024-05-08 23:38:38.523636: Pseudo dice [0.9382] 
2024-05-08 23:38:38.523689: Epoch time: 41.08 s 
2024-05-08 23:38:39.798661:  
2024-05-08 23:38:39.798810: Epoch 964 
2024-05-08 23:38:39.798909: Current learning rate: 0.0005 
2024-05-08 23:39:21.259488: train_loss -0.8842 
2024-05-08 23:39:21.259736: val_loss -0.8927 
2024-05-08 23:39:21.259789: Pseudo dice [0.9435] 
2024-05-08 23:39:21.259846: Epoch time: 41.46 s 
2024-05-08 23:39:22.495476:  
2024-05-08 23:39:22.495644: Epoch 965 
2024-05-08 23:39:22.495737: Current learning rate: 0.00049 
2024-05-08 23:40:03.553864: train_loss -0.8812 
2024-05-08 23:40:03.554049: val_loss -0.8827 
2024-05-08 23:40:03.554098: Pseudo dice [0.9375] 
2024-05-08 23:40:03.554154: Epoch time: 41.06 s 
2024-05-08 23:40:04.624839:  
2024-05-08 23:40:04.624981: Epoch 966 
2024-05-08 23:40:04.625076: Current learning rate: 0.00048 
2024-05-08 23:40:45.681847: train_loss -0.8852 
2024-05-08 23:40:45.682028: val_loss -0.8842 
2024-05-08 23:40:45.682079: Pseudo dice [0.9399] 
2024-05-08 23:40:45.682132: Epoch time: 41.06 s 
2024-05-08 23:40:46.752238:  
2024-05-08 23:40:46.752376: Epoch 967 
2024-05-08 23:40:46.752516: Current learning rate: 0.00046 
2024-05-08 23:41:27.787688: train_loss -0.8806 
2024-05-08 23:41:27.787871: val_loss -0.8894 
2024-05-08 23:41:27.787920: Pseudo dice [0.9405] 
2024-05-08 23:41:27.787973: Epoch time: 41.04 s 
2024-05-08 23:41:27.788016: Yayy! New best EMA pseudo Dice: 0.9384 
2024-05-08 23:41:29.226883:  
2024-05-08 23:41:29.227119: Epoch 968 
2024-05-08 23:41:29.227216: Current learning rate: 0.00045 
2024-05-08 23:42:10.282209: train_loss -0.8846 
2024-05-08 23:42:10.282392: val_loss -0.8866 
2024-05-08 23:42:10.282444: Pseudo dice [0.9373] 
2024-05-08 23:42:10.282497: Epoch time: 41.06 s 
2024-05-08 23:42:11.357398:  
2024-05-08 23:42:11.357597: Epoch 969 
2024-05-08 23:42:11.357691: Current learning rate: 0.00044 
2024-05-08 23:42:52.763450: train_loss -0.8844 
2024-05-08 23:42:52.763637: val_loss -0.8825 
2024-05-08 23:42:52.763686: Pseudo dice [0.9394] 
2024-05-08 23:42:52.763739: Epoch time: 41.41 s 
2024-05-08 23:42:52.763782: Yayy! New best EMA pseudo Dice: 0.9384 
2024-05-08 23:42:54.381173:  
2024-05-08 23:42:54.381390: Epoch 970 
2024-05-08 23:42:54.381486: Current learning rate: 0.00043 
2024-05-08 23:43:35.471836: train_loss -0.8823 
2024-05-08 23:43:35.472023: val_loss -0.889 
2024-05-08 23:43:35.472073: Pseudo dice [0.9414] 
2024-05-08 23:43:35.472126: Epoch time: 41.09 s 
2024-05-08 23:43:35.472170: Yayy! New best EMA pseudo Dice: 0.9387 
2024-05-08 23:43:36.919348:  
2024-05-08 23:43:36.919624: Epoch 971 
2024-05-08 23:43:36.919723: Current learning rate: 0.00041 
2024-05-08 23:44:18.033504: train_loss -0.8828 
2024-05-08 23:44:18.033703: val_loss -0.8886 
2024-05-08 23:44:18.033753: Pseudo dice [0.9388] 
2024-05-08 23:44:18.033806: Epoch time: 41.12 s 
2024-05-08 23:44:18.033848: Yayy! New best EMA pseudo Dice: 0.9387 
2024-05-08 23:44:19.493368:  
2024-05-08 23:44:19.493584: Epoch 972 
2024-05-08 23:44:19.493711: Current learning rate: 0.0004 
2024-05-08 23:45:00.616518: train_loss -0.8844 
2024-05-08 23:45:00.616697: val_loss -0.8897 
2024-05-08 23:45:00.616750: Pseudo dice [0.9407] 
2024-05-08 23:45:00.616805: Epoch time: 41.12 s 
2024-05-08 23:45:00.616860: Yayy! New best EMA pseudo Dice: 0.9389 
2024-05-08 23:45:02.062254:  
2024-05-08 23:45:02.062464: Epoch 973 
2024-05-08 23:45:02.062558: Current learning rate: 0.00039 
2024-05-08 23:45:43.588516: train_loss -0.883 
2024-05-08 23:45:43.588708: val_loss -0.8994 
2024-05-08 23:45:43.588762: Pseudo dice [0.9446] 
2024-05-08 23:45:43.588814: Epoch time: 41.53 s 
2024-05-08 23:45:43.588856: Yayy! New best EMA pseudo Dice: 0.9395 
2024-05-08 23:45:45.051377:  
2024-05-08 23:45:45.051528: Epoch 974 
2024-05-08 23:45:45.051631: Current learning rate: 0.00037 
2024-05-08 23:46:26.172370: train_loss -0.885 
2024-05-08 23:46:26.172552: val_loss -0.8874 
2024-05-08 23:46:26.172601: Pseudo dice [0.9392] 
2024-05-08 23:46:26.172662: Epoch time: 41.12 s 
2024-05-08 23:46:27.258752:  
2024-05-08 23:46:27.258893: Epoch 975 
2024-05-08 23:46:27.258997: Current learning rate: 0.00036 
2024-05-08 23:47:08.360289: train_loss -0.8812 
2024-05-08 23:47:08.360465: val_loss -0.8911 
2024-05-08 23:47:08.360515: Pseudo dice [0.9394] 
2024-05-08 23:47:08.360569: Epoch time: 41.1 s 
2024-05-08 23:47:09.649333:  
2024-05-08 23:47:09.649480: Epoch 976 
2024-05-08 23:47:09.649577: Current learning rate: 0.00035 
2024-05-08 23:47:50.724521: train_loss -0.8815 
2024-05-08 23:47:50.724707: val_loss -0.8814 
2024-05-08 23:47:50.724757: Pseudo dice [0.94] 
2024-05-08 23:47:50.724811: Epoch time: 41.08 s 
2024-05-08 23:47:50.724854: Yayy! New best EMA pseudo Dice: 0.9395 
2024-05-08 23:47:52.172093:  
2024-05-08 23:47:52.172236: Epoch 977 
2024-05-08 23:47:52.172330: Current learning rate: 0.00034 
2024-05-08 23:48:33.224726: train_loss -0.889 
2024-05-08 23:48:33.224908: val_loss -0.8826 
2024-05-08 23:48:33.224957: Pseudo dice [0.9371] 
2024-05-08 23:48:33.225010: Epoch time: 41.05 s 
2024-05-08 23:48:34.305825:  
2024-05-08 23:48:34.305947: Epoch 978 
2024-05-08 23:48:34.306042: Current learning rate: 0.00032 
2024-05-08 23:49:15.763763: train_loss -0.8836 
2024-05-08 23:49:15.763953: val_loss -0.8896 
2024-05-08 23:49:15.764002: Pseudo dice [0.9408] 
2024-05-08 23:49:15.764056: Epoch time: 41.46 s 
2024-05-08 23:49:16.883535:  
2024-05-08 23:49:16.883666: Epoch 979 
2024-05-08 23:49:16.883756: Current learning rate: 0.00031 
2024-05-08 23:49:58.263543: train_loss -0.8854 
2024-05-08 23:49:58.263721: val_loss -0.886 
2024-05-08 23:49:58.263769: Pseudo dice [0.9391] 
2024-05-08 23:49:58.263823: Epoch time: 41.38 s 
2024-05-08 23:49:59.409239:  
2024-05-08 23:49:59.409377: Epoch 980 
2024-05-08 23:49:59.409467: Current learning rate: 0.0003 
2024-05-08 23:50:40.435754: train_loss -0.8857 
2024-05-08 23:50:40.435947: val_loss -0.8921 
2024-05-08 23:50:40.435996: Pseudo dice [0.939] 
2024-05-08 23:50:40.436050: Epoch time: 41.03 s 
2024-05-08 23:50:41.530047:  
2024-05-08 23:50:41.530183: Epoch 981 
2024-05-08 23:50:41.530275: Current learning rate: 0.00028 
2024-05-08 23:51:22.572915: train_loss -0.8828 
2024-05-08 23:51:22.573096: val_loss -0.8849 
2024-05-08 23:51:22.573146: Pseudo dice [0.9369] 
2024-05-08 23:51:22.573200: Epoch time: 41.04 s 
2024-05-08 23:51:23.842092:  
2024-05-08 23:51:23.842252: Epoch 982 
2024-05-08 23:51:23.842342: Current learning rate: 0.00027 
2024-05-08 23:52:04.878829: train_loss -0.8839 
2024-05-08 23:52:04.879009: val_loss -0.8793 
2024-05-08 23:52:04.879059: Pseudo dice [0.9344] 
2024-05-08 23:52:04.879112: Epoch time: 41.04 s 
2024-05-08 23:52:05.959261:  
2024-05-08 23:52:05.959456: Epoch 983 
2024-05-08 23:52:05.959548: Current learning rate: 0.00026 
2024-05-08 23:52:46.991277: train_loss -0.8858 
2024-05-08 23:52:46.991460: val_loss -0.8846 
2024-05-08 23:52:46.991509: Pseudo dice [0.937] 
2024-05-08 23:52:46.991563: Epoch time: 41.03 s 
2024-05-08 23:52:48.071716:  
2024-05-08 23:52:48.071858: Epoch 984 
2024-05-08 23:52:48.071952: Current learning rate: 0.00024 
2024-05-08 23:53:29.163528: train_loss -0.8888 
2024-05-08 23:53:29.163706: val_loss -0.881 
2024-05-08 23:53:29.163755: Pseudo dice [0.9359] 
2024-05-08 23:53:29.163808: Epoch time: 41.09 s 
2024-05-08 23:53:30.258682:  
2024-05-08 23:53:30.258820: Epoch 985 
2024-05-08 23:53:30.258906: Current learning rate: 0.00023 
2024-05-08 23:54:11.561306: train_loss -0.8807 
2024-05-08 23:54:11.561498: val_loss -0.8867 
2024-05-08 23:54:11.561547: Pseudo dice [0.9392] 
2024-05-08 23:54:11.561601: Epoch time: 41.3 s 
2024-05-08 23:54:12.651226:  
2024-05-08 23:54:12.651436: Epoch 986 
2024-05-08 23:54:12.651551: Current learning rate: 0.00021 
2024-05-08 23:54:53.784197: train_loss -0.8877 
2024-05-08 23:54:53.784377: val_loss -0.8806 
2024-05-08 23:54:53.784426: Pseudo dice [0.9381] 
2024-05-08 23:54:53.784478: Epoch time: 41.13 s 
2024-05-08 23:54:54.872416:  
2024-05-08 23:54:54.872552: Epoch 987 
2024-05-08 23:54:54.872642: Current learning rate: 0.0002 
2024-05-08 23:55:36.351720: train_loss -0.8853 
2024-05-08 23:55:36.351940: val_loss -0.8949 
2024-05-08 23:55:36.351991: Pseudo dice [0.9417] 
2024-05-08 23:55:36.352046: Epoch time: 41.48 s 
2024-05-08 23:55:37.553960:  
2024-05-08 23:55:37.554190: Epoch 988 
2024-05-08 23:55:37.554288: Current learning rate: 0.00019 
2024-05-08 23:56:18.666581: train_loss -0.8874 
2024-05-08 23:56:18.666757: val_loss -0.8916 
2024-05-08 23:56:18.666866: Pseudo dice [0.9406] 
2024-05-08 23:56:18.666965: Epoch time: 41.11 s 
2024-05-08 23:56:19.961720:  
2024-05-08 23:56:19.961869: Epoch 989 
2024-05-08 23:56:19.961961: Current learning rate: 0.00017 
2024-05-08 23:57:01.082358: train_loss -0.8899 
2024-05-08 23:57:01.082536: val_loss -0.8891 
2024-05-08 23:57:01.082587: Pseudo dice [0.9408] 
2024-05-08 23:57:01.082640: Epoch time: 41.12 s 
2024-05-08 23:57:02.185515:  
2024-05-08 23:57:02.185796: Epoch 990 
2024-05-08 23:57:02.185892: Current learning rate: 0.00016 
2024-05-08 23:57:43.279897: train_loss -0.8854 
2024-05-08 23:57:43.280086: val_loss -0.8838 
2024-05-08 23:57:43.280135: Pseudo dice [0.9398] 
2024-05-08 23:57:43.280188: Epoch time: 41.1 s 
2024-05-08 23:57:44.370907:  
2024-05-08 23:57:44.371047: Epoch 991 
2024-05-08 23:57:44.371136: Current learning rate: 0.00014 
2024-05-08 23:58:25.453414: train_loss -0.8838 
2024-05-08 23:58:25.453596: val_loss -0.8842 
2024-05-08 23:58:25.453645: Pseudo dice [0.9425] 
2024-05-08 23:58:25.453698: Epoch time: 41.08 s 
2024-05-08 23:58:26.539960:  
2024-05-08 23:58:26.540159: Epoch 992 
2024-05-08 23:58:26.540261: Current learning rate: 0.00013 
2024-05-08 23:59:07.553368: train_loss -0.8848 
2024-05-08 23:59:07.553552: val_loss -0.8754 
2024-05-08 23:59:07.553604: Pseudo dice [0.9355] 
2024-05-08 23:59:07.553657: Epoch time: 41.01 s 
2024-05-08 23:59:08.641007:  
2024-05-08 23:59:08.641290: Epoch 993 
2024-05-08 23:59:08.641443: Current learning rate: 0.00011 
2024-05-08 23:59:49.646888: train_loss -0.8848 
2024-05-08 23:59:49.647069: val_loss -0.8833 
2024-05-08 23:59:49.647118: Pseudo dice [0.9402] 
2024-05-08 23:59:49.647172: Epoch time: 41.01 s 
2024-05-08 23:59:50.738598:  
2024-05-08 23:59:50.738729: Epoch 994 
2024-05-08 23:59:50.738821: Current learning rate: 0.0001 
2024-05-09 00:00:31.757889: train_loss -0.8798 
2024-05-09 00:00:31.758071: val_loss -0.8839 
2024-05-09 00:00:31.758121: Pseudo dice [0.9377] 
2024-05-09 00:00:31.758174: Epoch time: 41.02 s 
2024-05-09 00:00:33.039208:  
2024-05-09 00:00:33.039354: Epoch 995 
2024-05-09 00:00:33.039448: Current learning rate: 8e-05 
2024-05-09 00:01:14.064121: train_loss -0.8855 
2024-05-09 00:01:14.064302: val_loss -0.892 
2024-05-09 00:01:14.064353: Pseudo dice [0.9413] 
2024-05-09 00:01:14.064407: Epoch time: 41.03 s 
2024-05-09 00:01:15.152514:  
2024-05-09 00:01:15.152658: Epoch 996 
2024-05-09 00:01:15.152749: Current learning rate: 7e-05 
2024-05-09 00:01:56.168604: train_loss -0.8845 
2024-05-09 00:01:56.168791: val_loss -0.8931 
2024-05-09 00:01:56.168841: Pseudo dice [0.9397] 
2024-05-09 00:01:56.168894: Epoch time: 41.02 s 
2024-05-09 00:01:57.256484:  
2024-05-09 00:01:57.256624: Epoch 997 
2024-05-09 00:01:57.256722: Current learning rate: 5e-05 
2024-05-09 00:02:38.287627: train_loss -0.8868 
2024-05-09 00:02:38.287864: val_loss -0.8986 
2024-05-09 00:02:38.287925: Pseudo dice [0.9442] 
2024-05-09 00:02:38.288002: Epoch time: 41.03 s 
2024-05-09 00:02:38.288055: Yayy! New best EMA pseudo Dice: 0.9398 
2024-05-09 00:02:39.742302:  
2024-05-09 00:02:39.742586: Epoch 998 
2024-05-09 00:02:39.742684: Current learning rate: 4e-05 
2024-05-09 00:03:20.778455: train_loss -0.889 
2024-05-09 00:03:20.778636: val_loss -0.8911 
2024-05-09 00:03:20.778684: Pseudo dice [0.941] 
2024-05-09 00:03:20.778738: Epoch time: 41.04 s 
2024-05-09 00:03:20.778781: Yayy! New best EMA pseudo Dice: 0.9399 
2024-05-09 00:03:22.226575:  
2024-05-09 00:03:22.226723: Epoch 999 
2024-05-09 00:03:22.226826: Current learning rate: 2e-05 
2024-05-09 00:04:03.249086: train_loss -0.8844 
2024-05-09 00:04:03.249266: val_loss -0.8966 
2024-05-09 00:04:03.249315: Pseudo dice [0.9446] 
2024-05-09 00:04:03.249367: Epoch time: 41.02 s 
2024-05-09 00:04:03.249409: Yayy! New best EMA pseudo Dice: 0.9404 
2024-05-09 00:04:05.099135: Training done. 
2024-05-09 00:04:05.805652: Using splits from existing split file: /raid/dataset/nnUNet_preprocessed/Dataset022_COPD_addedArt/splits_final.json 
2024-05-09 00:04:05.811605: The split file contains 5 splits. 
2024-05-09 00:04:05.811702: Desired fold for training: 0 
2024-05-09 00:04:05.811775: This split has 7033 training and 1759 validation cases. 
2024-05-09 00:04:06.005930: predicting 10057J_8_2_data 
2024-05-09 00:04:06.006885: 10057J_8_2_data, shape torch.Size([1, 43, 197, 264]), rank 0 
2024-05-09 00:04:08.231506: predicting 10077P_4_2_data 
2024-05-09 00:04:08.232887: 10077P_4_2_data, shape torch.Size([1, 42, 198, 265]), rank 0 
2024-05-09 00:04:09.186993: predicting 10083K_4_2_data 
2024-05-09 00:04:09.187871: 10083K_4_2_data, shape torch.Size([1, 43, 196, 266]), rank 0 
2024-05-09 00:04:10.140734: predicting 10085O_16_2_data 
2024-05-09 00:04:10.141624: 10085O_16_2_data, shape torch.Size([1, 37, 196, 265]), rank 0 
2024-05-09 00:04:11.093712: predicting 10086Q_16_2_data 
2024-05-09 00:04:11.094550: 10086Q_16_2_data, shape torch.Size([1, 42, 194, 264]), rank 0 
2024-05-09 00:04:12.047592: predicting 10088U_8_2_data 
2024-05-09 00:04:12.048453: 10088U_8_2_data, shape torch.Size([1, 43, 200, 264]), rank 0 
2024-05-09 00:04:13.002459: predicting 10090H_4_3_data 
2024-05-09 00:04:13.003490: 10090H_4_3_data, shape torch.Size([1, 43, 200, 265]), rank 0 
2024-05-09 00:04:13.957612: predicting 10098X_4_3_data 
2024-05-09 00:04:13.958715: 10098X_4_3_data, shape torch.Size([1, 43, 190, 261]), rank 0 
2024-05-09 00:04:14.442713: predicting 10114V_16_2_data 
2024-05-09 00:04:14.443764: 10114V_16_2_data, shape torch.Size([1, 44, 189, 260]), rank 0 
2024-05-09 00:04:14.927577: predicting 10116Z_4_2_data 
2024-05-09 00:04:14.928613: 10116Z_4_2_data, shape torch.Size([1, 42, 194, 265]), rank 0 
2024-05-09 00:04:15.882314: predicting 10120Q_16_2_data 
2024-05-09 00:04:15.883347: 10120Q_16_2_data, shape torch.Size([1, 37, 194, 265]), rank 0 
2024-05-09 00:04:16.835826: predicting 10132X_4_2_data 
2024-05-09 00:04:16.836846: 10132X_4_2_data, shape torch.Size([1, 42, 185, 262]), rank 0 
2024-05-09 00:04:17.321073: predicting 10138J_16_2_data 
2024-05-09 00:04:17.321936: 10138J_16_2_data, shape torch.Size([1, 43, 192, 265]), rank 0 
2024-05-09 00:04:17.806785: predicting 10153F_4_2_data 
2024-05-09 00:04:17.807842: 10153F_4_2_data, shape torch.Size([1, 43, 196, 269]), rank 0 
2024-05-09 00:04:18.762668: predicting 10182M_16_2_data 
2024-05-09 00:04:18.763720: 10182M_16_2_data, shape torch.Size([1, 44, 192, 266]), rank 0 
2024-05-09 00:04:19.246588: predicting 10186U_16_2_data 
2024-05-09 00:04:19.247641: 10186U_16_2_data, shape torch.Size([1, 32, 193, 262]), rank 0 
2024-05-09 00:04:20.199414: predicting 10188Y_0_0_data 
2024-05-09 00:04:20.200631: 10188Y_0_0_data, shape torch.Size([1, 32, 193, 262]), rank 0 
2024-05-09 00:04:21.152451: predicting 10196X_8_3_data 
2024-05-09 00:04:21.153349: 10196X_8_3_data, shape torch.Size([1, 33, 198, 265]), rank 0 
2024-05-09 00:04:22.106200: predicting 10197Z_4_3_data 
2024-05-09 00:04:22.107172: 10197Z_4_3_data, shape torch.Size([1, 43, 194, 262]), rank 0 
2024-05-09 00:04:23.060453: predicting 10207C_16_2_data 
2024-05-09 00:04:23.061347: 10207C_16_2_data, shape torch.Size([1, 44, 194, 263]), rank 0 
2024-05-09 00:04:24.015288: predicting 10212V_4_3_data 
2024-05-09 00:04:24.016330: 10212V_4_3_data, shape torch.Size([1, 43, 186, 264]), rank 0 
2024-05-09 00:04:24.499049: predicting 10224C_4_3_data 
2024-05-09 00:04:24.500097: 10224C_4_3_data, shape torch.Size([1, 43, 195, 266]), rank 0 
2024-05-09 00:04:25.454281: predicting 10233D_4_3_data 
2024-05-09 00:04:25.455211: 10233D_4_3_data, shape torch.Size([1, 42, 196, 274]), rank 0 
2024-05-09 00:04:26.408856: predicting 10237L_8_2_data 
2024-05-09 00:04:26.409792: 10237L_8_2_data, shape torch.Size([1, 43, 194, 264]), rank 0 
2024-05-09 00:04:27.363024: predicting 10242E_0_0_data 
2024-05-09 00:04:27.363874: 10242E_0_0_data, shape torch.Size([1, 43, 194, 264]), rank 0 
2024-05-09 00:04:28.316737: predicting 10252H_4_2_data 
2024-05-09 00:04:28.317600: 10252H_4_2_data, shape torch.Size([1, 42, 194, 271]), rank 0 
2024-05-09 00:04:29.270996: predicting 10253J_4_3_data 
2024-05-09 00:04:29.271876: 10253J_4_3_data, shape torch.Size([1, 43, 194, 278]), rank 0 
2024-05-09 00:04:30.225348: predicting 10255N_8_3_data 
2024-05-09 00:04:30.226192: 10255N_8_3_data, shape torch.Size([1, 26, 182, 259]), rank 0 
2024-05-09 00:04:30.707901: predicting 10256P_16_2_data 
2024-05-09 00:04:30.708697: 10256P_16_2_data, shape torch.Size([1, 42, 196, 263]), rank 0 
2024-05-09 00:04:31.662997: predicting 10263M_8_3_data 
2024-05-09 00:04:31.663839: 10263M_8_3_data, shape torch.Size([1, 40, 196, 270]), rank 0 
2024-05-09 00:04:32.617368: predicting 10264O_8_3_data 
2024-05-09 00:04:32.618440: 10264O_8_3_data, shape torch.Size([1, 42, 198, 257]), rank 0 
2024-05-09 00:04:33.571514: predicting 10269Y_0_0_data 
2024-05-09 00:04:33.572450: 10269Y_0_0_data, shape torch.Size([1, 42, 196, 269]), rank 0 
2024-05-09 00:04:34.524871: predicting 10293V_0_0_data 
2024-05-09 00:04:34.525800: 10293V_0_0_data, shape torch.Size([1, 41, 198, 266]), rank 0 
2024-05-09 00:04:35.478420: predicting 10302W_8_3_data 
2024-05-09 00:04:35.479286: 10302W_8_3_data, shape torch.Size([1, 43, 192, 263]), rank 0 
2024-05-09 00:04:35.963121: predicting 10312Z_4_2_data 
2024-05-09 00:04:35.964145: 10312Z_4_2_data, shape torch.Size([1, 42, 198, 275]), rank 0 
2024-05-09 00:04:36.918257: predicting 10313B_8_2_data 
2024-05-09 00:04:36.919653: 10313B_8_2_data, shape torch.Size([1, 43, 198, 277]), rank 0 
2024-05-09 00:04:37.873560: predicting 10321A_4_3_data 
2024-05-09 00:04:37.874707: 10321A_4_3_data, shape torch.Size([1, 42, 200, 268]), rank 0 
2024-05-09 00:04:38.828249: predicting 10334J_8_3_data 
2024-05-09 00:04:38.829154: 10334J_8_3_data, shape torch.Size([1, 43, 188, 261]), rank 0 
2024-05-09 00:04:39.311016: predicting 10374V_0_0_data 
2024-05-09 00:04:39.311961: 10374V_0_0_data, shape torch.Size([1, 42, 197, 266]), rank 0 
2024-05-09 00:04:40.266291: predicting 10380Q_4_2_data 
2024-05-09 00:04:40.267187: 10380Q_4_2_data, shape torch.Size([1, 44, 196, 262]), rank 0 
2024-05-09 00:04:41.220812: predicting 10413F_8_3_data 
2024-05-09 00:04:41.221697: 10413F_8_3_data, shape torch.Size([1, 40, 196, 269]), rank 0 
2024-05-09 00:04:42.175088: predicting 10419R_8_3_data 
2024-05-09 00:04:42.176010: 10419R_8_3_data, shape torch.Size([1, 42, 193, 266]), rank 0 
2024-05-09 00:04:43.129711: predicting 10434N_4_2_data 
2024-05-09 00:04:43.130583: 10434N_4_2_data, shape torch.Size([1, 43, 196, 272]), rank 0 
2024-05-09 00:04:44.084337: predicting 10437T_16_2_data 
2024-05-09 00:04:44.085330: 10437T_16_2_data, shape torch.Size([1, 42, 194, 265]), rank 0 
2024-05-09 00:04:45.037815: predicting 10460O_16_2_data 
2024-05-09 00:04:45.038725: 10460O_16_2_data, shape torch.Size([1, 43, 190, 261]), rank 0 
2024-05-09 00:04:45.521755: predicting 10465Y_0_0_data 
2024-05-09 00:04:45.522742: 10465Y_0_0_data, shape torch.Size([1, 42, 198, 273]), rank 0 
2024-05-09 00:04:46.477401: predicting 10469G_8_2_data 
2024-05-09 00:04:46.478361: 10469G_8_2_data, shape torch.Size([1, 41, 186, 261]), rank 0 
2024-05-09 00:04:46.961342: predicting 10473X_8_2_data 
2024-05-09 00:04:46.962582: 10473X_8_2_data, shape torch.Size([1, 43, 199, 265]), rank 0 
2024-05-09 00:04:47.915854: predicting 10479J_16_2_data 
2024-05-09 00:04:47.916907: 10479J_16_2_data, shape torch.Size([1, 44, 189, 256]), rank 0 
2024-05-09 00:04:48.164559: predicting 10482Y_8_2_data 
2024-05-09 00:04:48.165464: 10482Y_8_2_data, shape torch.Size([1, 44, 192, 266]), rank 0 
2024-05-09 00:04:48.649848: predicting 10488K_4_2_data 
2024-05-09 00:04:48.650767: 10488K_4_2_data, shape torch.Size([1, 41, 197, 269]), rank 0 
2024-05-09 00:04:49.604085: predicting 10504I_4_2_data 
2024-05-09 00:04:49.604949: 10504I_4_2_data, shape torch.Size([1, 43, 185, 262]), rank 0 
2024-05-09 00:04:50.088748: predicting 10511F_8_2_data 
2024-05-09 00:04:50.089739: 10511F_8_2_data, shape torch.Size([1, 42, 194, 263]), rank 0 
2024-05-09 00:04:51.042995: predicting 10516P_16_2_data 
2024-05-09 00:04:51.044042: 10516P_16_2_data, shape torch.Size([1, 42, 190, 259]), rank 0 
2024-05-09 00:04:51.526031: predicting 10534R_8_3_data 
2024-05-09 00:04:51.526898: 10534R_8_3_data, shape torch.Size([1, 35, 193, 264]), rank 0 
2024-05-09 00:04:52.479782: predicting 10555Z_0_0_data 
2024-05-09 00:04:52.480620: 10555Z_0_0_data, shape torch.Size([1, 40, 194, 265]), rank 0 
2024-05-09 00:04:53.433734: predicting 10556B_16_2_data 
2024-05-09 00:04:53.434679: 10556B_16_2_data, shape torch.Size([1, 43, 194, 262]), rank 0 
2024-05-09 00:04:54.388323: predicting 10562W_8_3_data 
2024-05-09 00:04:54.389393: 10562W_8_3_data, shape torch.Size([1, 42, 185, 258]), rank 0 
2024-05-09 00:04:54.873175: predicting 10572Z_16_2_data 
2024-05-09 00:04:54.874088: 10572Z_16_2_data, shape torch.Size([1, 42, 192, 274]), rank 0 
2024-05-09 00:04:55.359137: predicting 10591D_0_0_data 
2024-05-09 00:04:55.360008: 10591D_0_0_data, shape torch.Size([1, 43, 203, 271]), rank 0 
2024-05-09 00:04:56.314783: predicting 10599T_16_2_data 
2024-05-09 00:04:56.315684: 10599T_16_2_data, shape torch.Size([1, 43, 189, 254]), rank 0 
2024-05-09 00:04:56.562377: predicting 10608U_16_2_data 
2024-05-09 00:04:56.563535: 10608U_16_2_data, shape torch.Size([1, 36, 189, 255]), rank 0 
2024-05-09 00:04:56.811753: predicting 10625U_16_2_data 
2024-05-09 00:04:56.812639: 10625U_16_2_data, shape torch.Size([1, 43, 195, 263]), rank 0 
2024-05-09 00:04:57.766769: predicting 10645A_8_3_data 
2024-05-09 00:04:57.767643: 10645A_8_3_data, shape torch.Size([1, 39, 189, 262]), rank 0 
2024-05-09 00:04:58.251800: predicting 10646C_4_3_data 
2024-05-09 00:04:58.252901: 10646C_4_3_data, shape torch.Size([1, 43, 194, 266]), rank 0 
2024-05-09 00:04:59.206834: predicting 10664E_0_0_data 
2024-05-09 00:04:59.207754: 10664E_0_0_data, shape torch.Size([1, 39, 184, 259]), rank 0 
2024-05-09 00:04:59.691578: predicting 10697T_4_2_data 
2024-05-09 00:04:59.692434: 10697T_4_2_data, shape torch.Size([1, 44, 201, 271]), rank 0 
2024-05-09 00:05:00.646204: predicting 10710L_8_3_data 
2024-05-09 00:05:00.647420: 10710L_8_3_data, shape torch.Size([1, 43, 201, 269]), rank 0 
2024-05-09 00:05:01.600682: predicting 10726A_4_2_data 
2024-05-09 00:05:01.601626: 10726A_4_2_data, shape torch.Size([1, 43, 194, 266]), rank 0 
2024-05-09 00:05:02.555614: predicting 10730R_16_2_data 
2024-05-09 00:05:02.556501: 10730R_16_2_data, shape torch.Size([1, 36, 182, 257]), rank 0 
2024-05-09 00:05:03.038193: predicting 10739J_16_2_data 
2024-05-09 00:05:03.039041: 10739J_16_2_data, shape torch.Size([1, 43, 200, 275]), rank 0 
2024-05-09 00:05:03.993538: predicting 10771F_8_3_data 
2024-05-09 00:05:03.994665: 10771F_8_3_data, shape torch.Size([1, 44, 200, 276]), rank 0 
2024-05-09 00:05:04.948013: predicting 10796V_4_3_data 
2024-05-09 00:05:04.949455: 10796V_4_3_data, shape torch.Size([1, 43, 189, 262]), rank 0 
2024-05-09 00:05:05.433353: predicting 10806Y_4_3_data 
2024-05-09 00:05:05.434619: 10806Y_4_3_data, shape torch.Size([1, 42, 194, 263]), rank 0 
2024-05-09 00:05:06.388893: predicting 10817D_8_3_data 
2024-05-09 00:05:06.390249: 10817D_8_3_data, shape torch.Size([1, 43, 192, 262]), rank 0 
2024-05-09 00:05:06.874036: predicting 10829K_8_3_data 
2024-05-09 00:05:06.875139: 10829K_8_3_data, shape torch.Size([1, 44, 194, 275]), rank 0 
2024-05-09 00:05:07.829656: predicting 10830V_4_3_data 
2024-05-09 00:05:07.830618: 10830V_4_3_data, shape torch.Size([1, 43, 191, 265]), rank 0 
2024-05-09 00:05:08.313481: predicting 10846K_0_0_data 
2024-05-09 00:05:08.314349: 10846K_0_0_data, shape torch.Size([1, 43, 189, 263]), rank 0 
2024-05-09 00:05:08.798309: predicting 10849Q_16_2_data 
2024-05-09 00:05:08.799376: 10849Q_16_2_data, shape torch.Size([1, 43, 193, 267]), rank 0 
2024-05-09 00:05:09.755256: predicting 10851D_8_2_data 
2024-05-09 00:05:09.756375: 10851D_8_2_data, shape torch.Size([1, 41, 187, 260]), rank 0 
2024-05-09 00:05:10.239948: predicting 10887Y_8_2_data 
2024-05-09 00:05:10.241025: 10887Y_8_2_data, shape torch.Size([1, 43, 197, 274]), rank 0 
2024-05-09 00:05:11.195262: predicting 10888A_8_3_data 
2024-05-09 00:05:11.196332: 10888A_8_3_data, shape torch.Size([1, 43, 198, 270]), rank 0 
2024-05-09 00:05:12.152969: predicting 10940C_8_2_data 
2024-05-09 00:05:12.154330: 10940C_8_2_data, shape torch.Size([1, 43, 193, 264]), rank 0 
2024-05-09 00:05:13.108890: predicting 10950F_0_0_data 
2024-05-09 00:05:13.109975: 10950F_0_0_data, shape torch.Size([1, 38, 195, 268]), rank 0 
2024-05-09 00:05:14.062670: predicting 10981Q_16_2_data 
2024-05-09 00:05:14.063623: 10981Q_16_2_data, shape torch.Size([1, 44, 185, 259]), rank 0 
2024-05-09 00:05:14.546954: predicting 11002P_4_3_data 
2024-05-09 00:05:14.547903: 11002P_4_3_data, shape torch.Size([1, 43, 193, 262]), rank 0 
2024-05-09 00:05:15.501888: predicting 11018E_16_2_data 
2024-05-09 00:05:15.502778: 11018E_16_2_data, shape torch.Size([1, 43, 193, 260]), rank 0 
2024-05-09 00:05:16.457717: predicting 11020R_16_2_data 
2024-05-09 00:05:16.458597: 11020R_16_2_data, shape torch.Size([1, 42, 190, 259]), rank 0 
2024-05-09 00:05:16.941932: predicting 11038K_8_3_data 
2024-05-09 00:05:16.942947: 11038K_8_3_data, shape torch.Size([1, 42, 189, 265]), rank 0 
2024-05-09 00:05:17.428254: predicting 11047L_8_2_data 
2024-05-09 00:05:17.429198: 11047L_8_2_data, shape torch.Size([1, 43, 201, 277]), rank 0 
2024-05-09 00:05:18.382897: predicting 11065N_8_2_data 
2024-05-09 00:05:18.384161: 11065N_8_2_data, shape torch.Size([1, 43, 188, 260]), rank 0 
2024-05-09 00:05:18.867174: predicting 11067R_16_2_data 
2024-05-09 00:05:18.868117: 11067R_16_2_data, shape torch.Size([1, 42, 200, 267]), rank 0 
2024-05-09 00:05:19.822482: predicting 11084R_8_3_data 
2024-05-09 00:05:19.823764: 11084R_8_3_data, shape torch.Size([1, 43, 195, 266]), rank 0 
2024-05-09 00:05:20.778335: predicting 11103V_8_2_data 
2024-05-09 00:05:20.779265: 11103V_8_2_data, shape torch.Size([1, 42, 193, 263]), rank 0 
2024-05-09 00:05:21.733308: predicting 11104X_4_2_data 
2024-05-09 00:05:21.734591: 11104X_4_2_data, shape torch.Size([1, 42, 185, 261]), rank 0 
2024-05-09 00:05:22.217885: predicting 11125F_8_2_data 
2024-05-09 00:05:22.218777: 11125F_8_2_data, shape torch.Size([1, 43, 201, 266]), rank 0 
2024-05-09 00:05:23.172920: predicting 11137M_8_2_data 
2024-05-09 00:05:23.173906: 11137M_8_2_data, shape torch.Size([1, 44, 190, 260]), rank 0 
2024-05-09 00:05:23.657348: predicting 11139Q_8_2_data 
2024-05-09 00:05:23.658242: 11139Q_8_2_data, shape torch.Size([1, 42, 195, 265]), rank 0 
2024-05-09 00:05:24.612479: predicting 11143H_4_2_data 
2024-05-09 00:05:24.613712: 11143H_4_2_data, shape torch.Size([1, 32, 197, 261]), rank 0 
2024-05-09 00:05:25.565804: predicting 11157S_8_2_data 
2024-05-09 00:05:25.566648: 11157S_8_2_data, shape torch.Size([1, 44, 200, 283]), rank 0 
2024-05-09 00:05:26.521271: predicting 11159W_8_3_data 
2024-05-09 00:05:26.522394: 11159W_8_3_data, shape torch.Size([1, 43, 198, 265]), rank 0 
2024-05-09 00:05:27.476177: predicting 11160H_4_2_data 
2024-05-09 00:05:27.477431: 11160H_4_2_data, shape torch.Size([1, 42, 196, 266]), rank 0 
2024-05-09 00:05:28.431163: predicting 11176W_16_2_data 
2024-05-09 00:05:28.432139: 11176W_16_2_data, shape torch.Size([1, 32, 193, 269]), rank 0 
2024-05-09 00:05:29.383911: predicting 11184V_4_3_data 
2024-05-09 00:05:29.384757: 11184V_4_3_data, shape torch.Size([1, 43, 191, 263]), rank 0 
2024-05-09 00:05:29.868442: predicting 11185X_4_3_data 
2024-05-09 00:05:29.869414: 11185X_4_3_data, shape torch.Size([1, 42, 179, 251]), rank 0 
2024-05-09 00:05:30.117065: predicting 11186Z_16_2_data 
2024-05-09 00:05:30.117995: 11186Z_16_2_data, shape torch.Size([1, 42, 177, 255]), rank 0 
2024-05-09 00:05:30.366482: predicting 11191S_16_2_data 
2024-05-09 00:05:30.367477: 11191S_16_2_data, shape torch.Size([1, 43, 195, 257]), rank 0 
2024-05-09 00:05:31.321743: predicting 11196C_0_0_data 
2024-05-09 00:05:31.322800: 11196C_0_0_data, shape torch.Size([1, 34, 194, 265]), rank 0 
2024-05-09 00:05:32.275886: predicting 11198G_16_2_data 
2024-05-09 00:05:32.276784: 11198G_16_2_data, shape torch.Size([1, 43, 181, 254]), rank 0 
2024-05-09 00:05:32.524617: predicting 11201V_8_2_data 
2024-05-09 00:05:32.525583: 11201V_8_2_data, shape torch.Size([1, 43, 202, 273]), rank 0 
2024-05-09 00:05:33.480334: predicting 11206F_4_2_data 
2024-05-09 00:05:33.481586: 11206F_4_2_data, shape torch.Size([1, 42, 200, 269]), rank 0 
2024-05-09 00:05:34.435811: predicting 11213C_8_2_data 
2024-05-09 00:05:34.436788: 11213C_8_2_data, shape torch.Size([1, 44, 189, 264]), rank 0 
2024-05-09 00:05:34.919405: predicting 11220Z_16_2_data 
2024-05-09 00:05:34.920298: 11220Z_16_2_data, shape torch.Size([1, 43, 193, 266]), rank 0 
2024-05-09 00:05:35.874715: predicting 11221B_16_2_data 
2024-05-09 00:05:35.875831: 11221B_16_2_data, shape torch.Size([1, 42, 193, 262]), rank 0 
2024-05-09 00:05:36.829134: predicting 11240F_0_0_data 
2024-05-09 00:05:36.830024: 11240F_0_0_data, shape torch.Size([1, 44, 198, 266]), rank 0 
2024-05-09 00:05:37.782788: predicting 11268B_4_2_data 
2024-05-09 00:05:37.783871: 11268B_4_2_data, shape torch.Size([1, 43, 191, 264]), rank 0 
2024-05-09 00:05:38.268078: predicting 11269D_0_0_data 
2024-05-09 00:05:38.269145: 11269D_0_0_data, shape torch.Size([1, 43, 191, 264]), rank 0 
2024-05-09 00:05:38.754405: predicting 11272S_8_3_data 
2024-05-09 00:05:38.755483: 11272S_8_3_data, shape torch.Size([1, 42, 188, 259]), rank 0 
2024-05-09 00:05:39.237982: predicting 11286D_16_2_data 
2024-05-09 00:05:39.239009: 11286D_16_2_data, shape torch.Size([1, 42, 197, 266]), rank 0 
2024-05-09 00:05:40.195752: predicting 11288H_8_3_data 
2024-05-09 00:05:40.196705: 11288H_8_3_data, shape torch.Size([1, 34, 188, 260]), rank 0 
2024-05-09 00:05:40.679741: predicting 11300X_8_3_data 
2024-05-09 00:05:40.680607: 11300X_8_3_data, shape torch.Size([1, 42, 196, 265]), rank 0 
2024-05-09 00:05:41.635663: predicting 11307L_0_0_data 
2024-05-09 00:05:41.636921: 11307L_0_0_data, shape torch.Size([1, 44, 194, 264]), rank 0 
2024-05-09 00:05:42.590240: predicting 11350M_4_3_data 
2024-05-09 00:05:42.591240: 11350M_4_3_data, shape torch.Size([1, 43, 192, 261]), rank 0 
2024-05-09 00:05:43.073815: predicting 11360P_8_2_data 
2024-05-09 00:05:43.074975: 11360P_8_2_data, shape torch.Size([1, 43, 194, 264]), rank 0 
2024-05-09 00:05:44.029682: predicting 11365Z_16_2_data 
2024-05-09 00:05:44.030667: 11365Z_16_2_data, shape torch.Size([1, 40, 192, 263]), rank 0 
2024-05-09 00:05:44.513777: predicting 11373Y_8_2_data 
2024-05-09 00:05:44.514808: 11373Y_8_2_data, shape torch.Size([1, 42, 182, 259]), rank 0 
2024-05-09 00:05:44.998185: predicting 11388L_4_2_data 
2024-05-09 00:05:44.999087: 11388L_4_2_data, shape torch.Size([1, 42, 177, 252]), rank 0 
2024-05-09 00:05:45.246828: predicting 11390Y_0_0_data 
2024-05-09 00:05:45.247672: 11390Y_0_0_data, shape torch.Size([1, 43, 191, 272]), rank 0 
2024-05-09 00:05:45.732687: predicting 11394G_16_2_data 
2024-05-09 00:05:45.733514: 11394G_16_2_data, shape torch.Size([1, 42, 192, 264]), rank 0 
2024-05-09 00:05:46.216769: predicting 11400B_4_3_data 
2024-05-09 00:05:46.217841: 11400B_4_3_data, shape torch.Size([1, 42, 198, 284]), rank 0 
2024-05-09 00:05:47.172364: predicting 11420H_4_2_data 
2024-05-09 00:05:47.173450: 11420H_4_2_data, shape torch.Size([1, 44, 183, 255]), rank 0 
2024-05-09 00:05:47.420622: predicting 11425R_4_3_data 
2024-05-09 00:05:47.421496: 11425R_4_3_data, shape torch.Size([1, 42, 195, 265]), rank 0 
2024-05-09 00:05:48.375730: predicting 11431M_8_2_data 
2024-05-09 00:05:48.376805: 11431M_8_2_data, shape torch.Size([1, 44, 190, 260]), rank 0 
2024-05-09 00:05:48.859814: predicting 11444V_8_2_data 
2024-05-09 00:05:48.861069: 11444V_8_2_data, shape torch.Size([1, 41, 192, 265]), rank 0 
2024-05-09 00:05:49.345015: predicting 11455A_4_3_data 
2024-05-09 00:05:49.346242: 11455A_4_3_data, shape torch.Size([1, 44, 198, 265]), rank 0 
2024-05-09 00:05:50.300700: predicting 11468J_4_2_data 
2024-05-09 00:05:50.301775: 11468J_4_2_data, shape torch.Size([1, 43, 198, 270]), rank 0 
2024-05-09 00:05:51.256378: predicting 11489R_8_2_data 
2024-05-09 00:05:51.257385: 11489R_8_2_data, shape torch.Size([1, 37, 198, 262]), rank 0 
2024-05-09 00:05:52.210198: predicting 11497Q_4_2_data 
2024-05-09 00:05:52.211055: 11497Q_4_2_data, shape torch.Size([1, 42, 188, 261]), rank 0 
2024-05-09 00:05:52.695182: predicting 11499U_0_0_data 
2024-05-09 00:05:52.696128: 11499U_0_0_data, shape torch.Size([1, 42, 188, 261]), rank 0 
2024-05-09 00:05:53.180793: predicting 11518Y_16_2_data 
2024-05-09 00:05:53.181869: 11518Y_16_2_data, shape torch.Size([1, 43, 202, 281]), rank 0 
2024-05-09 00:05:54.137504: predicting 11521N_0_0_data 
2024-05-09 00:05:54.138730: 11521N_0_0_data, shape torch.Size([1, 43, 195, 280]), rank 0 
2024-05-09 00:05:55.094084: predicting 11526X_16_2_data 
2024-05-09 00:05:55.095193: 11526X_16_2_data, shape torch.Size([1, 44, 184, 262]), rank 0 
2024-05-09 00:05:55.577479: predicting 11559M_0_0_data 
2024-05-09 00:05:55.578524: 11559M_0_0_data, shape torch.Size([1, 44, 188, 263]), rank 0 
2024-05-09 00:05:56.062463: predicting 11566J_4_3_data 
2024-05-09 00:05:56.063406: 11566J_4_3_data, shape torch.Size([1, 42, 185, 259]), rank 0 
2024-05-09 00:05:56.547546: predicting 11570A_8_3_data 
2024-05-09 00:05:56.548400: 11570A_8_3_data, shape torch.Size([1, 36, 196, 279]), rank 0 
2024-05-09 00:05:57.501801: predicting 11600J_0_0_data 
2024-05-09 00:05:57.502621: 11600J_0_0_data, shape torch.Size([1, 43, 197, 263]), rank 0 
2024-05-09 00:05:58.456176: predicting 11613S_8_2_data 
2024-05-09 00:05:58.457224: 11613S_8_2_data, shape torch.Size([1, 43, 197, 278]), rank 0 
2024-05-09 00:05:59.410424: predicting 11615W_0_0_data 
2024-05-09 00:05:59.411314: 11615W_0_0_data, shape torch.Size([1, 42, 198, 273]), rank 0 
2024-05-09 00:06:00.364473: predicting 11622T_8_2_data 
2024-05-09 00:06:00.365417: 11622T_8_2_data, shape torch.Size([1, 43, 187, 262]), rank 0 
2024-05-09 00:06:00.848396: predicting 11628F_8_3_data 
2024-05-09 00:06:00.849449: 11628F_8_3_data, shape torch.Size([1, 42, 186, 261]), rank 0 
2024-05-09 00:06:01.333298: predicting 11646H_16_2_data 
2024-05-09 00:06:01.334314: 11646H_16_2_data, shape torch.Size([1, 43, 188, 263]), rank 0 
2024-05-09 00:06:01.818324: predicting 11650Y_0_0_data 
2024-05-09 00:06:01.819368: 11650Y_0_0_data, shape torch.Size([1, 43, 196, 264]), rank 0 
2024-05-09 00:06:02.773043: predicting 11651A_0_0_data 
2024-05-09 00:06:02.774288: 11651A_0_0_data, shape torch.Size([1, 43, 196, 264]), rank 0 
2024-05-09 00:06:03.728863: predicting 11677S_0_0_data 
2024-05-09 00:06:03.730137: 11677S_0_0_data, shape torch.Size([1, 43, 196, 263]), rank 0 
2024-05-09 00:06:04.683831: predicting 11696W_8_3_data 
2024-05-09 00:06:04.684681: 11696W_8_3_data, shape torch.Size([1, 40, 188, 265]), rank 0 
2024-05-09 00:06:05.168034: predicting 11697Y_4_3_data 
2024-05-09 00:06:05.168859: 11697Y_4_3_data, shape torch.Size([1, 39, 195, 277]), rank 0 
2024-05-09 00:06:06.122265: predicting 11708D_16_2_data 
2024-05-09 00:06:06.123263: 11708D_16_2_data, shape torch.Size([1, 42, 193, 261]), rank 0 
2024-05-09 00:06:07.076077: predicting 11724B_4_2_data 
2024-05-09 00:06:07.077217: 11724B_4_2_data, shape torch.Size([1, 43, 197, 268]), rank 0 
2024-05-09 00:06:08.030107: predicting 11730W_4_2_data 
2024-05-09 00:06:08.031526: 11730W_4_2_data, shape torch.Size([1, 42, 186, 259]), rank 0 
2024-05-09 00:06:08.513433: predicting 11742D_16_2_data 
2024-05-09 00:06:08.514372: 11742D_16_2_data, shape torch.Size([1, 42, 200, 268]), rank 0 
2024-05-09 00:06:09.468448: predicting 11743F_16_2_data 
2024-05-09 00:06:09.469616: 11743F_16_2_data, shape torch.Size([1, 43, 188, 257]), rank 0 
2024-05-09 00:06:09.951926: predicting 11746L_16_2_data 
2024-05-09 00:06:09.952793: 11746L_16_2_data, shape torch.Size([1, 43, 200, 268]), rank 0 
2024-05-09 00:06:10.906659: predicting 11757Q_4_2_data 
2024-05-09 00:06:10.907547: 11757Q_4_2_data, shape torch.Size([1, 43, 193, 265]), rank 0 
2024-05-09 00:06:11.861709: predicting 11767T_16_2_data 
2024-05-09 00:06:11.862658: 11767T_16_2_data, shape torch.Size([1, 37, 196, 264]), rank 0 
2024-05-09 00:06:12.814612: predicting 11769X_4_3_data 
2024-05-09 00:06:12.815779: 11769X_4_3_data, shape torch.Size([1, 42, 189, 272]), rank 0 
2024-05-09 00:06:13.299092: predicting 11774Q_4_3_data 
2024-05-09 00:06:13.299979: 11774Q_4_3_data, shape torch.Size([1, 43, 190, 266]), rank 0 
2024-05-09 00:06:13.784833: predicting 11776U_16_2_data 
2024-05-09 00:06:13.785706: 11776U_16_2_data, shape torch.Size([1, 41, 196, 266]), rank 0 
2024-05-09 00:06:14.739463: predicting 11781N_4_3_data 
2024-05-09 00:06:14.740319: 11781N_4_3_data, shape torch.Size([1, 43, 195, 264]), rank 0 
2024-05-09 00:06:15.694216: predicting 11783R_16_2_data 
2024-05-09 00:06:15.695249: 11783R_16_2_data, shape torch.Size([1, 42, 194, 265]), rank 0 
2024-05-09 00:06:16.649151: predicting 11803X_16_2_data 
2024-05-09 00:06:16.650185: 11803X_16_2_data, shape torch.Size([1, 40, 194, 263]), rank 0 
2024-05-09 00:06:17.602920: predicting 11805B_16_2_data 
2024-05-09 00:06:17.603792: 11805B_16_2_data, shape torch.Size([1, 40, 194, 261]), rank 0 
2024-05-09 00:06:18.556230: predicting 11809J_16_2_data 
2024-05-09 00:06:18.557091: 11809J_16_2_data, shape torch.Size([1, 40, 187, 262]), rank 0 
2024-05-09 00:06:19.039671: predicting 11823D_0_0_data 
2024-05-09 00:06:19.040698: 11823D_0_0_data, shape torch.Size([1, 41, 195, 262]), rank 0 
2024-05-09 00:06:19.993785: predicting 11848T_0_0_data 
2024-05-09 00:06:19.994756: 11848T_0_0_data, shape torch.Size([1, 43, 198, 266]), rank 0 
2024-05-09 00:06:20.948395: predicting 11852K_4_2_data 
2024-05-09 00:06:20.949311: 11852K_4_2_data, shape torch.Size([1, 44, 198, 271]), rank 0 
2024-05-09 00:06:21.903275: predicting 11868Z_0_0_data 
2024-05-09 00:06:21.904233: 11868Z_0_0_data, shape torch.Size([1, 43, 195, 263]), rank 0 
2024-05-09 00:06:22.857827: predicting 11875W_8_2_data 
2024-05-09 00:06:22.858940: 11875W_8_2_data, shape torch.Size([1, 44, 198, 273]), rank 0 
2024-05-09 00:06:23.812701: predicting 11882T_8_2_data 
2024-05-09 00:06:23.813767: 11882T_8_2_data, shape torch.Size([1, 39, 186, 259]), rank 0 
2024-05-09 00:06:24.296233: predicting 11886B_16_2_data 
2024-05-09 00:06:24.297173: 11886B_16_2_data, shape torch.Size([1, 43, 182, 253]), rank 0 
2024-05-09 00:06:24.545378: predicting 11906H_16_2_data 
2024-05-09 00:06:24.546314: 11906H_16_2_data, shape torch.Size([1, 43, 197, 265]), rank 0 
2024-05-09 00:06:25.500511: predicting 11908L_4_3_data 
2024-05-09 00:06:25.501453: 11908L_4_3_data, shape torch.Size([1, 43, 196, 265]), rank 0 
2024-05-09 00:06:26.455060: predicting 11914G_0_0_data 
2024-05-09 00:06:26.456046: 11914G_0_0_data, shape torch.Size([1, 43, 192, 264]), rank 0 
2024-05-09 00:06:26.938914: predicting 11917M_4_3_data 
2024-05-09 00:06:26.939825: 11917M_4_3_data, shape torch.Size([1, 43, 198, 264]), rank 0 
2024-05-09 00:06:27.893466: predicting 11937S_0_0_data 
2024-05-09 00:06:27.894634: 11937S_0_0_data, shape torch.Size([1, 40, 193, 266]), rank 0 
2024-05-09 00:06:28.848351: predicting 11940H_8_2_data 
2024-05-09 00:06:28.849328: 11940H_8_2_data, shape torch.Size([1, 42, 191, 264]), rank 0 
2024-05-09 00:06:29.337477: predicting 11946T_16_2_data 
2024-05-09 00:06:29.338505: 11946T_16_2_data, shape torch.Size([1, 37, 180, 256]), rank 0 
2024-05-09 00:06:29.586726: predicting 11955U_16_2_data 
2024-05-09 00:06:29.587627: 11955U_16_2_data, shape torch.Size([1, 43, 192, 259]), rank 0 
2024-05-09 00:06:30.071665: predicting 11961P_16_2_data 
2024-05-09 00:06:30.072562: 11961P_16_2_data, shape torch.Size([1, 42, 198, 272]), rank 0 
2024-05-09 00:06:31.026295: predicting 11967B_8_3_data 
2024-05-09 00:06:31.027205: 11967B_8_3_data, shape torch.Size([1, 43, 197, 275]), rank 0 
2024-05-09 00:06:31.981338: predicting 11969F_16_2_data 
2024-05-09 00:06:31.982223: 11969F_16_2_data, shape torch.Size([1, 44, 195, 267]), rank 0 
2024-05-09 00:06:32.935328: predicting 11975A_4_3_data 
2024-05-09 00:06:32.936179: 11975A_4_3_data, shape torch.Size([1, 43, 187, 261]), rank 0 
2024-05-09 00:06:33.419098: predicting 11981V_0_0_data 
2024-05-09 00:06:33.420136: 11981V_0_0_data, shape torch.Size([1, 43, 187, 261]), rank 0 
2024-05-09 00:06:33.904254: predicting 12013Z_0_0_data 
2024-05-09 00:06:33.905309: 12013Z_0_0_data, shape torch.Size([1, 44, 179, 248]), rank 0 
2024-05-09 00:06:34.153600: predicting 12014B_0_0_data 
2024-05-09 00:06:34.154492: 12014B_0_0_data, shape torch.Size([1, 44, 179, 248]), rank 0 
2024-05-09 00:06:34.403212: predicting 12021Y_16_2_data 
2024-05-09 00:06:34.404025: 12021Y_16_2_data, shape torch.Size([1, 41, 197, 266]), rank 0 
2024-05-09 00:06:35.358510: predicting 12022A_0_0_data 
2024-05-09 00:06:35.359351: 12022A_0_0_data, shape torch.Size([1, 41, 197, 266]), rank 0 
2024-05-09 00:06:36.313169: predicting 12025G_4_3_data 
2024-05-09 00:06:36.314066: 12025G_4_3_data, shape torch.Size([1, 39, 187, 264]), rank 0 
2024-05-09 00:06:36.796631: predicting 12027K_8_3_data 
2024-05-09 00:06:36.797500: 12027K_8_3_data, shape torch.Size([1, 44, 198, 280]), rank 0 
2024-05-09 00:06:37.752205: predicting 12039R_8_3_data 
2024-05-09 00:06:37.753131: 12039R_8_3_data, shape torch.Size([1, 43, 197, 268]), rank 0 
2024-05-09 00:06:38.706849: predicting 12053L_4_3_data 
2024-05-09 00:06:38.707822: 12053L_4_3_data, shape torch.Size([1, 43, 197, 270]), rank 0 
2024-05-09 00:06:39.661252: predicting 12085Y_0_0_data 
2024-05-09 00:06:39.662110: 12085Y_0_0_data, shape torch.Size([1, 42, 199, 265]), rank 0 
2024-05-09 00:06:40.615301: predicting 12093X_4_2_data 
2024-05-09 00:06:40.616165: 12093X_4_2_data, shape torch.Size([1, 43, 198, 266]), rank 0 
2024-05-09 00:06:41.570146: predicting 12101W_8_3_data 
2024-05-09 00:06:41.571208: 12101W_8_3_data, shape torch.Size([1, 42, 194, 260]), rank 0 
2024-05-09 00:06:42.524856: predicting 12105E_4_2_data 
2024-05-09 00:06:42.526093: 12105E_4_2_data, shape torch.Size([1, 43, 192, 264]), rank 0 
2024-05-09 00:06:43.009537: predicting 12110X_0_0_data 
2024-05-09 00:06:43.010442: 12110X_0_0_data, shape torch.Size([1, 44, 196, 265]), rank 0 
2024-05-09 00:06:43.964595: predicting 12112B_4_3_data 
2024-05-09 00:06:43.965496: 12112B_4_3_data, shape torch.Size([1, 42, 196, 266]), rank 0 
2024-05-09 00:06:44.919058: predicting 12123G_8_2_data 
2024-05-09 00:06:44.919937: 12123G_8_2_data, shape torch.Size([1, 42, 194, 262]), rank 0 
2024-05-09 00:06:45.873335: predicting 12125K_16_2_data 
2024-05-09 00:06:45.874274: 12125K_16_2_data, shape torch.Size([1, 41, 200, 254]), rank 0 
2024-05-09 00:06:46.356797: predicting 12139V_16_2_data 
2024-05-09 00:06:46.357698: 12139V_16_2_data, shape torch.Size([1, 43, 201, 267]), rank 0 
2024-05-09 00:06:47.311117: predicting 12145Q_16_2_data 
2024-05-09 00:06:47.312246: 12145Q_16_2_data, shape torch.Size([1, 44, 186, 257]), rank 0 
2024-05-09 00:06:47.795177: predicting 12168C_0_0_data 
2024-05-09 00:06:47.796217: 12168C_0_0_data, shape torch.Size([1, 43, 196, 265]), rank 0 
2024-05-09 00:06:48.750326: predicting 12170P_8_3_data 
2024-05-09 00:06:48.751359: 12170P_8_3_data, shape torch.Size([1, 29, 186, 260]), rank 0 
2024-05-09 00:06:49.232902: predicting 12181U_16_2_data 
2024-05-09 00:06:49.233702: 12181U_16_2_data, shape torch.Size([1, 40, 194, 266]), rank 0 
2024-05-09 00:06:50.186948: predicting 12192Z_0_0_data 
2024-05-09 00:06:50.188290: 12192Z_0_0_data, shape torch.Size([1, 38, 195, 272]), rank 0 
2024-05-09 00:06:51.143090: predicting 12231J_4_2_data 
2024-05-09 00:06:51.143995: 12231J_4_2_data, shape torch.Size([1, 43, 202, 272]), rank 0 
2024-05-09 00:06:52.098582: predicting 12243Q_8_2_data 
2024-05-09 00:06:52.099783: 12243Q_8_2_data, shape torch.Size([1, 42, 193, 266]), rank 0 
2024-05-09 00:06:53.053310: predicting 12258D_8_3_data 
2024-05-09 00:06:53.054384: 12258D_8_3_data, shape torch.Size([1, 43, 197, 262]), rank 0 
2024-05-09 00:06:54.007563: predicting 12273Z_4_2_data 
2024-05-09 00:06:54.008415: 12273Z_4_2_data, shape torch.Size([1, 42, 195, 262]), rank 0 
2024-05-09 00:06:54.962597: predicting 12281Y_4_2_data 
2024-05-09 00:06:54.963656: 12281Y_4_2_data, shape torch.Size([1, 43, 193, 262]), rank 0 
2024-05-09 00:06:55.916511: predicting 12296L_4_2_data 
2024-05-09 00:06:55.917581: 12296L_4_2_data, shape torch.Size([1, 43, 188, 259]), rank 0 
2024-05-09 00:06:56.401442: predicting 12300C_8_2_data 
2024-05-09 00:06:56.402330: 12300C_8_2_data, shape torch.Size([1, 37, 190, 262]), rank 0 
2024-05-09 00:06:56.885194: predicting 12329A_8_2_data 
2024-05-09 00:06:56.886048: 12329A_8_2_data, shape torch.Size([1, 42, 187, 258]), rank 0 
2024-05-09 00:06:57.373222: predicting 12339D_0_0_data 
2024-05-09 00:06:57.374249: 12339D_0_0_data, shape torch.Size([1, 43, 194, 262]), rank 0 
2024-05-09 00:06:58.327787: predicting 12343U_4_3_data 
2024-05-09 00:06:58.328862: 12343U_4_3_data, shape torch.Size([1, 41, 190, 262]), rank 0 
2024-05-09 00:06:58.812883: predicting 12347C_16_2_data 
2024-05-09 00:06:58.813784: 12347C_16_2_data, shape torch.Size([1, 35, 197, 262]), rank 0 
2024-05-09 00:06:59.765754: predicting 12354Z_8_3_data 
2024-05-09 00:06:59.767008: 12354Z_8_3_data, shape torch.Size([1, 42, 194, 265]), rank 0 
2024-05-09 00:07:00.720413: predicting 12380A_0_0_data 
2024-05-09 00:07:00.721585: 12380A_0_0_data, shape torch.Size([1, 43, 197, 264]), rank 0 
2024-05-09 00:07:01.674373: predicting 12388Q_8_3_data 
2024-05-09 00:07:01.675256: 12388Q_8_3_data, shape torch.Size([1, 44, 197, 271]), rank 0 
2024-05-09 00:07:02.628826: predicting 12390D_8_3_data 
2024-05-09 00:07:02.630223: 12390D_8_3_data, shape torch.Size([1, 42, 198, 264]), rank 0 
2024-05-09 00:07:03.583811: predicting 12419B_4_3_data 
2024-05-09 00:07:03.584794: 12419B_4_3_data, shape torch.Size([1, 44, 188, 265]), rank 0 
2024-05-09 00:07:04.067121: predicting 12422Q_16_2_data 
2024-05-09 00:07:04.068379: 12422Q_16_2_data, shape torch.Size([1, 38, 195, 264]), rank 0 
2024-05-09 00:07:05.021071: predicting 12427A_8_2_data 
2024-05-09 00:07:05.022089: 12427A_8_2_data, shape torch.Size([1, 42, 199, 265]), rank 0 
2024-05-09 00:07:05.974919: predicting 12456H_4_3_data 
2024-05-09 00:07:05.976185: 12456H_4_3_data, shape torch.Size([1, 42, 190, 262]), rank 0 
2024-05-09 00:07:06.460226: predicting 12457J_8_3_data 
2024-05-09 00:07:06.461292: 12457J_8_3_data, shape torch.Size([1, 42, 191, 260]), rank 0 
2024-05-09 00:07:06.944773: predicting 12474J_8_2_data 
2024-05-09 00:07:06.945922: 12474J_8_2_data, shape torch.Size([1, 43, 194, 265]), rank 0 
2024-05-09 00:07:07.898962: predicting 12478R_0_0_data 
2024-05-09 00:07:07.900154: 12478R_0_0_data, shape torch.Size([1, 32, 194, 261]), rank 0 
2024-05-09 00:07:08.851441: predicting 12489W_16_2_data 
2024-05-09 00:07:08.852350: 12489W_16_2_data, shape torch.Size([1, 42, 194, 261]), rank 0 
2024-05-09 00:07:09.806216: predicting 12493N_0_0_data 
2024-05-09 00:07:09.807308: 12493N_0_0_data, shape torch.Size([1, 36, 186, 261]), rank 0 
2024-05-09 00:07:10.290345: predicting 12496T_4_3_data 
2024-05-09 00:07:10.291324: 12496T_4_3_data, shape torch.Size([1, 42, 197, 264]), rank 0 
2024-05-09 00:07:11.245326: predicting 12504S_0_0_data 
2024-05-09 00:07:11.246282: 12504S_0_0_data, shape torch.Size([1, 42, 197, 264]), rank 0 
2024-05-09 00:07:12.200194: predicting 12516Z_0_0_data 
2024-05-09 00:07:12.201453: 12516Z_0_0_data, shape torch.Size([1, 41, 198, 265]), rank 0 
2024-05-09 00:07:13.152953: predicting 12525A_0_0_data 
2024-05-09 00:07:13.153869: 12525A_0_0_data, shape torch.Size([1, 42, 195, 263]), rank 0 
2024-05-09 00:07:14.107230: predicting 12560C_0_0_data 
2024-05-09 00:07:14.108121: 12560C_0_0_data, shape torch.Size([1, 44, 192, 266]), rank 0 
2024-05-09 00:07:14.591638: predicting 12565M_0_0_data 
2024-05-09 00:07:14.592771: 12565M_0_0_data, shape torch.Size([1, 44, 188, 262]), rank 0 
2024-05-09 00:07:15.076880: predicting 12574N_0_0_data 
2024-05-09 00:07:15.077985: 12574N_0_0_data, shape torch.Size([1, 38, 193, 267]), rank 0 
2024-05-09 00:07:16.030610: predicting 12606A_0_0_data 
2024-05-09 00:07:16.031512: 12606A_0_0_data, shape torch.Size([1, 43, 198, 271]), rank 0 
2024-05-09 00:07:16.985235: predicting 12620U_16_2_data 
2024-05-09 00:07:16.986286: 12620U_16_2_data, shape torch.Size([1, 43, 198, 267]), rank 0 
2024-05-09 00:07:17.939820: predicting 12628K_0_0_data 
2024-05-09 00:07:17.940811: 12628K_0_0_data, shape torch.Size([1, 44, 196, 265]), rank 0 
2024-05-09 00:07:18.893986: predicting 12629M_0_0_data 
2024-05-09 00:07:18.895247: 12629M_0_0_data, shape torch.Size([1, 44, 196, 265]), rank 0 
2024-05-09 00:07:19.848776: predicting 12638N_4_3_data 
2024-05-09 00:07:19.849769: 12638N_4_3_data, shape torch.Size([1, 44, 186, 260]), rank 0 
2024-05-09 00:07:20.332251: predicting 12641C_8_3_data 
2024-05-09 00:07:20.333126: 12641C_8_3_data, shape torch.Size([1, 42, 190, 264]), rank 0 
2024-05-09 00:07:20.818445: predicting 12654L_4_3_data 
2024-05-09 00:07:20.819304: 12654L_4_3_data, shape torch.Size([1, 40, 198, 269]), rank 0 
2024-05-09 00:07:21.772283: predicting 12670J_0_0_data 
2024-05-09 00:07:21.773490: 12670J_0_0_data, shape torch.Size([1, 42, 195, 264]), rank 0 
2024-05-09 00:07:22.726416: predicting 12690P_8_3_data 
2024-05-09 00:07:22.727371: 12690P_8_3_data, shape torch.Size([1, 41, 188, 260]), rank 0 
2024-05-09 00:07:23.210970: predicting 12741G_0_0_data 
2024-05-09 00:07:23.211887: 12741G_0_0_data, shape torch.Size([1, 42, 195, 265]), rank 0 
2024-05-09 00:07:24.165436: predicting 12746Q_4_3_data 
2024-05-09 00:07:24.166367: 12746Q_4_3_data, shape torch.Size([1, 42, 190, 259]), rank 0 
2024-05-09 00:07:24.649074: predicting 12763Q_4_2_data 
2024-05-09 00:07:24.650005: 12763Q_4_2_data, shape torch.Size([1, 44, 197, 266]), rank 0 
2024-05-09 00:07:25.604890: predicting 12771P_8_3_data 
2024-05-09 00:07:25.605763: 12771P_8_3_data, shape torch.Size([1, 40, 193, 265]), rank 0 
2024-05-09 00:07:26.558608: predicting 12795D_0_0_data 
2024-05-09 00:07:26.559509: 12795D_0_0_data, shape torch.Size([1, 38, 192, 262]), rank 0 
2024-05-09 00:07:27.042377: predicting 12810Z_0_0_data 
2024-05-09 00:07:27.043371: 12810Z_0_0_data, shape torch.Size([1, 42, 192, 267]), rank 0 
2024-05-09 00:07:27.526929: predicting 12812D_0_0_data 
2024-05-09 00:07:27.527904: 12812D_0_0_data, shape torch.Size([1, 43, 194, 261]), rank 0 
2024-05-09 00:07:28.480838: predicting 12813F_0_0_data 
2024-05-09 00:07:28.481726: 12813F_0_0_data, shape torch.Size([1, 43, 194, 261]), rank 0 
2024-05-09 00:07:29.434576: predicting 12817N_8_2_data 
2024-05-09 00:07:29.435754: 12817N_8_2_data, shape torch.Size([1, 42, 201, 267]), rank 0 
2024-05-09 00:07:30.389338: predicting 12821E_4_3_data 
2024-05-09 00:07:30.390200: 12821E_4_3_data, shape torch.Size([1, 43, 193, 268]), rank 0 
2024-05-09 00:07:31.343041: predicting 12828S_0_0_data 
2024-05-09 00:07:31.344184: 12828S_0_0_data, shape torch.Size([1, 42, 182, 256]), rank 0 
2024-05-09 00:07:31.591021: predicting 12832J_8_3_data 
2024-05-09 00:07:31.592241: 12832J_8_3_data, shape torch.Size([1, 42, 196, 264]), rank 0 
2024-05-09 00:07:32.546434: predicting 12850L_16_2_data 
2024-05-09 00:07:32.547304: 12850L_16_2_data, shape torch.Size([1, 31, 188, 261]), rank 0 
2024-05-09 00:07:33.029634: predicting 12889M_0_0_data 
2024-05-09 00:07:33.030548: 12889M_0_0_data, shape torch.Size([1, 42, 200, 265]), rank 0 
2024-05-09 00:07:33.983399: predicting 12891Z_4_3_data 
2024-05-09 00:07:33.984308: 12891Z_4_3_data, shape torch.Size([1, 43, 195, 264]), rank 0 
2024-05-09 00:07:34.938157: predicting 12895H_0_0_data 
2024-05-09 00:07:34.939522: 12895H_0_0_data, shape torch.Size([1, 42, 198, 262]), rank 0 
2024-05-09 00:07:35.893274: predicting 12902E_0_0_data 
2024-05-09 00:07:35.894318: 12902E_0_0_data, shape torch.Size([1, 43, 198, 269]), rank 0 
2024-05-09 00:07:36.848019: predicting 12908Q_16_2_data 
2024-05-09 00:07:36.849092: 12908Q_16_2_data, shape torch.Size([1, 44, 198, 262]), rank 0 
2024-05-09 00:07:37.803061: predicting 12933P_16_2_data 
2024-05-09 00:07:37.803940: 12933P_16_2_data, shape torch.Size([1, 44, 198, 262]), rank 0 
2024-05-09 00:07:38.756348: predicting 12939B_16_2_data 
2024-05-09 00:07:38.757278: 12939B_16_2_data, shape torch.Size([1, 40, 189, 263]), rank 0 
2024-05-09 00:07:39.239841: predicting 12958F_8_3_data 
2024-05-09 00:07:39.240719: 12958F_8_3_data, shape torch.Size([1, 39, 197, 266]), rank 0 
2024-05-09 00:07:40.193819: predicting 12959H_0_0_data 
2024-05-09 00:07:40.194829: 12959H_0_0_data, shape torch.Size([1, 39, 197, 266]), rank 0 
2024-05-09 00:07:41.147441: predicting 12961U_4_3_data 
2024-05-09 00:07:41.148500: 12961U_4_3_data, shape torch.Size([1, 43, 195, 261]), rank 0 
2024-05-09 00:07:42.101843: predicting 12962W_8_2_data 
2024-05-09 00:07:42.102888: 12962W_8_2_data, shape torch.Size([1, 44, 190, 266]), rank 0 
2024-05-09 00:07:42.586814: predicting 12966E_4_2_data 
2024-05-09 00:07:42.587853: 12966E_4_2_data, shape torch.Size([1, 42, 189, 259]), rank 0 
2024-05-09 00:07:43.071324: predicting 12969K_0_0_data 
2024-05-09 00:07:43.072222: 12969K_0_0_data, shape torch.Size([1, 44, 192, 265]), rank 0 
2024-05-09 00:07:43.556885: predicting 12971X_0_0_data 
2024-05-09 00:07:43.557792: 12971X_0_0_data, shape torch.Size([1, 44, 192, 265]), rank 0 
2024-05-09 00:07:44.042127: predicting 12981A_8_2_data 
2024-05-09 00:07:44.042988: 12981A_8_2_data, shape torch.Size([1, 43, 193, 265]), rank 0 
2024-05-09 00:07:44.996306: predicting 13002Z_4_3_data 
2024-05-09 00:07:44.997340: 13002Z_4_3_data, shape torch.Size([1, 44, 199, 264]), rank 0 
2024-05-09 00:07:45.950513: predicting 13012C_4_2_data 
2024-05-09 00:07:45.951381: 13012C_4_2_data, shape torch.Size([1, 43, 198, 273]), rank 0 
2024-05-09 00:07:46.905444: predicting 13016K_8_2_data 
2024-05-09 00:07:46.906515: 13016K_8_2_data, shape torch.Size([1, 42, 193, 264]), rank 0 
2024-05-09 00:07:47.860489: predicting 13019Q_4_3_data 
2024-05-09 00:07:47.861489: 13019Q_4_3_data, shape torch.Size([1, 43, 176, 251]), rank 0 
2024-05-09 00:07:48.109246: predicting 13026N_8_2_data 
2024-05-09 00:07:48.110252: 13026N_8_2_data, shape torch.Size([1, 42, 197, 267]), rank 0 
2024-05-09 00:07:49.063020: predicting 13027P_8_2_data 
2024-05-09 00:07:49.064260: 13027P_8_2_data, shape torch.Size([1, 42, 195, 271]), rank 0 
2024-05-09 00:07:50.018718: predicting 13033K_4_3_data 
2024-05-09 00:07:50.019648: 13033K_4_3_data, shape torch.Size([1, 42, 189, 260]), rank 0 
2024-05-09 00:07:50.504173: predicting 13036Q_0_0_data 
2024-05-09 00:07:50.505113: 13036Q_0_0_data, shape torch.Size([1, 43, 198, 263]), rank 0 
2024-05-09 00:07:51.462845: predicting 13040H_16_2_data 
2024-05-09 00:07:51.463938: 13040H_16_2_data, shape torch.Size([1, 44, 195, 263]), rank 0 
2024-05-09 00:07:52.417664: predicting 13045R_8_2_data 
2024-05-09 00:07:52.418554: 13045R_8_2_data, shape torch.Size([1, 43, 200, 272]), rank 0 
2024-05-09 00:07:53.373389: predicting 13054S_0_0_data 
2024-05-09 00:07:53.374420: 13054S_0_0_data, shape torch.Size([1, 42, 201, 267]), rank 0 
2024-05-09 00:07:54.328276: predicting 13058A_8_2_data 
2024-05-09 00:07:54.329535: 13058A_8_2_data, shape torch.Size([1, 42, 192, 261]), rank 0 
2024-05-09 00:07:54.812250: predicting 13059C_8_3_data 
2024-05-09 00:07:54.813675: 13059C_8_3_data, shape torch.Size([1, 43, 194, 266]), rank 0 
2024-05-09 00:07:55.766804: predicting 13072U_16_2_data 
2024-05-09 00:07:55.767862: 13072U_16_2_data, shape torch.Size([1, 34, 181, 255]), rank 0 
2024-05-09 00:07:56.015673: predicting 13073W_8_3_data 
2024-05-09 00:07:56.016511: 13073W_8_3_data, shape torch.Size([1, 32, 191, 263]), rank 0 
2024-05-09 00:07:56.499788: predicting 13091Y_0_0_data 
2024-05-09 00:07:56.500633: 13091Y_0_0_data, shape torch.Size([1, 42, 194, 265]), rank 0 
2024-05-09 00:07:57.453509: predicting 13097K_8_2_data 
2024-05-09 00:07:57.454390: 13097K_8_2_data, shape torch.Size([1, 42, 198, 268]), rank 0 
2024-05-09 00:07:58.407775: predicting 13098M_8_3_data 
2024-05-09 00:07:58.408913: 13098M_8_3_data, shape torch.Size([1, 40, 194, 270]), rank 0 
2024-05-09 00:07:59.361747: predicting 13114K_16_2_data 
2024-05-09 00:07:59.362590: 13114K_16_2_data, shape torch.Size([1, 43, 198, 270]), rank 0 
2024-05-09 00:08:00.315653: predicting 13123L_8_2_data 
2024-05-09 00:08:00.316632: 13123L_8_2_data, shape torch.Size([1, 42, 192, 261]), rank 0 
2024-05-09 00:08:00.799349: predicting 13125P_4_2_data 
2024-05-09 00:08:00.800288: 13125P_4_2_data, shape torch.Size([1, 43, 204, 266]), rank 0 
2024-05-09 00:08:01.754378: predicting 13129X_16_2_data 
2024-05-09 00:08:01.755505: 13129X_16_2_data, shape torch.Size([1, 43, 188, 266]), rank 0 
2024-05-09 00:08:02.238879: predicting 13136U_4_3_data 
2024-05-09 00:08:02.239704: 13136U_4_3_data, shape torch.Size([1, 44, 190, 260]), rank 0 
2024-05-09 00:08:02.724516: predicting 13163X_0_0_data 
2024-05-09 00:08:02.725468: 13163X_0_0_data, shape torch.Size([1, 43, 194, 266]), rank 0 
2024-05-09 00:08:03.679927: predicting 13165B_4_3_data 
2024-05-09 00:08:03.681359: 13165B_4_3_data, shape torch.Size([1, 42, 188, 262]), rank 0 
2024-05-09 00:08:04.163980: predicting 13176G_16_2_data 
2024-05-09 00:08:04.165228: 13176G_16_2_data, shape torch.Size([1, 43, 199, 264]), rank 0 
2024-05-09 00:08:05.117884: predicting 13195K_4_3_data 
2024-05-09 00:08:05.118760: 13195K_4_3_data, shape torch.Size([1, 44, 186, 262]), rank 0 
2024-05-09 00:08:05.601549: predicting 13196M_0_0_data 
2024-05-09 00:08:05.602629: 13196M_0_0_data, shape torch.Size([1, 44, 186, 262]), rank 0 
2024-05-09 00:08:06.086688: predicting 13220J_0_0_data 
2024-05-09 00:08:06.087711: 13220J_0_0_data, shape torch.Size([1, 43, 192, 265]), rank 0 
2024-05-09 00:08:06.571434: predicting 13237A_4_2_data 
2024-05-09 00:08:06.572256: 13237A_4_2_data, shape torch.Size([1, 42, 193, 266]), rank 0 
2024-05-09 00:08:07.524784: predicting 13261X_4_3_data 
2024-05-09 00:08:07.525690: 13261X_4_3_data, shape torch.Size([1, 43, 194, 265]), rank 0 
2024-05-09 00:08:08.480094: predicting 13270Y_4_2_data 
2024-05-09 00:08:08.481024: 13270Y_4_2_data, shape torch.Size([1, 43, 190, 265]), rank 0 
2024-05-09 00:08:08.964760: predicting 13272C_16_2_data 
2024-05-09 00:08:08.965633: 13272C_16_2_data, shape torch.Size([1, 42, 197, 264]), rank 0 
2024-05-09 00:08:09.919527: predicting 13283H_4_3_data 
2024-05-09 00:08:09.920632: 13283H_4_3_data, shape torch.Size([1, 43, 191, 256]), rank 0 
2024-05-09 00:08:10.168590: predicting 13297S_0_0_data 
2024-05-09 00:08:10.169887: 13297S_0_0_data, shape torch.Size([1, 42, 181, 257]), rank 0 
2024-05-09 00:08:10.654068: predicting 13299W_8_3_data 
2024-05-09 00:08:10.654985: 13299W_8_3_data, shape torch.Size([1, 40, 194, 263]), rank 0 
2024-05-09 00:08:11.608299: predicting 13308X_8_2_data 
2024-05-09 00:08:11.609231: 13308X_8_2_data, shape torch.Size([1, 41, 191, 265]), rank 0 
2024-05-09 00:08:12.092636: predicting 13313Q_0_0_data 
2024-05-09 00:08:12.093663: 13313Q_0_0_data, shape torch.Size([1, 43, 189, 264]), rank 0 
2024-05-09 00:08:12.577882: predicting 13318A_8_3_data 
2024-05-09 00:08:12.578942: 13318A_8_3_data, shape torch.Size([1, 43, 194, 264]), rank 0 
2024-05-09 00:08:13.532967: predicting 13319C_8_3_data 
2024-05-09 00:08:13.533818: 13319C_8_3_data, shape torch.Size([1, 39, 175, 255]), rank 0 
2024-05-09 00:08:13.781090: predicting 13342X_8_2_data 
2024-05-09 00:08:13.781957: 13342X_8_2_data, shape torch.Size([1, 39, 182, 260]), rank 0 
2024-05-09 00:08:14.266114: predicting 13350W_4_3_data 
2024-05-09 00:08:14.266982: 13350W_4_3_data, shape torch.Size([1, 39, 198, 264]), rank 0 
2024-05-09 00:08:15.220182: predicting 13351Y_0_0_data 
2024-05-09 00:08:15.221066: 13351Y_0_0_data, shape torch.Size([1, 39, 198, 264]), rank 0 
2024-05-09 00:08:16.174790: predicting 13353C_0_0_data 
2024-05-09 00:08:16.176135: 13353C_0_0_data, shape torch.Size([1, 39, 198, 264]), rank 0 
2024-05-09 00:08:17.129302: predicting 13361B_4_3_data 
2024-05-09 00:08:17.130196: 13361B_4_3_data, shape torch.Size([1, 42, 197, 273]), rank 0 
2024-05-09 00:08:18.083067: predicting 13371E_0_0_data 
2024-05-09 00:08:18.084336: 13371E_0_0_data, shape torch.Size([1, 42, 196, 267]), rank 0 
2024-05-09 00:08:19.038689: predicting 13378S_8_3_data 
2024-05-09 00:08:19.039736: 13378S_8_3_data, shape torch.Size([1, 33, 193, 263]), rank 0 
2024-05-09 00:08:19.991762: predicting 13382J_16_2_data 
2024-05-09 00:08:19.992758: 13382J_16_2_data, shape torch.Size([1, 42, 197, 276]), rank 0 
2024-05-09 00:08:20.947212: predicting 13398Y_4_2_data 
2024-05-09 00:08:20.948169: 13398Y_4_2_data, shape torch.Size([1, 44, 189, 261]), rank 0 
2024-05-09 00:08:21.430817: predicting 13399A_0_0_data 
2024-05-09 00:08:21.431698: 13399A_0_0_data, shape torch.Size([1, 44, 189, 261]), rank 0 
2024-05-09 00:08:21.916387: predicting 13403R_8_2_data 
2024-05-09 00:08:21.917296: 13403R_8_2_data, shape torch.Size([1, 39, 193, 263]), rank 0 
2024-05-09 00:08:22.870352: predicting 13408B_8_3_data 
2024-05-09 00:08:22.871254: 13408B_8_3_data, shape torch.Size([1, 43, 192, 262]), rank 0 
2024-05-09 00:08:23.354825: predicting 13423X_8_2_data 
2024-05-09 00:08:23.355741: 13423X_8_2_data, shape torch.Size([1, 43, 196, 272]), rank 0 
2024-05-09 00:08:24.309110: predicting 13429J_4_3_data 
2024-05-09 00:08:24.310012: 13429J_4_3_data, shape torch.Size([1, 43, 200, 265]), rank 0 
2024-05-09 00:08:25.262890: predicting 13431W_16_2_data 
2024-05-09 00:08:25.263848: 13431W_16_2_data, shape torch.Size([1, 42, 195, 266]), rank 0 
2024-05-09 00:08:26.216986: predicting 13458Q_4_2_data 
2024-05-09 00:08:26.217958: 13458Q_4_2_data, shape torch.Size([1, 44, 194, 265]), rank 0 
2024-05-09 00:08:27.171969: predicting 13462H_16_2_data 
2024-05-09 00:08:27.172930: 13462H_16_2_data, shape torch.Size([1, 44, 189, 262]), rank 0 
2024-05-09 00:08:27.655222: predicting 13479Y_8_2_data 
2024-05-09 00:08:27.656289: 13479Y_8_2_data, shape torch.Size([1, 42, 179, 260]), rank 0 
2024-05-09 00:08:28.139986: predicting 13496Y_8_2_data 
2024-05-09 00:08:28.140809: 13496Y_8_2_data, shape torch.Size([1, 43, 199, 267]), rank 0 
2024-05-09 00:08:29.095571: predicting 13504X_0_0_data 
2024-05-09 00:08:29.096604: 13504X_0_0_data, shape torch.Size([1, 42, 191, 265]), rank 0 
2024-05-09 00:08:29.581940: predicting 13509H_16_2_data 
2024-05-09 00:08:29.582810: 13509H_16_2_data, shape torch.Size([1, 43, 201, 266]), rank 0 
2024-05-09 00:08:30.536528: predicting 13517G_8_3_data 
2024-05-09 00:08:30.537443: 13517G_8_3_data, shape torch.Size([1, 43, 194, 265]), rank 0 
2024-05-09 00:08:31.492617: predicting 13519K_8_3_data 
2024-05-09 00:08:31.494018: 13519K_8_3_data, shape torch.Size([1, 43, 202, 265]), rank 0 
2024-05-09 00:08:32.447943: predicting 13522Z_4_3_data 
2024-05-09 00:08:32.448997: 13522Z_4_3_data, shape torch.Size([1, 42, 195, 266]), rank 0 
2024-05-09 00:08:33.403849: predicting 13540B_4_2_data 
2024-05-09 00:08:33.404784: 13540B_4_2_data, shape torch.Size([1, 42, 195, 269]), rank 0 
2024-05-09 00:08:34.357843: predicting 13542F_0_0_data 
2024-05-09 00:08:34.358728: 13542F_0_0_data, shape torch.Size([1, 43, 197, 258]), rank 0 
2024-05-09 00:08:35.311775: predicting 13563N_4_3_data 
2024-05-09 00:08:35.312853: 13563N_4_3_data, shape torch.Size([1, 43, 193, 264]), rank 0 
2024-05-09 00:08:36.266411: predicting 13573Q_4_3_data 
2024-05-09 00:08:36.267380: 13573Q_4_3_data, shape torch.Size([1, 43, 190, 263]), rank 0 
2024-05-09 00:08:36.751126: predicting 13576W_16_2_data 
2024-05-09 00:08:36.752148: 13576W_16_2_data, shape torch.Size([1, 41, 195, 267]), rank 0 
2024-05-09 00:08:37.705626: predicting 13583T_4_2_data 
2024-05-09 00:08:37.706799: 13583T_4_2_data, shape torch.Size([1, 42, 198, 267]), rank 0 
2024-05-09 00:08:38.660998: predicting 13649X_8_3_data 
2024-05-09 00:08:38.662098: 13649X_8_3_data, shape torch.Size([1, 43, 194, 263]), rank 0 
2024-05-09 00:08:39.615508: predicting 13671Q_4_3_data 
2024-05-09 00:08:39.616788: 13671Q_4_3_data, shape torch.Size([1, 42, 195, 264]), rank 0 
2024-05-09 00:08:40.570622: predicting 13684Z_16_2_data 
2024-05-09 00:08:40.571539: 13684Z_16_2_data, shape torch.Size([1, 43, 193, 264]), rank 0 
2024-05-09 00:08:41.524897: predicting 13695E_0_0_data 
2024-05-09 00:08:41.525835: 13695E_0_0_data, shape torch.Size([1, 40, 194, 264]), rank 0 
2024-05-09 00:08:42.481917: predicting 13704F_4_2_data 
2024-05-09 00:08:42.482824: 13704F_4_2_data, shape torch.Size([1, 43, 194, 263]), rank 0 
2024-05-09 00:08:43.437386: predicting 13710A_0_0_data 
2024-05-09 00:08:43.438470: 13710A_0_0_data, shape torch.Size([1, 43, 193, 264]), rank 0 
2024-05-09 00:08:44.392378: predicting 13720D_16_2_data 
2024-05-09 00:08:44.393329: 13720D_16_2_data, shape torch.Size([1, 35, 184, 262]), rank 0 
2024-05-09 00:08:44.874883: predicting 13726P_16_2_data 
2024-05-09 00:08:44.876000: 13726P_16_2_data, shape torch.Size([1, 41, 191, 266]), rank 0 
2024-05-09 00:08:45.359805: predicting 13759E_0_0_data 
2024-05-09 00:08:45.360809: 13759E_0_0_data, shape torch.Size([1, 42, 195, 263]), rank 0 
2024-05-09 00:08:46.314890: predicting 13774A_0_0_data 
2024-05-09 00:08:46.315837: 13774A_0_0_data, shape torch.Size([1, 42, 197, 264]), rank 0 
2024-05-09 00:08:47.269376: predicting 13775C_16_2_data 
2024-05-09 00:08:47.270282: 13775C_16_2_data, shape torch.Size([1, 35, 202, 257]), rank 0 
2024-05-09 00:08:48.222813: predicting 13786H_8_3_data 
2024-05-09 00:08:48.223817: 13786H_8_3_data, shape torch.Size([1, 43, 198, 279]), rank 0 
2024-05-09 00:08:49.177546: predicting 13802F_16_2_data 
2024-05-09 00:08:49.178471: 13802F_16_2_data, shape torch.Size([1, 41, 184, 263]), rank 0 
2024-05-09 00:08:49.660341: predicting 13803H_16_2_data 
2024-05-09 00:08:49.661279: 13803H_16_2_data, shape torch.Size([1, 42, 194, 268]), rank 0 
2024-05-09 00:08:50.615675: predicting 13806N_16_2_data 
2024-05-09 00:08:50.616660: 13806N_16_2_data, shape torch.Size([1, 43, 186, 258]), rank 0 
2024-05-09 00:08:51.099336: predicting 13820H_8_3_data 
2024-05-09 00:08:51.100303: 13820H_8_3_data, shape torch.Size([1, 42, 198, 264]), rank 0 
2024-05-09 00:08:52.054176: predicting 13824P_4_3_data 
2024-05-09 00:08:52.055099: 13824P_4_3_data, shape torch.Size([1, 43, 198, 268]), rank 0 
2024-05-09 00:08:53.008531: predicting 13828X_0_0_data 
2024-05-09 00:08:53.009407: 13828X_0_0_data, shape torch.Size([1, 43, 198, 268]), rank 0 
2024-05-09 00:08:53.962496: predicting 13829Z_4_2_data 
2024-05-09 00:08:53.963678: 13829Z_4_2_data, shape torch.Size([1, 44, 186, 260]), rank 0 
2024-05-09 00:08:54.445803: predicting 13830K_16_2_data 
2024-05-09 00:08:54.446816: 13830K_16_2_data, shape torch.Size([1, 43, 198, 267]), rank 0 
2024-05-09 00:08:55.399006: predicting 13883F_8_3_data 
2024-05-09 00:08:55.399950: 13883F_8_3_data, shape torch.Size([1, 38, 192, 260]), rank 0 
2024-05-09 00:08:55.881933: predicting 13888P_8_2_data 
2024-05-09 00:08:55.883057: 13888P_8_2_data, shape torch.Size([1, 43, 191, 262]), rank 0 
2024-05-09 00:08:56.367228: predicting 13890C_0_0_data 
2024-05-09 00:08:56.368071: 13890C_0_0_data, shape torch.Size([1, 43, 182, 260]), rank 0 
2024-05-09 00:08:56.850975: predicting 13898S_4_3_data 
2024-05-09 00:08:56.851839: 13898S_4_3_data, shape torch.Size([1, 43, 188, 265]), rank 0 
2024-05-09 00:08:57.336542: predicting 13899U_16_2_data 
2024-05-09 00:08:57.337643: 13899U_16_2_data, shape torch.Size([1, 41, 180, 255]), rank 0 
2024-05-09 00:08:57.585517: predicting 13901H_0_0_data 
2024-05-09 00:08:57.586424: 13901H_0_0_data, shape torch.Size([1, 38, 197, 266]), rank 0 
2024-05-09 00:08:58.540570: predicting 13905P_16_2_data 
2024-05-09 00:08:58.541492: 13905P_16_2_data, shape torch.Size([1, 42, 198, 264]), rank 0 
2024-05-09 00:08:59.494407: predicting 13919A_0_0_data 
2024-05-09 00:08:59.495554: 13919A_0_0_data, shape torch.Size([1, 44, 197, 268]), rank 0 
2024-05-09 00:09:00.450586: predicting 13929D_8_3_data 
2024-05-09 00:09:00.451692: 13929D_8_3_data, shape torch.Size([1, 33, 198, 255]), rank 0 
2024-05-09 00:09:00.932530: predicting 13935Y_4_2_data 
2024-05-09 00:09:00.933423: 13935Y_4_2_data, shape torch.Size([1, 43, 184, 256]), rank 0 
2024-05-09 00:09:01.181646: predicting 13940R_4_2_data 
2024-05-09 00:09:01.182563: 13940R_4_2_data, shape torch.Size([1, 44, 178, 256]), rank 0 
2024-05-09 00:09:01.431181: predicting 13941T_16_2_data 
2024-05-09 00:09:01.432467: 13941T_16_2_data, shape torch.Size([1, 42, 195, 266]), rank 0 
2024-05-09 00:09:02.385842: predicting 13945B_0_0_data 
2024-05-09 00:09:02.386813: 13945B_0_0_data, shape torch.Size([1, 42, 178, 255]), rank 0 
2024-05-09 00:09:02.635066: predicting 13947F_4_2_data 
2024-05-09 00:09:02.635946: 13947F_4_2_data, shape torch.Size([1, 42, 190, 260]), rank 0 
2024-05-09 00:09:03.119911: predicting 13949J_4_3_data 
2024-05-09 00:09:03.120815: 13949J_4_3_data, shape torch.Size([1, 42, 193, 266]), rank 0 
2024-05-09 00:09:04.073644: predicting 13959M_4_3_data 
2024-05-09 00:09:04.074705: 13959M_4_3_data, shape torch.Size([1, 43, 206, 266]), rank 0 
2024-05-09 00:09:05.029982: predicting 13981F_4_3_data 
2024-05-09 00:09:05.030912: 13981F_4_3_data, shape torch.Size([1, 42, 188, 264]), rank 0 
2024-05-09 00:09:05.513826: predicting 14011F_0_0_data 
2024-05-09 00:09:05.515014: 14011F_0_0_data, shape torch.Size([1, 44, 194, 264]), rank 0 
2024-05-09 00:09:06.469092: predicting 14012H_8_2_data 
2024-05-09 00:09:06.470158: 14012H_8_2_data, shape torch.Size([1, 43, 190, 259]), rank 0 
2024-05-09 00:09:06.953119: predicting 14017R_16_2_data 
2024-05-09 00:09:06.953980: 14017R_16_2_data, shape torch.Size([1, 33, 190, 263]), rank 0 
2024-05-09 00:09:07.437409: predicting 14019V_8_2_data 
2024-05-09 00:09:07.438340: 14019V_8_2_data, shape torch.Size([1, 29, 184, 258]), rank 0 
2024-05-09 00:09:07.920759: predicting 14025Q_8_2_data 
2024-05-09 00:09:07.921665: 14025Q_8_2_data, shape torch.Size([1, 42, 201, 266]), rank 0 
2024-05-09 00:09:08.874572: predicting 14031L_4_2_data 
2024-05-09 00:09:08.875530: 14031L_4_2_data, shape torch.Size([1, 43, 197, 264]), rank 0 
2024-05-09 00:09:09.830182: predicting 14039B_16_2_data 
2024-05-09 00:09:09.831175: 14039B_16_2_data, shape torch.Size([1, 43, 192, 261]), rank 0 
2024-05-09 00:09:10.314468: predicting 14062W_4_2_data 
2024-05-09 00:09:10.315413: 14062W_4_2_data, shape torch.Size([1, 44, 194, 264]), rank 0 
2024-05-09 00:09:11.268538: predicting 14066E_16_2_data 
2024-05-09 00:09:11.269426: 14066E_16_2_data, shape torch.Size([1, 43, 199, 266]), rank 0 
2024-05-09 00:09:12.221965: predicting 14097P_8_2_data 
2024-05-09 00:09:12.223140: 14097P_8_2_data, shape torch.Size([1, 40, 192, 264]), rank 0 
2024-05-09 00:09:12.705880: predicting 14101G_8_3_data 
2024-05-09 00:09:12.706742: 14101G_8_3_data, shape torch.Size([1, 43, 190, 262]), rank 0 
2024-05-09 00:09:13.190814: predicting 14106Q_16_2_data 
2024-05-09 00:09:13.192175: 14106Q_16_2_data, shape torch.Size([1, 42, 191, 261]), rank 0 
2024-05-09 00:09:13.675737: predicting 14113N_16_2_data 
2024-05-09 00:09:13.676644: 14113N_16_2_data, shape torch.Size([1, 37, 194, 261]), rank 0 
2024-05-09 00:09:14.628771: predicting 14114P_4_2_data 
2024-05-09 00:09:14.629623: 14114P_4_2_data, shape torch.Size([1, 44, 195, 266]), rank 0 
2024-05-09 00:09:15.584000: predicting 14119Z_16_2_data 
2024-05-09 00:09:15.584906: 14119Z_16_2_data, shape torch.Size([1, 43, 193, 263]), rank 0 
2024-05-09 00:09:16.541591: predicting 14121M_0_0_data 
2024-05-09 00:09:16.542519: 14121M_0_0_data, shape torch.Size([1, 43, 193, 263]), rank 0 
2024-05-09 00:09:17.496689: predicting 14131P_8_3_data 
2024-05-09 00:09:17.497651: 14131P_8_3_data, shape torch.Size([1, 32, 190, 261]), rank 0 
2024-05-09 00:09:17.979425: predicting 14163C_16_2_data 
2024-05-09 00:09:17.980314: 14163C_16_2_data, shape torch.Size([1, 32, 194, 254]), rank 0 
2024-05-09 00:09:18.462399: predicting 14165G_16_2_data 
2024-05-09 00:09:18.463253: 14165G_16_2_data, shape torch.Size([1, 43, 190, 264]), rank 0 
2024-05-09 00:09:18.946407: predicting 14170Z_16_2_data 
2024-05-09 00:09:18.947277: 14170Z_16_2_data, shape torch.Size([1, 42, 190, 261]), rank 0 
2024-05-09 00:09:19.431587: predicting 14171B_4_3_data 
2024-05-09 00:09:19.432547: 14171B_4_3_data, shape torch.Size([1, 42, 194, 263]), rank 0 
2024-05-09 00:09:20.385971: predicting 14173F_4_3_data 
2024-05-09 00:09:20.386846: 14173F_4_3_data, shape torch.Size([1, 42, 200, 269]), rank 0 
2024-05-09 00:09:21.341235: predicting 14180C_0_0_data 
2024-05-09 00:09:21.342128: 14180C_0_0_data, shape torch.Size([1, 42, 193, 266]), rank 0 
2024-05-09 00:09:22.296735: predicting 14187Q_16_2_data 
2024-05-09 00:09:22.297950: 14187Q_16_2_data, shape torch.Size([1, 42, 195, 266]), rank 0 
2024-05-09 00:09:23.250574: predicting 14192J_4_2_data 
2024-05-09 00:09:23.251730: 14192J_4_2_data, shape torch.Size([1, 43, 198, 270]), rank 0 
2024-05-09 00:09:24.204907: predicting 14198V_0_0_data 
2024-05-09 00:09:24.205885: 14198V_0_0_data, shape torch.Size([1, 42, 197, 266]), rank 0 
2024-05-09 00:09:25.159785: predicting 14219D_8_2_data 
2024-05-09 00:09:25.160721: 14219D_8_2_data, shape torch.Size([1, 42, 194, 276]), rank 0 
2024-05-09 00:09:26.114388: predicting 14224W_0_0_data 
2024-05-09 00:09:26.115481: 14224W_0_0_data, shape torch.Size([1, 43, 198, 280]), rank 0 
2024-05-09 00:09:27.069346: predicting 14232V_8_2_data 
2024-05-09 00:09:27.070626: 14232V_8_2_data, shape torch.Size([1, 43, 197, 263]), rank 0 
2024-05-09 00:09:28.024745: predicting 14240U_8_3_data 
2024-05-09 00:09:28.025625: 14240U_8_3_data, shape torch.Size([1, 31, 198, 274]), rank 0 
2024-05-09 00:09:28.977752: predicting 14241W_16_2_data 
2024-05-09 00:09:28.978765: 14241W_16_2_data, shape torch.Size([1, 43, 196, 265]), rank 0 
2024-05-09 00:09:29.932010: predicting 14246G_4_3_data 
2024-05-09 00:09:29.933163: 14246G_4_3_data, shape torch.Size([1, 39, 201, 279]), rank 0 
2024-05-09 00:09:30.887675: predicting 14282K_16_2_data 
2024-05-09 00:09:30.888774: 14282K_16_2_data, shape torch.Size([1, 43, 196, 265]), rank 0 
2024-05-09 00:09:31.843315: predicting 14300M_0_0_data 
2024-05-09 00:09:31.844231: 14300M_0_0_data, shape torch.Size([1, 43, 194, 263]), rank 0 
2024-05-09 00:09:32.798212: predicting 14309E_8_2_data 
2024-05-09 00:09:32.799309: 14309E_8_2_data, shape torch.Size([1, 43, 197, 277]), rank 0 
2024-05-09 00:09:33.753112: predicting 14311R_16_2_data 
2024-05-09 00:09:33.754024: 14311R_16_2_data, shape torch.Size([1, 32, 198, 249]), rank 0 
2024-05-09 00:09:34.238156: predicting 14316B_4_3_data 
2024-05-09 00:09:34.238977: 14316B_4_3_data, shape torch.Size([1, 43, 193, 264]), rank 0 
2024-05-09 00:09:35.191959: predicting 14317D_0_0_data 
2024-05-09 00:09:35.192828: 14317D_0_0_data, shape torch.Size([1, 43, 193, 264]), rank 0 
2024-05-09 00:09:36.145690: predicting 14335F_8_3_data 
2024-05-09 00:09:36.146857: 14335F_8_3_data, shape torch.Size([1, 42, 201, 262]), rank 0 
2024-05-09 00:09:37.099650: predicting 14342C_16_2_data 
2024-05-09 00:09:37.100614: 14342C_16_2_data, shape torch.Size([1, 43, 197, 266]), rank 0 
2024-05-09 00:09:38.054920: predicting 14350B_8_2_data 
2024-05-09 00:09:38.055786: 14350B_8_2_data, shape torch.Size([1, 42, 196, 276]), rank 0 
2024-05-09 00:09:39.010492: predicting 14366Q_4_2_data 
2024-05-09 00:09:39.011792: 14366Q_4_2_data, shape torch.Size([1, 43, 196, 266]), rank 0 
2024-05-09 00:09:39.965177: predicting 14380K_16_2_data 
2024-05-09 00:09:39.966385: 14380K_16_2_data, shape torch.Size([1, 43, 190, 260]), rank 0 
2024-05-09 00:09:40.449667: predicting 14388A_8_3_data 
2024-05-09 00:09:40.450711: 14388A_8_3_data, shape torch.Size([1, 43, 192, 264]), rank 0 
2024-05-09 00:09:40.934246: predicting 14393T_8_2_data 
2024-05-09 00:09:40.935156: 14393T_8_2_data, shape torch.Size([1, 43, 199, 267]), rank 0 
2024-05-09 00:09:41.889119: predicting 14401S_4_2_data 
2024-05-09 00:09:41.890000: 14401S_4_2_data, shape torch.Size([1, 43, 197, 266]), rank 0 
2024-05-09 00:09:42.843065: predicting 14404Y_0_0_data 
2024-05-09 00:09:42.844098: 14404Y_0_0_data, shape torch.Size([1, 44, 188, 264]), rank 0 
2024-05-09 00:09:43.327008: predicting 14437N_8_3_data 
2024-05-09 00:09:43.328057: 14437N_8_3_data, shape torch.Size([1, 43, 183, 259]), rank 0 
2024-05-09 00:09:43.811083: predicting 14462M_16_2_data 
2024-05-09 00:09:43.812041: 14462M_16_2_data, shape torch.Size([1, 43, 202, 265]), rank 0 
2024-05-09 00:09:44.766183: predicting 14465S_4_2_data 
2024-05-09 00:09:44.767270: 14465S_4_2_data, shape torch.Size([1, 42, 183, 252]), rank 0 
2024-05-09 00:09:45.015115: predicting 14466U_16_2_data 
2024-05-09 00:09:45.016118: 14466U_16_2_data, shape torch.Size([1, 41, 169, 250]), rank 0 
2024-05-09 00:09:45.264042: predicting 14468Y_4_2_data 
2024-05-09 00:09:45.264913: 14468Y_4_2_data, shape torch.Size([1, 43, 177, 259]), rank 0 
2024-05-09 00:09:45.748591: predicting 14469A_16_2_data 
2024-05-09 00:09:45.749508: 14469A_16_2_data, shape torch.Size([1, 42, 183, 257]), rank 0 
2024-05-09 00:09:46.233611: predicting 14476X_8_2_data 
2024-05-09 00:09:46.234487: 14476X_8_2_data, shape torch.Size([1, 40, 193, 264]), rank 0 
2024-05-09 00:09:47.187553: predicting 14484W_4_2_data 
2024-05-09 00:09:47.188391: 14484W_4_2_data, shape torch.Size([1, 43, 194, 268]), rank 0 
2024-05-09 00:09:48.142118: predicting 14498H_8_3_data 
2024-05-09 00:09:48.143194: 14498H_8_3_data, shape torch.Size([1, 43, 178, 250]), rank 0 
2024-05-09 00:09:48.390870: predicting 14499J_16_2_data 
2024-05-09 00:09:48.391790: 14499J_16_2_data, shape torch.Size([1, 32, 202, 263]), rank 0 
2024-05-09 00:09:49.343582: predicting 14500U_8_3_data 
2024-05-09 00:09:49.344763: 14500U_8_3_data, shape torch.Size([1, 44, 194, 267]), rank 0 
2024-05-09 00:09:50.297361: predicting 14502Y_16_2_data 
2024-05-09 00:09:50.298327: 14502Y_16_2_data, shape torch.Size([1, 37, 188, 257]), rank 0 
2024-05-09 00:09:50.781019: predicting 14506G_4_3_data 
2024-05-09 00:09:50.781883: 14506G_4_3_data, shape torch.Size([1, 43, 193, 262]), rank 0 
2024-05-09 00:09:51.735370: predicting 14528Q_4_2_data 
2024-05-09 00:09:51.736253: 14528Q_4_2_data, shape torch.Size([1, 43, 198, 264]), rank 0 
2024-05-09 00:09:52.690242: predicting 14546S_8_3_data 
2024-05-09 00:09:52.691218: 14546S_8_3_data, shape torch.Size([1, 42, 193, 265]), rank 0 
2024-05-09 00:09:53.645350: predicting 14548W_4_2_data 
2024-05-09 00:09:53.646231: 14548W_4_2_data, shape torch.Size([1, 42, 190, 261]), rank 0 
2024-05-09 00:09:54.133440: predicting 14562Q_0_0_data 
2024-05-09 00:09:54.134311: 14562Q_0_0_data, shape torch.Size([1, 42, 182, 259]), rank 0 
2024-05-09 00:09:54.617682: predicting 14599N_4_2_data 
2024-05-09 00:09:54.618725: 14599N_4_2_data, shape torch.Size([1, 44, 194, 264]), rank 0 
2024-05-09 00:09:55.574775: predicting 14602C_4_2_data 
2024-05-09 00:09:55.575607: 14602C_4_2_data, shape torch.Size([1, 42, 195, 266]), rank 0 
2024-05-09 00:09:56.529454: predicting 14626Q_4_2_data 
2024-05-09 00:09:56.530565: 14626Q_4_2_data, shape torch.Size([1, 43, 191, 257]), rank 0 
2024-05-09 00:09:57.014235: predicting 14640K_16_2_data 
2024-05-09 00:09:57.015189: 14640K_16_2_data, shape torch.Size([1, 32, 192, 261]), rank 0 
2024-05-09 00:09:57.498443: predicting 14653T_4_3_data 
2024-05-09 00:09:57.499336: 14653T_4_3_data, shape torch.Size([1, 44, 194, 266]), rank 0 
2024-05-09 00:09:58.453805: predicting 14657B_16_2_data 
2024-05-09 00:09:58.454696: 14657B_16_2_data, shape torch.Size([1, 43, 194, 259]), rank 0 
2024-05-09 00:09:59.408979: predicting 14658D_4_2_data 
2024-05-09 00:09:59.410090: 14658D_4_2_data, shape torch.Size([1, 43, 201, 268]), rank 0 
2024-05-09 00:10:00.364480: predicting 14668G_8_2_data 
2024-05-09 00:10:00.365619: 14668G_8_2_data, shape torch.Size([1, 42, 198, 265]), rank 0 
2024-05-09 00:10:01.320119: predicting 14672X_16_2_data 
2024-05-09 00:10:01.321266: 14672X_16_2_data, shape torch.Size([1, 43, 194, 266]), rank 0 
2024-05-09 00:10:02.274847: predicting 14676F_4_2_data 
2024-05-09 00:10:02.275802: 14676F_4_2_data, shape torch.Size([1, 43, 186, 262]), rank 0 
2024-05-09 00:10:02.758403: predicting 14678J_8_2_data 
2024-05-09 00:10:02.759321: 14678J_8_2_data, shape torch.Size([1, 43, 185, 262]), rank 0 
2024-05-09 00:10:03.244473: predicting 14707Q_8_2_data 
2024-05-09 00:10:03.245389: 14707Q_8_2_data, shape torch.Size([1, 42, 188, 262]), rank 0 
2024-05-09 00:10:03.729322: predicting 14712J_8_3_data 
2024-05-09 00:10:03.730242: 14712J_8_3_data, shape torch.Size([1, 42, 194, 263]), rank 0 
2024-05-09 00:10:04.683440: predicting 14715P_4_3_data 
2024-05-09 00:10:04.684325: 14715P_4_3_data, shape torch.Size([1, 43, 198, 267]), rank 0 
2024-05-09 00:10:05.638184: predicting 14764C_4_2_data 
2024-05-09 00:10:05.639067: 14764C_4_2_data, shape torch.Size([1, 43, 189, 266]), rank 0 
2024-05-09 00:10:06.122386: predicting 14772B_0_0_data 
2024-05-09 00:10:06.123290: 14772B_0_0_data, shape torch.Size([1, 29, 189, 259]), rank 0 
2024-05-09 00:10:06.605035: predicting 14773D_4_2_data 
2024-05-09 00:10:06.605860: 14773D_4_2_data, shape torch.Size([1, 44, 194, 265]), rank 0 
2024-05-09 00:10:07.559989: predicting 14785K_16_2_data 
2024-05-09 00:10:07.560957: 14785K_16_2_data, shape torch.Size([1, 43, 186, 264]), rank 0 
2024-05-09 00:10:08.043417: predicting 14786M_0_0_data 
2024-05-09 00:10:08.044468: 14786M_0_0_data, shape torch.Size([1, 43, 186, 264]), rank 0 
2024-05-09 00:10:08.528479: predicting 14806S_8_3_data 
2024-05-09 00:10:08.529345: 14806S_8_3_data, shape torch.Size([1, 43, 192, 272]), rank 0 
2024-05-09 00:10:09.012874: predicting 14817X_4_2_data 
2024-05-09 00:10:09.013714: 14817X_4_2_data, shape torch.Size([1, 43, 194, 265]), rank 0 
2024-05-09 00:10:09.967949: predicting 14819B_4_2_data 
2024-05-09 00:10:09.969172: 14819B_4_2_data, shape torch.Size([1, 42, 190, 264]), rank 0 
2024-05-09 00:10:10.452032: predicting 14825W_0_0_data 
2024-05-09 00:10:10.453366: 14825W_0_0_data, shape torch.Size([1, 42, 190, 264]), rank 0 
2024-05-09 00:10:10.938123: predicting 14827A_0_0_data 
2024-05-09 00:10:10.939056: 14827A_0_0_data, shape torch.Size([1, 43, 198, 265]), rank 0 
2024-05-09 00:10:11.893460: predicting 14836B_0_0_data 
2024-05-09 00:10:11.894503: 14836B_0_0_data, shape torch.Size([1, 43, 200, 265]), rank 0 
2024-05-09 00:10:12.847922: predicting 14837D_16_2_data 
2024-05-09 00:10:12.848778: 14837D_16_2_data, shape torch.Size([1, 44, 197, 263]), rank 0 
2024-05-09 00:10:13.801409: predicting 14851X_0_0_data 
2024-05-09 00:10:13.802299: 14851X_0_0_data, shape torch.Size([1, 44, 186, 261]), rank 0 
2024-05-09 00:10:14.285301: predicting 14859N_8_2_data 
2024-05-09 00:10:14.286590: 14859N_8_2_data, shape torch.Size([1, 43, 190, 265]), rank 0 
2024-05-09 00:10:14.771846: predicting 14878R_16_2_data 
2024-05-09 00:10:14.772885: 14878R_16_2_data, shape torch.Size([1, 43, 206, 269]), rank 0 
2024-05-09 00:10:15.727018: predicting 14881G_0_0_data 
2024-05-09 00:10:15.728227: 14881G_0_0_data, shape torch.Size([1, 42, 193, 270]), rank 0 
2024-05-09 00:10:16.681752: predicting 14884M_4_3_data 
2024-05-09 00:10:16.682880: 14884M_4_3_data, shape torch.Size([1, 43, 187, 262]), rank 0 
2024-05-09 00:10:17.165120: predicting 14892L_8_2_data 
2024-05-09 00:10:17.166103: 14892L_8_2_data, shape torch.Size([1, 43, 194, 273]), rank 0 
2024-05-09 00:10:18.121784: predicting 14894P_0_0_data 
2024-05-09 00:10:18.122883: 14894P_0_0_data, shape torch.Size([1, 43, 194, 273]), rank 0 
2024-05-09 00:10:19.078252: predicting 14898X_8_3_data 
2024-05-09 00:10:19.079214: 14898X_8_3_data, shape torch.Size([1, 43, 200, 265]), rank 0 
2024-05-09 00:10:20.033504: predicting 14909C_4_3_data 
2024-05-09 00:10:20.034471: 14909C_4_3_data, shape torch.Size([1, 44, 197, 264]), rank 0 
2024-05-09 00:10:20.991141: predicting 14924Y_4_2_data 
2024-05-09 00:10:20.992054: 14924Y_4_2_data, shape torch.Size([1, 42, 195, 266]), rank 0 
2024-05-09 00:10:21.946275: predicting 14936F_8_2_data 
2024-05-09 00:10:21.947212: 14936F_8_2_data, shape torch.Size([1, 40, 199, 276]), rank 0 
2024-05-09 00:10:22.900160: predicting 14945G_0_0_data 
2024-05-09 00:10:22.901400: 14945G_0_0_data, shape torch.Size([1, 42, 197, 266]), rank 0 
2024-05-09 00:10:23.854382: predicting 14950Z_0_0_data 
2024-05-09 00:10:23.855309: 14950Z_0_0_data, shape torch.Size([1, 42, 197, 266]), rank 0 
2024-05-09 00:10:24.810819: predicting 14958P_8_2_data 
2024-05-09 00:10:24.811898: 14958P_8_2_data, shape torch.Size([1, 44, 191, 261]), rank 0 
2024-05-09 00:10:25.295114: predicting 14959R_8_2_data 
2024-05-09 00:10:25.296189: 14959R_8_2_data, shape torch.Size([1, 43, 189, 261]), rank 0 
2024-05-09 00:10:25.781198: predicting 14967Q_8_2_data 
2024-05-09 00:10:25.782299: 14967Q_8_2_data, shape torch.Size([1, 42, 190, 259]), rank 0 
2024-05-09 00:10:26.267276: predicting 14970F_4_3_data 
2024-05-09 00:10:26.268339: 14970F_4_3_data, shape torch.Size([1, 43, 195, 265]), rank 0 
2024-05-09 00:10:27.222323: predicting 14981K_0_0_data 
2024-05-09 00:10:27.223575: 14981K_0_0_data, shape torch.Size([1, 43, 190, 264]), rank 0 
2024-05-09 00:10:27.712259: predicting 14986U_16_2_data 
2024-05-09 00:10:27.713341: 14986U_16_2_data, shape torch.Size([1, 42, 198, 267]), rank 0 
2024-05-09 00:10:28.668253: predicting 14993R_4_3_data 
2024-05-09 00:10:28.669232: 14993R_4_3_data, shape torch.Size([1, 43, 195, 271]), rank 0 
2024-05-09 00:10:29.623763: predicting 14998B_16_2_data 
2024-05-09 00:10:29.624679: 14998B_16_2_data, shape torch.Size([1, 43, 195, 265]), rank 0 
2024-05-09 00:10:30.579217: predicting 15006R_8_2_data 
2024-05-09 00:10:30.580392: 15006R_8_2_data, shape torch.Size([1, 42, 194, 270]), rank 0 
2024-05-09 00:10:31.535125: predicting 15007T_16_2_data 
2024-05-09 00:10:31.536091: 15007T_16_2_data, shape torch.Size([1, 42, 190, 263]), rank 0 
2024-05-09 00:10:32.020225: predicting 15020L_4_2_data 
2024-05-09 00:10:32.021200: 15020L_4_2_data, shape torch.Size([1, 42, 188, 263]), rank 0 
2024-05-09 00:10:32.505793: predicting 15040R_8_3_data 
2024-05-09 00:10:32.506674: 15040R_8_3_data, shape torch.Size([1, 43, 192, 265]), rank 0 
2024-05-09 00:10:32.990711: predicting 15048H_16_2_data 
2024-05-09 00:10:32.991669: 15048H_16_2_data, shape torch.Size([1, 43, 189, 263]), rank 0 
2024-05-09 00:10:33.476408: predicting 15061Z_8_2_data 
2024-05-09 00:10:33.477479: 15061Z_8_2_data, shape torch.Size([1, 43, 198, 267]), rank 0 
2024-05-09 00:10:34.430915: predicting 15065H_16_2_data 
2024-05-09 00:10:34.431765: 15065H_16_2_data, shape torch.Size([1, 43, 186, 258]), rank 0 
2024-05-09 00:10:34.915644: predicting 15087R_8_3_data 
2024-05-09 00:10:34.916506: 15087R_8_3_data, shape torch.Size([1, 43, 190, 259]), rank 0 
2024-05-09 00:10:35.399976: predicting 15116Y_8_3_data 
2024-05-09 00:10:35.401053: 15116Y_8_3_data, shape torch.Size([1, 43, 194, 260]), rank 0 
2024-05-09 00:10:36.355300: predicting 15123V_16_2_data 
2024-05-09 00:10:36.356154: 15123V_16_2_data, shape torch.Size([1, 44, 196, 265]), rank 0 
2024-05-09 00:10:37.311115: predicting 15127D_8_2_data 
2024-05-09 00:10:37.312033: 15127D_8_2_data, shape torch.Size([1, 43, 193, 265]), rank 0 
2024-05-09 00:10:38.265803: predicting 15137G_8_2_data 
2024-05-09 00:10:38.267069: 15137G_8_2_data, shape torch.Size([1, 36, 192, 263]), rank 0 
2024-05-09 00:10:38.751238: predicting 15141X_4_3_data 
2024-05-09 00:10:38.752142: 15141X_4_3_data, shape torch.Size([1, 44, 199, 266]), rank 0 
2024-05-09 00:10:39.706784: predicting 15143B_4_2_data 
2024-05-09 00:10:39.707701: 15143B_4_2_data, shape torch.Size([1, 42, 201, 270]), rank 0 
2024-05-09 00:10:40.663517: predicting 15157M_4_2_data 
2024-05-09 00:10:40.664382: 15157M_4_2_data, shape torch.Size([1, 42, 194, 261]), rank 0 
2024-05-09 00:10:41.617959: predicting 15160B_4_2_data 
2024-05-09 00:10:41.619184: 15160B_4_2_data, shape torch.Size([1, 43, 193, 260]), rank 0 
2024-05-09 00:10:42.573345: predicting 15168R_16_2_data 
2024-05-09 00:10:42.574292: 15168R_16_2_data, shape torch.Size([1, 39, 196, 262]), rank 0 
2024-05-09 00:10:43.527557: predicting 15177S_4_3_data 
2024-05-09 00:10:43.528502: 15177S_4_3_data, shape torch.Size([1, 43, 194, 265]), rank 0 
2024-05-09 00:10:44.481100: predicting 15183N_8_3_data 
2024-05-09 00:10:44.482246: 15183N_8_3_data, shape torch.Size([1, 37, 190, 263]), rank 0 
2024-05-09 00:10:44.965076: predicting 15186T_0_0_data 
2024-05-09 00:10:44.965858: 15186T_0_0_data, shape torch.Size([1, 38, 194, 270]), rank 0 
2024-05-09 00:10:45.919577: predicting 15196W_8_2_data 
2024-05-09 00:10:45.920425: 15196W_8_2_data, shape torch.Size([1, 42, 190, 262]), rank 0 
2024-05-09 00:10:46.405546: predicting 15220T_4_2_data 
2024-05-09 00:10:46.406604: 15220T_4_2_data, shape torch.Size([1, 42, 193, 261]), rank 0 
2024-05-09 00:10:47.361184: predicting 15221V_16_2_data 
2024-05-09 00:10:47.362177: 15221V_16_2_data, shape torch.Size([1, 44, 191, 266]), rank 0 
2024-05-09 00:10:47.847028: predicting 15227H_16_2_data 
2024-05-09 00:10:47.847899: 15227H_16_2_data, shape torch.Size([1, 44, 188, 259]), rank 0 
2024-05-09 00:10:48.332165: predicting 15251E_16_2_data 
2024-05-09 00:10:48.333087: 15251E_16_2_data, shape torch.Size([1, 43, 193, 261]), rank 0 
2024-05-09 00:10:49.286218: predicting 15259U_4_3_data 
2024-05-09 00:10:49.287071: 15259U_4_3_data, shape torch.Size([1, 43, 186, 262]), rank 0 
2024-05-09 00:10:49.770815: predicting 15261H_4_2_data 
2024-05-09 00:10:49.771829: 15261H_4_2_data, shape torch.Size([1, 42, 189, 263]), rank 0 
2024-05-09 00:10:50.254398: predicting 15262J_8_3_data 
2024-05-09 00:10:50.255855: 15262J_8_3_data, shape torch.Size([1, 42, 181, 259]), rank 0 
2024-05-09 00:10:50.739634: predicting 15267T_4_3_data 
2024-05-09 00:10:50.740559: 15267T_4_3_data, shape torch.Size([1, 42, 189, 263]), rank 0 
2024-05-09 00:10:51.225757: predicting 15278Y_4_2_data 
2024-05-09 00:10:51.226661: 15278Y_4_2_data, shape torch.Size([1, 42, 198, 264]), rank 0 
2024-05-09 00:10:52.180942: predicting 15280L_8_2_data 
2024-05-09 00:10:52.181848: 15280L_8_2_data, shape torch.Size([1, 42, 181, 255]), rank 0 
2024-05-09 00:10:52.430217: predicting 15284T_4_2_data 
2024-05-09 00:10:52.431221: 15284T_4_2_data, shape torch.Size([1, 42, 200, 267]), rank 0 
2024-05-09 00:10:53.385385: predicting 15300R_16_2_data 
2024-05-09 00:10:53.386770: 15300R_16_2_data, shape torch.Size([1, 41, 196, 271]), rank 0 
2024-05-09 00:10:54.340027: predicting 15316G_0_0_data 
2024-05-09 00:10:54.340954: 15316G_0_0_data, shape torch.Size([1, 42, 185, 259]), rank 0 
2024-05-09 00:10:54.825941: predicting 15343J_4_3_data 
2024-05-09 00:10:54.826834: 15343J_4_3_data, shape torch.Size([1, 43, 181, 256]), rank 0 
2024-05-09 00:10:55.075155: predicting 15350G_16_2_data 
2024-05-09 00:10:55.076020: 15350G_16_2_data, shape torch.Size([1, 42, 177, 255]), rank 0 
2024-05-09 00:10:55.325459: predicting 15360J_8_3_data 
2024-05-09 00:10:55.326328: 15360J_8_3_data, shape torch.Size([1, 44, 194, 265]), rank 0 
2024-05-09 00:10:56.280286: predicting 15377A_8_3_data 
2024-05-09 00:10:56.281251: 15377A_8_3_data, shape torch.Size([1, 42, 189, 259]), rank 0 
2024-05-09 00:10:56.764376: predicting 15379E_4_2_data 
2024-05-09 00:10:56.765445: 15379E_4_2_data, shape torch.Size([1, 42, 181, 256]), rank 0 
2024-05-09 00:10:57.013714: predicting 15385Z_0_0_data 
2024-05-09 00:10:57.014662: 15385Z_0_0_data, shape torch.Size([1, 43, 193, 262]), rank 0 
2024-05-09 00:10:57.968699: predicting 15396E_8_2_data 
2024-05-09 00:10:57.969901: 15396E_8_2_data, shape torch.Size([1, 36, 195, 270]), rank 0 
2024-05-09 00:10:58.922360: predicting 15401X_8_3_data 
2024-05-09 00:10:58.923199: 15401X_8_3_data, shape torch.Size([1, 43, 196, 272]), rank 0 
2024-05-09 00:10:59.876295: predicting 15402Z_8_2_data 
2024-05-09 00:10:59.877352: 15402Z_8_2_data, shape torch.Size([1, 43, 193, 263]), rank 0 
2024-05-09 00:11:00.830340: predicting 15414G_0_0_data 
2024-05-09 00:11:00.831432: 15414G_0_0_data, shape torch.Size([1, 44, 189, 259]), rank 0 
2024-05-09 00:11:01.314410: predicting 15422F_16_2_data 
2024-05-09 00:11:01.315359: 15422F_16_2_data, shape torch.Size([1, 36, 189, 263]), rank 0 
2024-05-09 00:11:01.798516: predicting 15439W_4_3_data 
2024-05-09 00:11:01.799338: 15439W_4_3_data, shape torch.Size([1, 43, 194, 264]), rank 0 
2024-05-09 00:11:02.752150: predicting 15442L_4_3_data 
2024-05-09 00:11:02.753026: 15442L_4_3_data, shape torch.Size([1, 43, 193, 265]), rank 0 
2024-05-09 00:11:03.706205: predicting 15443N_8_2_data 
2024-05-09 00:11:03.707552: 15443N_8_2_data, shape torch.Size([1, 39, 182, 261]), rank 0 
2024-05-09 00:11:04.189244: predicting 15444P_16_2_data 
2024-05-09 00:11:04.190251: 15444P_16_2_data, shape torch.Size([1, 43, 202, 270]), rank 0 
2024-05-09 00:11:05.144821: predicting 15453Q_16_2_data 
2024-05-09 00:11:05.145892: 15453Q_16_2_data, shape torch.Size([1, 35, 190, 262]), rank 0 
2024-05-09 00:11:05.628001: predicting 15454S_4_2_data 
2024-05-09 00:11:05.628958: 15454S_4_2_data, shape torch.Size([1, 43, 194, 266]), rank 0 
2024-05-09 00:11:06.583516: predicting 15489L_8_3_data 
2024-05-09 00:11:06.584358: 15489L_8_3_data, shape torch.Size([1, 41, 182, 259]), rank 0 
2024-05-09 00:11:07.067722: predicting 15490W_4_2_data 
2024-05-09 00:11:07.068586: 15490W_4_2_data, shape torch.Size([1, 43, 195, 264]), rank 0 
2024-05-09 00:11:08.021988: predicting 15493C_16_2_data 
2024-05-09 00:11:08.022881: 15493C_16_2_data, shape torch.Size([1, 43, 194, 264]), rank 0 
2024-05-09 00:11:08.976914: predicting 15503F_0_0_data 
2024-05-09 00:11:08.978302: 15503F_0_0_data, shape torch.Size([1, 42, 189, 262]), rank 0 
2024-05-09 00:11:09.460116: predicting 15504H_0_0_data 
2024-05-09 00:11:09.461291: 15504H_0_0_data, shape torch.Size([1, 42, 189, 262]), rank 0 
2024-05-09 00:11:09.944539: predicting 15511E_16_2_data 
2024-05-09 00:11:09.945561: 15511E_16_2_data, shape torch.Size([1, 42, 194, 266]), rank 0 
2024-05-09 00:11:10.898964: predicting 15520F_4_3_data 
2024-05-09 00:11:10.899825: 15520F_4_3_data, shape torch.Size([1, 42, 199, 265]), rank 0 
2024-05-09 00:11:11.853026: predicting 15526R_0_0_data 
2024-05-09 00:11:11.853892: 15526R_0_0_data, shape torch.Size([1, 42, 197, 263]), rank 0 
2024-05-09 00:11:12.806523: predicting 15532M_0_0_data 
2024-05-09 00:11:12.807389: 15532M_0_0_data, shape torch.Size([1, 27, 181, 258]), rank 0 
2024-05-09 00:11:13.289476: predicting 15544T_4_2_data 
2024-05-09 00:11:13.290247: 15544T_4_2_data, shape torch.Size([1, 42, 196, 264]), rank 0 
2024-05-09 00:11:14.243186: predicting 15551Q_8_3_data 
2024-05-09 00:11:14.244083: 15551Q_8_3_data, shape torch.Size([1, 43, 186, 263]), rank 0 
2024-05-09 00:11:14.727728: predicting 15563X_16_2_data 
2024-05-09 00:11:14.728602: 15563X_16_2_data, shape torch.Size([1, 42, 189, 263]), rank 0 
2024-05-09 00:11:15.213339: predicting 15574C_0_0_data 
2024-05-09 00:11:15.214281: 15574C_0_0_data, shape torch.Size([1, 29, 190, 254]), rank 0 
2024-05-09 00:11:15.461955: predicting 15583D_16_2_data 
2024-05-09 00:11:15.462882: 15583D_16_2_data, shape torch.Size([1, 43, 190, 263]), rank 0 
2024-05-09 00:11:15.947209: predicting 15588N_0_0_data 
2024-05-09 00:11:15.948437: 15588N_0_0_data, shape torch.Size([1, 42, 188, 262]), rank 0 
2024-05-09 00:11:16.432095: predicting 15607R_8_3_data 
2024-05-09 00:11:16.432915: 15607R_8_3_data, shape torch.Size([1, 44, 186, 260]), rank 0 
2024-05-09 00:11:16.916765: predicting 15628Z_0_0_data 
2024-05-09 00:11:16.917794: 15628Z_0_0_data, shape torch.Size([1, 42, 188, 259]), rank 0 
2024-05-09 00:11:17.401819: predicting 15629B_4_3_data 
2024-05-09 00:11:17.402699: 15629B_4_3_data, shape torch.Size([1, 42, 190, 257]), rank 0 
2024-05-09 00:11:17.886278: predicting 15637A_4_3_data 
2024-05-09 00:11:17.887084: 15637A_4_3_data, shape torch.Size([1, 40, 190, 261]), rank 0 
2024-05-09 00:11:18.369453: predicting 15650S_16_2_data 
2024-05-09 00:11:18.370662: 15650S_16_2_data, shape torch.Size([1, 42, 196, 265]), rank 0 
2024-05-09 00:11:19.325620: predicting 15660V_16_2_data 
2024-05-09 00:11:19.326521: 15660V_16_2_data, shape torch.Size([1, 42, 189, 263]), rank 0 
2024-05-09 00:11:19.812800: predicting 15669N_4_2_data 
2024-05-09 00:11:19.813874: 15669N_4_2_data, shape torch.Size([1, 43, 196, 268]), rank 0 
2024-05-09 00:11:20.767923: predicting 15674G_4_2_data 
2024-05-09 00:11:20.768831: 15674G_4_2_data, shape torch.Size([1, 44, 187, 263]), rank 0 
2024-05-09 00:11:21.251865: predicting 15682F_16_2_data 
2024-05-09 00:11:21.252774: 15682F_16_2_data, shape torch.Size([1, 33, 193, 264]), rank 0 
2024-05-09 00:11:22.205140: predicting 15693K_8_3_data 
2024-05-09 00:11:22.206043: 15693K_8_3_data, shape torch.Size([1, 35, 195, 265]), rank 0 
2024-05-09 00:11:23.158656: predicting 15696Q_16_2_data 
2024-05-09 00:11:23.159433: 15696Q_16_2_data, shape torch.Size([1, 37, 192, 265]), rank 0 
2024-05-09 00:11:23.642877: predicting 15699W_8_2_data 
2024-05-09 00:11:23.643904: 15699W_8_2_data, shape torch.Size([1, 38, 193, 261]), rank 0 
2024-05-09 00:11:24.596908: predicting 15702L_16_2_data 
2024-05-09 00:11:24.597802: 15702L_16_2_data, shape torch.Size([1, 40, 196, 261]), rank 0 
2024-05-09 00:11:25.551857: predicting 15721P_16_2_data 
2024-05-09 00:11:25.552883: 15721P_16_2_data, shape torch.Size([1, 43, 189, 264]), rank 0 
2024-05-09 00:11:26.036053: predicting 15726Z_8_2_data 
2024-05-09 00:11:26.037485: 15726Z_8_2_data, shape torch.Size([1, 33, 185, 264]), rank 0 
2024-05-09 00:11:26.522089: predicting 15727B_0_0_data 
2024-05-09 00:11:26.522896: 15727B_0_0_data, shape torch.Size([1, 33, 185, 264]), rank 0 
2024-05-09 00:11:27.004837: predicting 15730Q_8_2_data 
2024-05-09 00:11:27.005861: 15730Q_8_2_data, shape torch.Size([1, 42, 214, 266]), rank 0 
2024-05-09 00:11:27.961179: predicting 15738G_8_3_data 
2024-05-09 00:11:27.962259: 15738G_8_3_data, shape torch.Size([1, 42, 187, 260]), rank 0 
2024-05-09 00:11:28.445233: predicting 15762D_0_0_data 
2024-05-09 00:11:28.446252: 15762D_0_0_data, shape torch.Size([1, 34, 193, 257]), rank 0 
2024-05-09 00:11:29.398464: predicting 15765J_4_2_data 
2024-05-09 00:11:29.399541: 15765J_4_2_data, shape torch.Size([1, 43, 194, 265]), rank 0 
2024-05-09 00:11:30.353363: predicting 15770C_16_2_data 
2024-05-09 00:11:30.354251: 15770C_16_2_data, shape torch.Size([1, 42, 192, 265]), rank 0 
2024-05-09 00:11:30.836546: predicting 15808B_4_2_data 
2024-05-09 00:11:30.837609: 15808B_4_2_data, shape torch.Size([1, 42, 194, 265]), rank 0 
2024-05-09 00:11:31.790823: predicting 15819G_16_2_data 
2024-05-09 00:11:31.792153: 15819G_16_2_data, shape torch.Size([1, 42, 181, 260]), rank 0 
2024-05-09 00:11:32.274830: predicting 15824Z_8_3_data 
2024-05-09 00:11:32.275697: 15824Z_8_3_data, shape torch.Size([1, 43, 194, 266]), rank 0 
2024-05-09 00:11:33.231335: predicting 15826D_0_0_data 
2024-05-09 00:11:33.232262: 15826D_0_0_data, shape torch.Size([1, 43, 194, 266]), rank 0 
2024-05-09 00:11:34.186435: predicting 15850A_0_0_data 
2024-05-09 00:11:34.187459: 15850A_0_0_data, shape torch.Size([1, 44, 188, 259]), rank 0 
2024-05-09 00:11:34.671098: predicting 15852E_0_0_data 
2024-05-09 00:11:34.672012: 15852E_0_0_data, shape torch.Size([1, 44, 188, 259]), rank 0 
2024-05-09 00:11:35.156334: predicting 15853G_16_2_data 
2024-05-09 00:11:35.157332: 15853G_16_2_data, shape torch.Size([1, 40, 190, 261]), rank 0 
2024-05-09 00:11:35.640205: predicting 15882N_16_2_data 
2024-05-09 00:11:35.641234: 15882N_16_2_data, shape torch.Size([1, 35, 184, 262]), rank 0 
2024-05-09 00:11:36.124722: predicting 15885T_16_2_data 
2024-05-09 00:11:36.125554: 15885T_16_2_data, shape torch.Size([1, 35, 197, 269]), rank 0 
