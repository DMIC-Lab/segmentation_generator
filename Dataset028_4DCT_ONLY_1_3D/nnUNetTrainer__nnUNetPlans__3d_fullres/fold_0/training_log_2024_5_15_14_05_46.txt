
#######################################################################
Please cite the following paper when using nnU-Net:
Isensee, F., Jaeger, P. F., Kohl, S. A., Petersen, J., & Maier-Hein, K. H. (2021). nnU-Net: a self-configuring method for deep learning-based biomedical image segmentation. Nature methods, 18(2), 203-211.
#######################################################################
 

This is the configuration used by this training:
Configuration name: 3d_fullres
 {'data_identifier': 'nnUNetPlans_3d_fullres', 'preprocessor_name': 'DefaultPreprocessor', 'batch_size': 2, 'patch_size': [48, 192, 256], 'median_image_size_in_voxels': [46.0, 174.0, 253.0], 'spacing': [1.0, 1.0, 1.0], 'normalization_schemes': ['CTNormalization'], 'use_mask_for_norm': [False], 'resampling_fn_data': 'resample_data_or_seg_to_shape', 'resampling_fn_seg': 'resample_data_or_seg_to_shape', 'resampling_fn_data_kwargs': {'is_seg': False, 'order': 3, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_seg_kwargs': {'is_seg': True, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_probabilities': 'resample_data_or_seg_to_shape', 'resampling_fn_probabilities_kwargs': {'is_seg': False, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'architecture': {'network_class_name': 'dynamic_network_architectures.architectures.unet.PlainConvUNet', 'arch_kwargs': {'n_stages': 6, 'features_per_stage': [32, 64, 128, 256, 320, 320], 'conv_op': 'torch.nn.modules.conv.Conv3d', 'kernel_sizes': [[3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3]], 'strides': [[1, 1, 1], [2, 2, 2], [2, 2, 2], [2, 2, 2], [1, 2, 2], [1, 2, 2]], 'n_conv_per_stage': [2, 2, 2, 2, 2, 2], 'n_conv_per_stage_decoder': [2, 2, 2, 2, 2], 'conv_bias': True, 'norm_op': 'torch.nn.modules.instancenorm.InstanceNorm3d', 'norm_op_kwargs': {'eps': 1e-05, 'affine': True}, 'dropout_op': None, 'dropout_op_kwargs': None, 'nonlin': 'torch.nn.LeakyReLU', 'nonlin_kwargs': {'inplace': True}, 'deep_supervision': True}, '_kw_requires_import': ['conv_op', 'norm_op', 'dropout_op', 'nonlin']}, 'batch_dice': False} 
 
These are the global plan.json settings:
 {'dataset_name': 'Dataset028_4DCT_ONLY_1_3D', 'plans_name': 'nnUNetPlans', 'original_median_spacing_after_transp': [1.0, 1.0, 1.0], 'original_median_shape_after_transp': [46, 174, 253], 'image_reader_writer': 'SimpleITKIO', 'transpose_forward': [0, 1, 2], 'transpose_backward': [0, 1, 2], 'experiment_planner_used': 'ExperimentPlanner', 'label_manager': 'LabelManager', 'foreground_intensity_properties_per_channel': {'0': {'max': 1.0, 'mean': 0.49756415650771285, 'median': 0.5285958051681519, 'min': 0.0, 'percentile_00_5': 0.10679470747709274, 'percentile_99_5': 0.8809682130813599, 'std': 0.1428699312680128}}} 
 
2024-05-15 14:05:48.437323: unpacking dataset... 
2024-05-15 14:05:51.843478: unpacking done... 
2024-05-15 14:05:51.844206: do_dummy_2d_data_aug: True 
2024-05-15 14:05:51.845234: Using splits from existing split file: /raid/dataset/nnUNet_preprocessed/Dataset028_4DCT_ONLY_1_3D/splits_final.json 
2024-05-15 14:05:51.845665: The split file contains 5 splits. 
2024-05-15 14:05:51.845701: Desired fold for training: 0 
2024-05-15 14:05:51.845727: This split has 118 training and 30 validation cases. 
2024-05-15 14:05:51.924659: Unable to plot network architecture: 
2024-05-15 14:05:51.924745: No module named 'hiddenlayer' 
2024-05-15 14:05:52.000135:  
2024-05-15 14:05:52.000216: Epoch 900 
2024-05-15 14:05:52.000346: Current learning rate: 0.00126 
2024-05-15 14:06:39.534049: train_loss -0.6909 
2024-05-15 14:06:39.534257: val_loss -0.6273 
2024-05-15 14:06:39.534308: Pseudo dice [0.8165] 
2024-05-15 14:06:39.534369: Epoch time: 47.54 s 
2024-05-15 14:06:40.703364:  
2024-05-15 14:06:40.703499: Epoch 901 
2024-05-15 14:06:40.703590: Current learning rate: 0.00125 
2024-05-15 14:07:21.260728: train_loss -0.6976 
2024-05-15 14:07:21.260901: val_loss -0.4977 
2024-05-15 14:07:21.260966: Pseudo dice [0.7768] 
2024-05-15 14:07:21.261018: Epoch time: 40.56 s 
2024-05-15 14:07:22.567141:  
2024-05-15 14:07:22.567280: Epoch 902 
2024-05-15 14:07:22.567390: Current learning rate: 0.00124 
2024-05-15 14:08:03.192488: train_loss -0.6931 
2024-05-15 14:08:03.192666: val_loss -0.5328 
2024-05-15 14:08:03.192713: Pseudo dice [0.7711] 
2024-05-15 14:08:03.192765: Epoch time: 40.63 s 
2024-05-15 14:08:04.288537:  
2024-05-15 14:08:04.288668: Epoch 903 
2024-05-15 14:08:04.288756: Current learning rate: 0.00122 
2024-05-15 14:08:44.966579: train_loss -0.7139 
2024-05-15 14:08:44.966754: val_loss -0.5147 
2024-05-15 14:08:44.966822: Pseudo dice [0.7821] 
2024-05-15 14:08:44.966875: Epoch time: 40.68 s 
2024-05-15 14:08:46.064552:  
2024-05-15 14:08:46.064692: Epoch 904 
2024-05-15 14:08:46.064790: Current learning rate: 0.00121 
2024-05-15 14:09:26.747886: train_loss -0.6924 
2024-05-15 14:09:26.748063: val_loss -0.5736 
2024-05-15 14:09:26.748112: Pseudo dice [0.7903] 
2024-05-15 14:09:26.748164: Epoch time: 40.68 s 
2024-05-15 14:09:27.851640:  
2024-05-15 14:09:27.851978: Epoch 905 
2024-05-15 14:09:27.852074: Current learning rate: 0.0012 
2024-05-15 14:10:08.611718: train_loss -0.6952 
2024-05-15 14:10:08.611950: val_loss -0.519 
2024-05-15 14:10:08.612030: Pseudo dice [0.7608] 
2024-05-15 14:10:08.612084: Epoch time: 40.76 s 
2024-05-15 14:10:09.713516:  
2024-05-15 14:10:09.713724: Epoch 906 
2024-05-15 14:10:09.713817: Current learning rate: 0.00119 
2024-05-15 14:10:50.465025: train_loss -0.7223 
2024-05-15 14:10:50.465199: val_loss -0.5799 
2024-05-15 14:10:50.465247: Pseudo dice [0.808] 
2024-05-15 14:10:50.465302: Epoch time: 40.75 s 
2024-05-15 14:10:51.567257:  
2024-05-15 14:10:51.567388: Epoch 907 
2024-05-15 14:10:51.567478: Current learning rate: 0.00118 
2024-05-15 14:11:32.320389: train_loss -0.7089 
2024-05-15 14:11:32.320565: val_loss -0.6031 
2024-05-15 14:11:32.320612: Pseudo dice [0.7737] 
2024-05-15 14:11:32.320664: Epoch time: 40.75 s 
2024-05-15 14:11:33.420720:  
2024-05-15 14:11:33.420847: Epoch 908 
2024-05-15 14:11:33.420937: Current learning rate: 0.00117 
2024-05-15 14:12:14.150525: train_loss -0.6983 
2024-05-15 14:12:14.150708: val_loss -0.4815 
2024-05-15 14:12:14.150756: Pseudo dice [0.7617] 
2024-05-15 14:12:14.150809: Epoch time: 40.73 s 
2024-05-15 14:12:15.461582:  
2024-05-15 14:12:15.461722: Epoch 909 
2024-05-15 14:12:15.461816: Current learning rate: 0.00116 
2024-05-15 14:12:56.260898: train_loss -0.6736 
2024-05-15 14:12:56.261076: val_loss -0.5135 
2024-05-15 14:12:56.261123: Pseudo dice [0.7711] 
2024-05-15 14:12:56.261176: Epoch time: 40.8 s 
2024-05-15 14:12:57.357063:  
2024-05-15 14:12:57.357197: Epoch 910 
2024-05-15 14:12:57.357289: Current learning rate: 0.00115 
2024-05-15 14:13:38.179560: train_loss -0.6775 
2024-05-15 14:13:38.179735: val_loss -0.5784 
2024-05-15 14:13:38.179782: Pseudo dice [0.7984] 
2024-05-15 14:13:38.179834: Epoch time: 40.82 s 
2024-05-15 14:13:39.295309:  
2024-05-15 14:13:39.295443: Epoch 911 
2024-05-15 14:13:39.295534: Current learning rate: 0.00113 
2024-05-15 14:14:20.055293: train_loss -0.6994 
2024-05-15 14:14:20.055471: val_loss -0.5455 
2024-05-15 14:14:20.055518: Pseudo dice [0.7763] 
2024-05-15 14:14:20.055570: Epoch time: 40.76 s 
2024-05-15 14:14:21.145885:  
2024-05-15 14:14:21.146056: Epoch 912 
2024-05-15 14:14:21.146150: Current learning rate: 0.00112 
2024-05-15 14:15:01.892756: train_loss -0.707 
2024-05-15 14:15:01.892931: val_loss -0.5766 
2024-05-15 14:15:01.892980: Pseudo dice [0.797] 
2024-05-15 14:15:01.893032: Epoch time: 40.75 s 
2024-05-15 14:15:02.984174:  
2024-05-15 14:15:02.984305: Epoch 913 
2024-05-15 14:15:02.984394: Current learning rate: 0.00111 
2024-05-15 14:15:43.744300: train_loss -0.71 
2024-05-15 14:15:43.744473: val_loss -0.5292 
2024-05-15 14:15:43.744519: Pseudo dice [0.7803] 
2024-05-15 14:15:43.744571: Epoch time: 40.76 s 
2024-05-15 14:15:44.842667:  
2024-05-15 14:15:44.842805: Epoch 914 
2024-05-15 14:15:44.842904: Current learning rate: 0.0011 
2024-05-15 14:16:25.581154: train_loss -0.7152 
2024-05-15 14:16:25.581332: val_loss -0.571 
2024-05-15 14:16:25.581380: Pseudo dice [0.7854] 
2024-05-15 14:16:25.581432: Epoch time: 40.74 s 
2024-05-15 14:16:26.669214:  
2024-05-15 14:16:26.669403: Epoch 915 
2024-05-15 14:16:26.669517: Current learning rate: 0.00109 
2024-05-15 14:17:07.355870: train_loss -0.7296 
2024-05-15 14:17:07.356045: val_loss -0.5029 
2024-05-15 14:17:07.356093: Pseudo dice [0.789] 
2024-05-15 14:17:07.356147: Epoch time: 40.69 s 
2024-05-15 14:17:08.635618:  
2024-05-15 14:17:08.635759: Epoch 916 
2024-05-15 14:17:08.635859: Current learning rate: 0.00108 
2024-05-15 14:17:49.322327: train_loss -0.7 
2024-05-15 14:17:49.322503: val_loss -0.6142 
2024-05-15 14:17:49.322551: Pseudo dice [0.7934] 
2024-05-15 14:17:49.322603: Epoch time: 40.69 s 
2024-05-15 14:17:50.409817:  
2024-05-15 14:17:50.410012: Epoch 917 
2024-05-15 14:17:50.410103: Current learning rate: 0.00106 
2024-05-15 14:18:31.028136: train_loss -0.679 
2024-05-15 14:18:31.028317: val_loss -0.584 
2024-05-15 14:18:31.028365: Pseudo dice [0.7867] 
2024-05-15 14:18:31.028416: Epoch time: 40.62 s 
2024-05-15 14:18:32.115925:  
2024-05-15 14:18:32.116058: Epoch 918 
2024-05-15 14:18:32.116159: Current learning rate: 0.00105 
2024-05-15 14:19:12.743160: train_loss -0.6953 
2024-05-15 14:19:12.743340: val_loss -0.5962 
2024-05-15 14:19:12.743386: Pseudo dice [0.7783] 
2024-05-15 14:19:12.743438: Epoch time: 40.63 s 
2024-05-15 14:19:13.833173:  
2024-05-15 14:19:13.833308: Epoch 919 
2024-05-15 14:19:13.833398: Current learning rate: 0.00104 
2024-05-15 14:19:54.459866: train_loss -0.7108 
2024-05-15 14:19:54.460038: val_loss -0.518 
2024-05-15 14:19:54.460087: Pseudo dice [0.7886] 
2024-05-15 14:19:54.460139: Epoch time: 40.63 s 
2024-05-15 14:19:55.543328:  
2024-05-15 14:19:55.543542: Epoch 920 
2024-05-15 14:19:55.543637: Current learning rate: 0.00103 
2024-05-15 14:20:36.188817: train_loss -0.7164 
2024-05-15 14:20:36.189000: val_loss -0.5137 
2024-05-15 14:20:36.189048: Pseudo dice [0.7783] 
2024-05-15 14:20:36.189100: Epoch time: 40.65 s 
2024-05-15 14:20:37.272746:  
2024-05-15 14:20:37.272883: Epoch 921 
2024-05-15 14:20:37.272985: Current learning rate: 0.00102 
2024-05-15 14:21:17.961871: train_loss -0.7018 
2024-05-15 14:21:17.962045: val_loss -0.5574 
2024-05-15 14:21:17.962093: Pseudo dice [0.7974] 
2024-05-15 14:21:17.962144: Epoch time: 40.69 s 
2024-05-15 14:21:19.046023:  
2024-05-15 14:21:19.046243: Epoch 922 
2024-05-15 14:21:19.046338: Current learning rate: 0.00101 
2024-05-15 14:21:59.778699: train_loss -0.7562 
2024-05-15 14:21:59.778873: val_loss -0.5173 
2024-05-15 14:21:59.778921: Pseudo dice [0.7658] 
2024-05-15 14:21:59.778973: Epoch time: 40.73 s 
2024-05-15 14:22:01.055718:  
2024-05-15 14:22:01.055931: Epoch 923 
2024-05-15 14:22:01.056048: Current learning rate: 0.001 
2024-05-15 14:22:41.741222: train_loss -0.7153 
2024-05-15 14:22:41.741398: val_loss -0.5393 
2024-05-15 14:22:41.741445: Pseudo dice [0.7679] 
2024-05-15 14:22:41.741499: Epoch time: 40.69 s 
2024-05-15 14:22:42.826347:  
2024-05-15 14:22:42.826539: Epoch 924 
2024-05-15 14:22:42.826633: Current learning rate: 0.00098 
2024-05-15 14:23:23.473073: train_loss -0.6967 
2024-05-15 14:23:23.473348: val_loss -0.572 
2024-05-15 14:23:23.473399: Pseudo dice [0.8057] 
2024-05-15 14:23:23.473450: Epoch time: 40.65 s 
2024-05-15 14:23:24.555959:  
2024-05-15 14:23:24.556097: Epoch 925 
2024-05-15 14:23:24.556188: Current learning rate: 0.00097 
2024-05-15 14:24:05.239632: train_loss -0.7001 
2024-05-15 14:24:05.239815: val_loss -0.5022 
2024-05-15 14:24:05.239862: Pseudo dice [0.7678] 
2024-05-15 14:24:05.239916: Epoch time: 40.68 s 
2024-05-15 14:24:06.519122:  
2024-05-15 14:24:06.519257: Epoch 926 
2024-05-15 14:24:06.519349: Current learning rate: 0.00096 
2024-05-15 14:24:47.206955: train_loss -0.6741 
2024-05-15 14:24:47.207130: val_loss -0.5434 
2024-05-15 14:24:47.207177: Pseudo dice [0.7844] 
2024-05-15 14:24:47.207228: Epoch time: 40.69 s 
2024-05-15 14:24:48.293635:  
2024-05-15 14:24:48.293766: Epoch 927 
2024-05-15 14:24:48.293858: Current learning rate: 0.00095 
2024-05-15 14:25:29.007675: train_loss -0.6976 
2024-05-15 14:25:29.007850: val_loss -0.5658 
2024-05-15 14:25:29.007909: Pseudo dice [0.7781] 
2024-05-15 14:25:29.007962: Epoch time: 40.72 s 
2024-05-15 14:25:30.098054:  
2024-05-15 14:25:30.098236: Epoch 928 
2024-05-15 14:25:30.098330: Current learning rate: 0.00094 
2024-05-15 14:26:10.730079: train_loss -0.6887 
2024-05-15 14:26:10.730253: val_loss -0.5636 
2024-05-15 14:26:10.730300: Pseudo dice [0.7968] 
2024-05-15 14:26:10.730350: Epoch time: 40.63 s 
2024-05-15 14:26:11.995359:  
2024-05-15 14:26:11.995501: Epoch 929 
2024-05-15 14:26:11.995594: Current learning rate: 0.00092 
2024-05-15 14:26:52.651876: train_loss -0.7132 
2024-05-15 14:26:52.652058: val_loss -0.525 
2024-05-15 14:26:52.652105: Pseudo dice [0.784] 
2024-05-15 14:26:52.652156: Epoch time: 40.66 s 
2024-05-15 14:26:53.740918:  
2024-05-15 14:26:53.741052: Epoch 930 
2024-05-15 14:26:53.741141: Current learning rate: 0.00091 
2024-05-15 14:27:34.420143: train_loss -0.7099 
2024-05-15 14:27:34.420317: val_loss -0.5334 
2024-05-15 14:27:34.420365: Pseudo dice [0.7691] 
2024-05-15 14:27:34.420415: Epoch time: 40.68 s 
2024-05-15 14:27:35.525557:  
2024-05-15 14:27:35.525690: Epoch 931 
2024-05-15 14:27:35.525779: Current learning rate: 0.0009 
2024-05-15 14:28:16.172888: train_loss -0.6955 
2024-05-15 14:28:16.173061: val_loss -0.5481 
2024-05-15 14:28:16.173109: Pseudo dice [0.7814] 
2024-05-15 14:28:16.173162: Epoch time: 40.65 s 
2024-05-15 14:28:17.266700:  
2024-05-15 14:28:17.266833: Epoch 932 
2024-05-15 14:28:17.266925: Current learning rate: 0.00089 
2024-05-15 14:28:57.928689: train_loss -0.7138 
2024-05-15 14:28:57.928870: val_loss -0.5571 
2024-05-15 14:28:57.928919: Pseudo dice [0.7717] 
2024-05-15 14:28:57.928971: Epoch time: 40.66 s 
2024-05-15 14:28:59.025817:  
2024-05-15 14:28:59.025956: Epoch 933 
2024-05-15 14:28:59.026048: Current learning rate: 0.00088 
2024-05-15 14:29:39.696227: train_loss -0.6983 
2024-05-15 14:29:39.696405: val_loss -0.5616 
2024-05-15 14:29:39.696453: Pseudo dice [0.7759] 
2024-05-15 14:29:39.696505: Epoch time: 40.67 s 
2024-05-15 14:29:40.791697:  
2024-05-15 14:29:40.791838: Epoch 934 
2024-05-15 14:29:40.791940: Current learning rate: 0.00087 
2024-05-15 14:30:21.515229: train_loss -0.7247 
2024-05-15 14:30:21.515407: val_loss -0.5233 
2024-05-15 14:30:21.515455: Pseudo dice [0.7845] 
2024-05-15 14:30:21.515506: Epoch time: 40.72 s 
2024-05-15 14:30:22.602315:  
2024-05-15 14:30:22.602457: Epoch 935 
2024-05-15 14:30:22.602546: Current learning rate: 0.00085 
2024-05-15 14:31:03.296488: train_loss -0.6873 
2024-05-15 14:31:03.296772: val_loss -0.5508 
2024-05-15 14:31:03.296820: Pseudo dice [0.7743] 
2024-05-15 14:31:03.296873: Epoch time: 40.7 s 
2024-05-15 14:31:04.574372:  
2024-05-15 14:31:04.574511: Epoch 936 
2024-05-15 14:31:04.574605: Current learning rate: 0.00084 
2024-05-15 14:31:45.243315: train_loss -0.7018 
2024-05-15 14:31:45.243498: val_loss -0.5647 
2024-05-15 14:31:45.243555: Pseudo dice [0.7965] 
2024-05-15 14:31:45.243617: Epoch time: 40.67 s 
2024-05-15 14:31:46.336872:  
2024-05-15 14:31:46.337072: Epoch 937 
2024-05-15 14:31:46.337167: Current learning rate: 0.00083 
2024-05-15 14:32:26.988389: train_loss -0.6988 
2024-05-15 14:32:26.988563: val_loss -0.5595 
2024-05-15 14:32:26.988611: Pseudo dice [0.7844] 
2024-05-15 14:32:26.988662: Epoch time: 40.65 s 
2024-05-15 14:32:28.083898:  
2024-05-15 14:32:28.084042: Epoch 938 
2024-05-15 14:32:28.084144: Current learning rate: 0.00082 
2024-05-15 14:33:08.719598: train_loss -0.7157 
2024-05-15 14:33:08.719776: val_loss -0.5236 
2024-05-15 14:33:08.719822: Pseudo dice [0.7673] 
2024-05-15 14:33:08.719874: Epoch time: 40.64 s 
2024-05-15 14:33:09.808814:  
2024-05-15 14:33:09.808953: Epoch 939 
2024-05-15 14:33:09.809043: Current learning rate: 0.00081 
2024-05-15 14:33:50.479739: train_loss -0.7295 
2024-05-15 14:33:50.479916: val_loss -0.5488 
2024-05-15 14:33:50.479964: Pseudo dice [0.7996] 
2024-05-15 14:33:50.480016: Epoch time: 40.67 s 
2024-05-15 14:33:51.565552:  
2024-05-15 14:33:51.565744: Epoch 940 
2024-05-15 14:33:51.565836: Current learning rate: 0.00079 
2024-05-15 14:34:32.321556: train_loss -0.7002 
2024-05-15 14:34:32.321728: val_loss -0.5423 
2024-05-15 14:34:32.321777: Pseudo dice [0.7908] 
2024-05-15 14:34:32.321829: Epoch time: 40.76 s 
2024-05-15 14:34:33.410905:  
2024-05-15 14:34:33.411035: Epoch 941 
2024-05-15 14:34:33.411127: Current learning rate: 0.00078 
2024-05-15 14:35:14.201657: train_loss -0.728 
2024-05-15 14:35:14.201833: val_loss -0.4909 
2024-05-15 14:35:14.201881: Pseudo dice [0.7621] 
2024-05-15 14:35:14.201933: Epoch time: 40.79 s 
2024-05-15 14:35:15.300939:  
2024-05-15 14:35:15.301067: Epoch 942 
2024-05-15 14:35:15.301156: Current learning rate: 0.00077 
2024-05-15 14:35:56.079885: train_loss -0.6672 
2024-05-15 14:35:56.080063: val_loss -0.5319 
2024-05-15 14:35:56.080110: Pseudo dice [0.7759] 
2024-05-15 14:35:56.080162: Epoch time: 40.78 s 
2024-05-15 14:35:57.353793:  
2024-05-15 14:35:57.353933: Epoch 943 
2024-05-15 14:35:57.354022: Current learning rate: 0.00076 
2024-05-15 14:36:38.199278: train_loss -0.7092 
2024-05-15 14:36:38.199466: val_loss -0.5363 
2024-05-15 14:36:38.199514: Pseudo dice [0.7945] 
2024-05-15 14:36:38.199566: Epoch time: 40.85 s 
2024-05-15 14:36:39.297412:  
2024-05-15 14:36:39.297550: Epoch 944 
2024-05-15 14:36:39.297647: Current learning rate: 0.00075 
2024-05-15 14:37:20.149033: train_loss -0.7081 
2024-05-15 14:37:20.149214: val_loss -0.563 
2024-05-15 14:37:20.149264: Pseudo dice [0.8055] 
2024-05-15 14:37:20.149316: Epoch time: 40.85 s 
2024-05-15 14:37:21.240170:  
2024-05-15 14:37:21.240377: Epoch 945 
2024-05-15 14:37:21.240476: Current learning rate: 0.00074 
2024-05-15 14:38:02.082799: train_loss -0.7042 
2024-05-15 14:38:02.082980: val_loss -0.4689 
2024-05-15 14:38:02.083029: Pseudo dice [0.7736] 
2024-05-15 14:38:02.083081: Epoch time: 40.84 s 
2024-05-15 14:38:03.176064:  
2024-05-15 14:38:03.176211: Epoch 946 
2024-05-15 14:38:03.176314: Current learning rate: 0.00072 
2024-05-15 14:38:44.050858: train_loss -0.7142 
2024-05-15 14:38:44.051041: val_loss -0.538 
2024-05-15 14:38:44.051089: Pseudo dice [0.7853] 
2024-05-15 14:38:44.051141: Epoch time: 40.88 s 
2024-05-15 14:38:45.152084:  
2024-05-15 14:38:45.152225: Epoch 947 
2024-05-15 14:38:45.152318: Current learning rate: 0.00071 
2024-05-15 14:39:26.044029: train_loss -0.6896 
2024-05-15 14:39:26.044203: val_loss -0.5125 
2024-05-15 14:39:26.044250: Pseudo dice [0.7712] 
2024-05-15 14:39:26.044302: Epoch time: 40.89 s 
2024-05-15 14:39:27.183730:  
2024-05-15 14:39:27.183864: Epoch 948 
2024-05-15 14:39:27.183956: Current learning rate: 0.0007 
2024-05-15 14:40:08.079073: train_loss -0.7138 
2024-05-15 14:40:08.079253: val_loss -0.522 
2024-05-15 14:40:08.079301: Pseudo dice [0.7753] 
2024-05-15 14:40:08.079353: Epoch time: 40.9 s 
2024-05-15 14:40:09.167124:  
2024-05-15 14:40:09.167458: Epoch 949 
2024-05-15 14:40:09.167553: Current learning rate: 0.00069 
2024-05-15 14:40:50.051962: train_loss -0.7141 
2024-05-15 14:40:50.052142: val_loss -0.6053 
2024-05-15 14:40:50.052191: Pseudo dice [0.8153] 
2024-05-15 14:40:50.052243: Epoch time: 40.89 s 
2024-05-15 14:40:51.664524:  
2024-05-15 14:40:51.664734: Epoch 950 
2024-05-15 14:40:51.664885: Current learning rate: 0.00067 
2024-05-15 14:41:32.500992: train_loss -0.6818 
2024-05-15 14:41:32.501176: val_loss -0.5642 
2024-05-15 14:41:32.501223: Pseudo dice [0.8029] 
2024-05-15 14:41:32.501275: Epoch time: 40.84 s 
2024-05-15 14:41:33.596117:  
2024-05-15 14:41:33.596301: Epoch 951 
2024-05-15 14:41:33.596398: Current learning rate: 0.00066 
2024-05-15 14:42:14.433193: train_loss -0.6856 
2024-05-15 14:42:14.433373: val_loss -0.5301 
2024-05-15 14:42:14.433421: Pseudo dice [0.7961] 
2024-05-15 14:42:14.433472: Epoch time: 40.84 s 
2024-05-15 14:42:15.534904:  
2024-05-15 14:42:15.535058: Epoch 952 
2024-05-15 14:42:15.535150: Current learning rate: 0.00065 
2024-05-15 14:42:56.400966: train_loss -0.7118 
2024-05-15 14:42:56.401150: val_loss -0.5194 
2024-05-15 14:42:56.401194: Pseudo dice [0.7713] 
2024-05-15 14:42:56.401245: Epoch time: 40.87 s 
2024-05-15 14:42:57.521306:  
2024-05-15 14:42:57.521450: Epoch 953 
2024-05-15 14:42:57.521544: Current learning rate: 0.00064 
2024-05-15 14:43:38.351172: train_loss -0.6989 
2024-05-15 14:43:38.351350: val_loss -0.5588 
2024-05-15 14:43:38.351397: Pseudo dice [0.7915] 
2024-05-15 14:43:38.351447: Epoch time: 40.83 s 
2024-05-15 14:43:39.460415:  
2024-05-15 14:43:39.460556: Epoch 954 
2024-05-15 14:43:39.460649: Current learning rate: 0.00063 
2024-05-15 14:44:20.317904: train_loss -0.7228 
2024-05-15 14:44:20.318117: val_loss -0.5489 
2024-05-15 14:44:20.318226: Pseudo dice [0.7873] 
2024-05-15 14:44:20.318283: Epoch time: 40.86 s 
2024-05-15 14:44:21.435362:  
2024-05-15 14:44:21.435494: Epoch 955 
2024-05-15 14:44:21.435586: Current learning rate: 0.00061 
2024-05-15 14:45:02.319709: train_loss -0.7014 
2024-05-15 14:45:02.319888: val_loss -0.5575 
2024-05-15 14:45:02.319938: Pseudo dice [0.7766] 
2024-05-15 14:45:02.319989: Epoch time: 40.89 s 
2024-05-15 14:45:03.628845:  
2024-05-15 14:45:03.628991: Epoch 956 
2024-05-15 14:45:03.629081: Current learning rate: 0.0006 
2024-05-15 14:45:44.570067: train_loss -0.6933 
2024-05-15 14:45:44.570238: val_loss -0.5186 
2024-05-15 14:45:44.570286: Pseudo dice [0.7798] 
2024-05-15 14:45:44.570338: Epoch time: 40.94 s 
2024-05-15 14:45:45.685508:  
2024-05-15 14:45:45.685655: Epoch 957 
2024-05-15 14:45:45.685748: Current learning rate: 0.00059 
2024-05-15 14:46:26.634970: train_loss -0.7088 
2024-05-15 14:46:26.635153: val_loss -0.5809 
2024-05-15 14:46:26.635200: Pseudo dice [0.7667] 
2024-05-15 14:46:26.635252: Epoch time: 40.95 s 
2024-05-15 14:46:27.749679:  
2024-05-15 14:46:27.749827: Epoch 958 
2024-05-15 14:46:27.749918: Current learning rate: 0.00058 
2024-05-15 14:47:08.617945: train_loss -0.7053 
2024-05-15 14:47:08.618122: val_loss -0.5631 
2024-05-15 14:47:08.618170: Pseudo dice [0.8052] 
2024-05-15 14:47:08.618222: Epoch time: 40.87 s 
2024-05-15 14:47:09.729345:  
2024-05-15 14:47:09.729486: Epoch 959 
2024-05-15 14:47:09.729576: Current learning rate: 0.00056 
2024-05-15 14:47:50.454346: train_loss -0.7007 
2024-05-15 14:47:50.454531: val_loss -0.5631 
2024-05-15 14:47:50.454580: Pseudo dice [0.7938] 
2024-05-15 14:47:50.454632: Epoch time: 40.73 s 
2024-05-15 14:47:51.557963:  
2024-05-15 14:47:51.558150: Epoch 960 
2024-05-15 14:47:51.558245: Current learning rate: 0.00055 
2024-05-15 14:48:32.311423: train_loss -0.7267 
2024-05-15 14:48:32.311600: val_loss -0.5397 
2024-05-15 14:48:32.311647: Pseudo dice [0.7799] 
2024-05-15 14:48:32.311699: Epoch time: 40.75 s 
2024-05-15 14:48:33.417162:  
2024-05-15 14:48:33.417365: Epoch 961 
2024-05-15 14:48:33.417472: Current learning rate: 0.00054 
2024-05-15 14:49:14.371014: train_loss -0.733 
2024-05-15 14:49:14.371198: val_loss -0.5312 
2024-05-15 14:49:14.371246: Pseudo dice [0.8022] 
2024-05-15 14:49:14.371298: Epoch time: 40.95 s 
2024-05-15 14:49:15.488495:  
2024-05-15 14:49:15.488632: Epoch 962 
2024-05-15 14:49:15.488729: Current learning rate: 0.00053 
2024-05-15 14:49:56.384482: train_loss -0.759 
2024-05-15 14:49:56.384659: val_loss -0.5504 
2024-05-15 14:49:56.384706: Pseudo dice [0.8009] 
2024-05-15 14:49:56.384758: Epoch time: 40.9 s 
2024-05-15 14:49:57.687639:  
2024-05-15 14:49:57.687783: Epoch 963 
2024-05-15 14:49:57.687879: Current learning rate: 0.00051 
2024-05-15 14:50:38.591705: train_loss -0.7275 
2024-05-15 14:50:38.591885: val_loss -0.5313 
2024-05-15 14:50:38.591934: Pseudo dice [0.7683] 
2024-05-15 14:50:38.591986: Epoch time: 40.91 s 
2024-05-15 14:50:39.704692:  
2024-05-15 14:50:39.704837: Epoch 964 
2024-05-15 14:50:39.704927: Current learning rate: 0.0005 
2024-05-15 14:51:20.596695: train_loss -0.6963 
2024-05-15 14:51:20.596872: val_loss -0.5549 
2024-05-15 14:51:20.596919: Pseudo dice [0.7964] 
2024-05-15 14:51:20.596970: Epoch time: 40.89 s 
2024-05-15 14:51:21.712123:  
2024-05-15 14:51:21.712259: Epoch 965 
2024-05-15 14:51:21.712348: Current learning rate: 0.00049 
2024-05-15 14:52:02.603570: train_loss -0.724 
2024-05-15 14:52:02.603749: val_loss -0.5792 
2024-05-15 14:52:02.603795: Pseudo dice [0.7919] 
2024-05-15 14:52:02.603846: Epoch time: 40.89 s 
2024-05-15 14:52:03.719459:  
2024-05-15 14:52:03.719604: Epoch 966 
2024-05-15 14:52:03.719697: Current learning rate: 0.00048 
2024-05-15 14:52:44.636470: train_loss -0.7298 
2024-05-15 14:52:44.636654: val_loss -0.5426 
2024-05-15 14:52:44.636702: Pseudo dice [0.7797] 
2024-05-15 14:52:44.636754: Epoch time: 40.92 s 
2024-05-15 14:52:45.749831:  
2024-05-15 14:52:45.749964: Epoch 967 
2024-05-15 14:52:45.750059: Current learning rate: 0.00046 
2024-05-15 14:53:26.636245: train_loss -0.7133 
2024-05-15 14:53:26.636425: val_loss -0.6391 
2024-05-15 14:53:26.636476: Pseudo dice [0.7958] 
2024-05-15 14:53:26.636530: Epoch time: 40.89 s 
2024-05-15 14:53:27.780831:  
2024-05-15 14:53:27.780962: Epoch 968 
2024-05-15 14:53:27.781049: Current learning rate: 0.00045 
2024-05-15 14:54:08.657327: train_loss -0.6947 
2024-05-15 14:54:08.657506: val_loss -0.5441 
2024-05-15 14:54:08.657554: Pseudo dice [0.8083] 
2024-05-15 14:54:08.657605: Epoch time: 40.88 s 
2024-05-15 14:54:09.775953:  
2024-05-15 14:54:09.776080: Epoch 969 
2024-05-15 14:54:09.776170: Current learning rate: 0.00044 
2024-05-15 14:54:50.678816: train_loss -0.6722 
2024-05-15 14:54:50.678996: val_loss -0.5387 
2024-05-15 14:54:50.679042: Pseudo dice [0.7877] 
2024-05-15 14:54:50.679094: Epoch time: 40.9 s 
2024-05-15 14:54:51.991806:  
2024-05-15 14:54:51.991947: Epoch 970 
2024-05-15 14:54:51.992034: Current learning rate: 0.00043 
2024-05-15 14:55:32.889489: train_loss -0.7346 
2024-05-15 14:55:32.889670: val_loss -0.5445 
2024-05-15 14:55:32.889718: Pseudo dice [0.7721] 
2024-05-15 14:55:32.889769: Epoch time: 40.9 s 
2024-05-15 14:55:34.004194:  
2024-05-15 14:55:34.004340: Epoch 971 
2024-05-15 14:55:34.004436: Current learning rate: 0.00041 
2024-05-15 14:56:14.837102: train_loss -0.7188 
2024-05-15 14:56:14.837278: val_loss -0.525 
2024-05-15 14:56:14.837327: Pseudo dice [0.8099] 
2024-05-15 14:56:14.837380: Epoch time: 40.83 s 
2024-05-15 14:56:15.958428:  
2024-05-15 14:56:15.958570: Epoch 972 
2024-05-15 14:56:15.958664: Current learning rate: 0.0004 
2024-05-15 14:56:56.800288: train_loss -0.6885 
2024-05-15 14:56:56.800466: val_loss -0.5565 
2024-05-15 14:56:56.800514: Pseudo dice [0.7858] 
2024-05-15 14:56:56.800568: Epoch time: 40.84 s 
2024-05-15 14:56:57.924514:  
2024-05-15 14:56:57.924646: Epoch 973 
2024-05-15 14:56:57.924736: Current learning rate: 0.00039 
2024-05-15 14:57:38.727384: train_loss -0.6986 
2024-05-15 14:57:38.727560: val_loss -0.5454 
2024-05-15 14:57:38.727611: Pseudo dice [0.7847] 
2024-05-15 14:57:38.727669: Epoch time: 40.8 s 
2024-05-15 14:57:39.868939:  
2024-05-15 14:57:39.869319: Epoch 974 
2024-05-15 14:57:39.869451: Current learning rate: 0.00037 
2024-05-15 14:58:20.759855: train_loss -0.6962 
2024-05-15 14:58:20.760031: val_loss -0.5679 
2024-05-15 14:58:20.760081: Pseudo dice [0.7958] 
2024-05-15 14:58:20.760133: Epoch time: 40.89 s 
2024-05-15 14:58:21.876746:  
2024-05-15 14:58:21.876877: Epoch 975 
2024-05-15 14:58:21.876966: Current learning rate: 0.00036 
2024-05-15 14:59:02.746415: train_loss -0.7069 
2024-05-15 14:59:02.746603: val_loss -0.5648 
2024-05-15 14:59:02.746651: Pseudo dice [0.7964] 
2024-05-15 14:59:02.746703: Epoch time: 40.87 s 
2024-05-15 14:59:04.056041:  
2024-05-15 14:59:04.056177: Epoch 976 
2024-05-15 14:59:04.056269: Current learning rate: 0.00035 
2024-05-15 14:59:44.965278: train_loss -0.7069 
2024-05-15 14:59:44.965460: val_loss -0.5919 
2024-05-15 14:59:44.965509: Pseudo dice [0.7833] 
2024-05-15 14:59:44.965560: Epoch time: 40.91 s 
2024-05-15 14:59:46.082049:  
2024-05-15 14:59:46.082199: Epoch 977 
2024-05-15 14:59:46.082289: Current learning rate: 0.00034 
2024-05-15 15:00:27.025412: train_loss -0.7204 
2024-05-15 15:00:27.025592: val_loss -0.5858 
2024-05-15 15:00:27.025640: Pseudo dice [0.8035] 
2024-05-15 15:00:27.025692: Epoch time: 40.94 s 
2024-05-15 15:00:27.025733: Yayy! New best EMA pseudo Dice: 0.7912 
2024-05-15 15:00:28.453985:  
2024-05-15 15:00:28.454173: Epoch 978 
2024-05-15 15:00:28.454265: Current learning rate: 0.00032 
2024-05-15 15:01:09.359339: train_loss -0.7058 
2024-05-15 15:01:09.359527: val_loss -0.5299 
2024-05-15 15:01:09.359575: Pseudo dice [0.7916] 
2024-05-15 15:01:09.359627: Epoch time: 40.91 s 
2024-05-15 15:01:09.359669: Yayy! New best EMA pseudo Dice: 0.7913 
2024-05-15 15:01:10.807751:  
2024-05-15 15:01:10.808046: Epoch 979 
2024-05-15 15:01:10.808142: Current learning rate: 0.00031 
2024-05-15 15:01:51.658453: train_loss -0.7084 
2024-05-15 15:01:51.658632: val_loss -0.5852 
2024-05-15 15:01:51.658679: Pseudo dice [0.7823] 
2024-05-15 15:01:51.658761: Epoch time: 40.85 s 
2024-05-15 15:01:52.794189:  
2024-05-15 15:01:52.794329: Epoch 980 
2024-05-15 15:01:52.794428: Current learning rate: 0.0003 
2024-05-15 15:02:33.606348: train_loss -0.7165 
2024-05-15 15:02:33.606531: val_loss -0.5878 
2024-05-15 15:02:33.606636: Pseudo dice [0.7973] 
2024-05-15 15:02:33.606689: Epoch time: 40.81 s 
2024-05-15 15:02:34.714546:  
2024-05-15 15:02:34.714683: Epoch 981 
2024-05-15 15:02:34.714773: Current learning rate: 0.00028 
2024-05-15 15:03:15.553715: train_loss -0.7145 
2024-05-15 15:03:15.553892: val_loss -0.5475 
2024-05-15 15:03:15.553939: Pseudo dice [0.7964] 
2024-05-15 15:03:15.553992: Epoch time: 40.84 s 
2024-05-15 15:03:15.554033: Yayy! New best EMA pseudo Dice: 0.7916 
2024-05-15 15:03:16.998470:  
2024-05-15 15:03:16.998611: Epoch 982 
2024-05-15 15:03:16.998705: Current learning rate: 0.00027 
2024-05-15 15:03:57.776877: train_loss -0.6952 
2024-05-15 15:03:57.777055: val_loss -0.5541 
2024-05-15 15:03:57.777102: Pseudo dice [0.7918] 
2024-05-15 15:03:57.777152: Epoch time: 40.78 s 
2024-05-15 15:03:57.777194: Yayy! New best EMA pseudo Dice: 0.7916 
2024-05-15 15:03:59.453833:  
2024-05-15 15:03:59.453975: Epoch 983 
2024-05-15 15:03:59.454064: Current learning rate: 0.00026 
2024-05-15 15:04:40.298089: train_loss -0.7405 
2024-05-15 15:04:40.298258: val_loss -0.5136 
2024-05-15 15:04:40.298309: Pseudo dice [0.7868] 
2024-05-15 15:04:40.298362: Epoch time: 40.85 s 
2024-05-15 15:04:41.409576:  
2024-05-15 15:04:41.409720: Epoch 984 
2024-05-15 15:04:41.409815: Current learning rate: 0.00024 
2024-05-15 15:05:22.281029: train_loss -0.6967 
2024-05-15 15:05:22.281229: val_loss -0.5147 
2024-05-15 15:05:22.281277: Pseudo dice [0.8016] 
2024-05-15 15:05:22.281328: Epoch time: 40.87 s 
2024-05-15 15:05:22.281371: Yayy! New best EMA pseudo Dice: 0.7922 
2024-05-15 15:05:23.727007:  
2024-05-15 15:05:23.727324: Epoch 985 
2024-05-15 15:05:23.727451: Current learning rate: 0.00023 
2024-05-15 15:06:04.600533: train_loss -0.6977 
2024-05-15 15:06:04.600711: val_loss -0.5512 
2024-05-15 15:06:04.600758: Pseudo dice [0.7827] 
2024-05-15 15:06:04.600809: Epoch time: 40.87 s 
2024-05-15 15:06:05.703699:  
2024-05-15 15:06:05.703896: Epoch 986 
2024-05-15 15:06:05.703988: Current learning rate: 0.00021 
2024-05-15 15:06:46.595847: train_loss -0.7313 
2024-05-15 15:06:46.596020: val_loss -0.5431 
2024-05-15 15:06:46.596068: Pseudo dice [0.7959] 
2024-05-15 15:06:46.596120: Epoch time: 40.89 s 
2024-05-15 15:06:47.707983:  
2024-05-15 15:06:47.708203: Epoch 987 
2024-05-15 15:06:47.708297: Current learning rate: 0.0002 
2024-05-15 15:07:28.601814: train_loss -0.7257 
2024-05-15 15:07:28.601992: val_loss -0.5934 
2024-05-15 15:07:28.602039: Pseudo dice [0.7913] 
2024-05-15 15:07:28.602091: Epoch time: 40.89 s 
2024-05-15 15:07:29.706461:  
2024-05-15 15:07:29.706648: Epoch 988 
2024-05-15 15:07:29.706744: Current learning rate: 0.00019 
2024-05-15 15:08:10.572642: train_loss -0.6914 
2024-05-15 15:08:10.572817: val_loss -0.5767 
2024-05-15 15:08:10.572866: Pseudo dice [0.7981] 
2024-05-15 15:08:10.572917: Epoch time: 40.87 s 
2024-05-15 15:08:10.572958: Yayy! New best EMA pseudo Dice: 0.7923 
2024-05-15 15:08:12.183234:  
2024-05-15 15:08:12.183433: Epoch 989 
2024-05-15 15:08:12.183556: Current learning rate: 0.00017 
2024-05-15 15:08:53.056020: train_loss -0.716 
2024-05-15 15:08:53.056200: val_loss -0.4763 
2024-05-15 15:08:53.056249: Pseudo dice [0.7644] 
2024-05-15 15:08:53.056300: Epoch time: 40.87 s 
2024-05-15 15:08:54.164495:  
2024-05-15 15:08:54.164638: Epoch 990 
2024-05-15 15:08:54.164731: Current learning rate: 0.00016 
2024-05-15 15:09:35.050867: train_loss -0.7094 
2024-05-15 15:09:35.051045: val_loss -0.5799 
2024-05-15 15:09:35.051092: Pseudo dice [0.7991] 
2024-05-15 15:09:35.051142: Epoch time: 40.89 s 
2024-05-15 15:09:36.157767:  
2024-05-15 15:09:36.157945: Epoch 991 
2024-05-15 15:09:36.158059: Current learning rate: 0.00014 
2024-05-15 15:10:17.030393: train_loss -0.7157 
2024-05-15 15:10:17.030573: val_loss -0.5227 
2024-05-15 15:10:17.030620: Pseudo dice [0.7824] 
2024-05-15 15:10:17.030672: Epoch time: 40.87 s 
2024-05-15 15:10:18.139897:  
2024-05-15 15:10:18.140028: Epoch 992 
2024-05-15 15:10:18.140123: Current learning rate: 0.00013 
2024-05-15 15:10:59.011751: train_loss -0.723 
2024-05-15 15:10:59.011931: val_loss -0.5805 
2024-05-15 15:10:59.011980: Pseudo dice [0.7865] 
2024-05-15 15:10:59.012031: Epoch time: 40.87 s 
2024-05-15 15:11:00.121408:  
2024-05-15 15:11:00.121540: Epoch 993 
2024-05-15 15:11:00.121629: Current learning rate: 0.00011 
2024-05-15 15:11:40.977714: train_loss -0.7203 
2024-05-15 15:11:40.977889: val_loss -0.5145 
2024-05-15 15:11:40.977936: Pseudo dice [0.781] 
2024-05-15 15:11:40.977987: Epoch time: 40.86 s 
2024-05-15 15:11:42.084483:  
2024-05-15 15:11:42.084669: Epoch 994 
2024-05-15 15:11:42.084762: Current learning rate: 0.0001 
2024-05-15 15:12:22.956372: train_loss -0.7164 
2024-05-15 15:12:22.956552: val_loss -0.5138 
2024-05-15 15:12:22.956599: Pseudo dice [0.7752] 
2024-05-15 15:12:22.956651: Epoch time: 40.87 s 
2024-05-15 15:12:24.068861:  
2024-05-15 15:12:24.068990: Epoch 995 
2024-05-15 15:12:24.069088: Current learning rate: 8e-05 
2024-05-15 15:13:05.006904: train_loss -0.7133 
2024-05-15 15:13:05.007089: val_loss -0.5343 
2024-05-15 15:13:05.007137: Pseudo dice [0.7733] 
2024-05-15 15:13:05.007189: Epoch time: 40.94 s 
2024-05-15 15:13:06.115465:  
2024-05-15 15:13:06.115664: Epoch 996 
2024-05-15 15:13:06.115760: Current learning rate: 7e-05 
2024-05-15 15:13:47.007241: train_loss -0.7142 
2024-05-15 15:13:47.007423: val_loss -0.5258 
2024-05-15 15:13:47.007472: Pseudo dice [0.7871] 
2024-05-15 15:13:47.007525: Epoch time: 40.89 s 
2024-05-15 15:13:48.140745:  
2024-05-15 15:13:48.140885: Epoch 997 
2024-05-15 15:13:48.140978: Current learning rate: 5e-05 
2024-05-15 15:14:29.032223: train_loss -0.7235 
2024-05-15 15:14:29.032399: val_loss -0.5391 
2024-05-15 15:14:29.032447: Pseudo dice [0.7834] 
2024-05-15 15:14:29.032497: Epoch time: 40.89 s 
2024-05-15 15:14:30.148635:  
2024-05-15 15:14:30.148766: Epoch 998 
2024-05-15 15:14:30.148857: Current learning rate: 4e-05 
2024-05-15 15:15:11.050933: train_loss -0.7127 
2024-05-15 15:15:11.051105: val_loss -0.5925 
2024-05-15 15:15:11.051154: Pseudo dice [0.8013] 
2024-05-15 15:15:11.051207: Epoch time: 40.9 s 
2024-05-15 15:15:12.347672:  
2024-05-15 15:15:12.347805: Epoch 999 
2024-05-15 15:15:12.347902: Current learning rate: 2e-05 
2024-05-15 15:15:53.198766: train_loss -0.7099 
2024-05-15 15:15:53.198949: val_loss -0.5105 
2024-05-15 15:15:53.199018: Pseudo dice [0.7837] 
2024-05-15 15:15:53.199069: Epoch time: 40.85 s 
2024-05-15 15:15:54.910047: Training done. 
2024-05-15 15:15:55.439994: Using splits from existing split file: /raid/dataset/nnUNet_preprocessed/Dataset028_4DCT_ONLY_1_3D/splits_final.json 
2024-05-15 15:15:55.440556: The split file contains 5 splits. 
2024-05-15 15:15:55.440595: Desired fold for training: 0 
2024-05-15 15:15:55.440629: This split has 118 training and 30 validation cases. 
2024-05-15 15:15:55.440863: predicting 112_5_16_2_0 
2024-05-15 15:15:55.442138: 112_5_16_2_0, shape torch.Size([1, 47, 178, 264]), rank 0 
2024-05-15 15:15:57.285218: predicting 114_4_0_0_0 
2024-05-15 15:15:57.286905: 114_4_0_0_0, shape torch.Size([1, 54, 206, 288]), rank 0 
2024-05-15 15:15:59.187625: predicting B10_00_8_2_0 
2024-05-15 15:15:59.188637: B10_00_8_2_0, shape torch.Size([1, 46, 139, 216]), rank 0 
2024-05-15 15:15:59.438053: predicting B21_01_16_1_0 
2024-05-15 15:15:59.439193: B21_01_16_1_0, shape torch.Size([1, 44, 160, 201]), rank 0 
2024-05-15 15:15:59.690817: predicting B25_00_8_3_0 
2024-05-15 15:15:59.691662: B25_00_8_3_0, shape torch.Size([1, 46, 163, 219]), rank 0 
2024-05-15 15:15:59.944580: predicting B27_00_0_0_0 
2024-05-15 15:15:59.945735: B27_00_0_0_0, shape torch.Size([1, 39, 162, 236]), rank 0 
2024-05-15 15:16:00.198120: predicting B40_01_0_0_0 
2024-05-15 15:16:00.198913: B40_01_0_0_0, shape torch.Size([1, 46, 159, 264]), rank 0 
2024-05-15 15:16:00.686675: predicting B_04_postCT_0_0_0 
2024-05-15 15:16:00.687504: B_04_postCT_0_0_0, shape torch.Size([1, 43, 151, 205]), rank 0 
2024-05-15 15:16:00.939585: predicting B_17_preCT_0_0_0 
2024-05-15 15:16:00.940338: B_17_preCT_0_0_0, shape torch.Size([1, 44, 131, 227]), rank 0 
2024-05-15 15:16:01.192579: predicting B_24_preCT_8_3_0 
2024-05-15 15:16:01.193407: B_24_preCT_8_3_0, shape torch.Size([1, 49, 175, 244]), rank 0 
2024-05-15 15:16:01.681150: predicting B_28_postCT_16_1_0 
2024-05-15 15:16:01.682055: B_28_postCT_16_1_0, shape torch.Size([1, 43, 183, 244]), rank 0 
2024-05-15 15:16:01.933917: predicting B_36_preCT_0_0_0 
2024-05-15 15:16:01.934851: B_36_preCT_0_0_0, shape torch.Size([1, 42, 185, 245]), rank 0 
2024-05-15 15:16:02.189073: predicting B_41_preCT_16_2_0 
2024-05-15 15:16:02.190022: B_41_preCT_16_2_0, shape torch.Size([1, 48, 218, 269]), rank 0 
2024-05-15 15:16:03.147366: predicting CU_04_postCT_4_2_0 
2024-05-15 15:16:03.148313: CU_04_postCT_4_2_0, shape torch.Size([1, 42, 123, 229]), rank 0 
2024-05-15 15:16:03.398579: predicting CU_21_postCT_4_3_0 
2024-05-15 15:16:03.399404: CU_21_postCT_4_3_0, shape torch.Size([1, 38, 123, 212]), rank 0 
2024-05-15 15:16:03.650001: predicting Lime_05_00_4_3_0 
2024-05-15 15:16:03.650819: Lime_05_00_4_3_0, shape torch.Size([1, 46, 207, 308]), rank 0 
2024-05-15 15:16:04.610652: predicting Lime_06_00_4_3_0 
2024-05-15 15:16:04.611678: Lime_06_00_4_3_0, shape torch.Size([1, 40, 158, 217]), rank 0 
2024-05-15 15:16:04.865139: predicting Lime_14_01_16_2_0 
2024-05-15 15:16:04.865953: Lime_14_01_16_2_0, shape torch.Size([1, 40, 186, 277]), rank 0 
2024-05-15 15:16:05.353891: predicting case1_4_3_0 
2024-05-15 15:16:05.354817: case1_4_3_0, shape torch.Size([1, 41, 157, 242]), rank 0 
2024-05-15 15:16:05.607836: predicting case4_4_2_0 
2024-05-15 15:16:05.608693: case4_4_2_0, shape torch.Size([1, 44, 165, 252]), rank 0 
2024-05-15 15:16:05.861577: predicting case9_8_2_0 
2024-05-15 15:16:05.862476: case9_8_2_0, shape torch.Size([1, 38, 191, 256]), rank 0 
2024-05-15 15:16:06.114924: predicting case_06_4_3_0 
2024-05-15 15:16:06.115798: case_06_4_3_0, shape torch.Size([1, 54, 182, 260]), rank 0 
2024-05-15 15:16:07.073768: predicting neg_38_4_2_0 
2024-05-15 15:16:07.074781: neg_38_4_2_0, shape torch.Size([1, 46, 171, 269]), rank 0 
2024-05-15 15:16:07.563041: predicting neg_47_8_3_0 
2024-05-15 15:16:07.564065: neg_47_8_3_0, shape torch.Size([1, 40, 175, 277]), rank 0 
2024-05-15 15:16:08.051708: predicting petGas_14_16_2_0 
2024-05-15 15:16:08.052607: petGas_14_16_2_0, shape torch.Size([1, 70, 195, 271]), rank 0 
2024-05-15 15:16:09.949751: predicting petGas_17_16_2_0 
2024-05-15 15:16:09.950911: petGas_17_16_2_0, shape torch.Size([1, 70, 183, 257]), rank 0 
2024-05-15 15:16:10.910698: predicting petGas_1_4_2_0 
2024-05-15 15:16:10.911793: petGas_1_4_2_0, shape torch.Size([1, 70, 199, 285]), rank 0 
2024-05-15 15:16:12.809075: predicting petGas_2_8_3_0 
2024-05-15 15:16:12.810293: petGas_2_8_3_0, shape torch.Size([1, 69, 227, 285]), rank 0 
2024-05-15 15:16:14.710174: predicting petGas_6_4_2_0 
2024-05-15 15:16:14.711406: petGas_6_4_2_0, shape torch.Size([1, 70, 227, 317]), rank 0 
2024-05-15 15:16:16.612755: predicting pos_47_16_2_0 
2024-05-15 15:16:16.614347: pos_47_16_2_0, shape torch.Size([1, 48, 207, 257]), rank 0 
2024-05-15 15:16:21.940526: Validation complete 
2024-05-15 15:16:21.940626: Mean Validation Dice:  0.7881016235350979 
